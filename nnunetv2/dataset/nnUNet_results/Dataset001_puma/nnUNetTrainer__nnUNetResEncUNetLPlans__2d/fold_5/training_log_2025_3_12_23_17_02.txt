
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-12 23:17:02.286129: do_dummy_2d_data_aug: False 
2025-03-12 23:17:02.288379: Using splits from existing split file: /data/hotaru/projects/nnUNet/nnunetv2/dataset/nnUNet_preprocessed/Dataset001_puma/splits_final.json 
2025-03-12 23:17:02.301006: The split file contains 5 splits. 
2025-03-12 23:17:02.301121: Desired fold for training: 5 
2025-03-12 23:17:02.301182: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-03-12 23:17:02.304260: This random 80:20 split has 322 training and 81 validation cases. 
2025-03-12 23:17:26.158243: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 13, 'patch_size': [896, 768], 'median_image_size_in_voxels': [1024.0, 1024.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_puma', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 1024, 1024], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 158.0860595703125, 'median': 166.0, 'min': 0.0, 'percentile_00_5': 30.0, 'percentile_99_5': 246.0, 'std': 50.084224700927734}, '1': {'max': 255.0, 'mean': 106.11481475830078, 'median': 101.0, 'min': 0.0, 'percentile_00_5': 15.0, 'percentile_99_5': 225.0, 'std': 49.32137680053711}, '2': {'max': 255.0, 'mean': 178.19647216796875, 'median': 182.0, 'min': 0.0, 'percentile_00_5': 46.0, 'percentile_99_5': 244.0, 'std': 34.66091537475586}}} 
 
2025-03-12 23:17:29.494483: unpacking dataset... 
2025-03-12 23:17:38.940045: unpacking done... 
2025-03-12 23:17:38.948994: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-03-12 23:17:39.101544:  
2025-03-12 23:17:39.102129: Epoch 0 
2025-03-12 23:17:39.103046: Current learning rate: 0.01 
2025-03-12 23:24:11.056758: train_loss 1.0994 
2025-03-12 23:24:11.057443: val_loss 0.9324 
2025-03-12 23:24:11.057710: Pseudo dice [0.5, 0.0, 0.6968, 0.0, 0.0] 
2025-03-12 23:24:11.057902: Epoch time: 391.97 s 
2025-03-12 23:24:11.057986: Yayy! New best EMA pseudo Dice: 0.2394 
2025-03-12 23:24:15.590522:  
2025-03-12 23:24:15.590923: Epoch 1 
2025-03-12 23:24:15.591222: Current learning rate: 0.00995 
2025-03-12 23:27:51.315463: train_loss 0.781 
2025-03-12 23:27:51.316048: val_loss 0.8468 
2025-03-12 23:27:51.316170: Pseudo dice [0.5313, 0.0, 0.725, 0.0012, 0.0] 
2025-03-12 23:27:51.316368: Epoch time: 215.73 s 
2025-03-12 23:27:51.316437: Yayy! New best EMA pseudo Dice: 0.2406 
2025-03-12 23:27:55.604706:  
2025-03-12 23:27:55.605076: Epoch 2 
2025-03-12 23:27:55.605382: Current learning rate: 0.00991 
2025-03-12 23:31:30.926833: train_loss 0.6363 
2025-03-12 23:31:30.927165: val_loss 0.6932 
2025-03-12 23:31:30.927306: Pseudo dice [0.5648, 0.0, 0.7898, 0.4514, 0.0] 
2025-03-12 23:31:30.927489: Epoch time: 215.33 s 
2025-03-12 23:31:30.927627: Yayy! New best EMA pseudo Dice: 0.2526 
2025-03-12 23:31:35.526789:  
2025-03-12 23:31:35.527158: Epoch 3 
2025-03-12 23:31:35.527616: Current learning rate: 0.00986 
2025-03-12 23:35:11.051514: train_loss 0.4287 
2025-03-12 23:35:11.052095: val_loss 0.6251 
2025-03-12 23:35:11.052386: Pseudo dice [0.5683, 0.2985, 0.7789, 0.5176, 0.0152] 
2025-03-12 23:35:11.052517: Epoch time: 215.53 s 
2025-03-12 23:35:11.052590: Yayy! New best EMA pseudo Dice: 0.2709 
2025-03-12 23:35:16.169538:  
2025-03-12 23:35:16.169837: Epoch 4 
2025-03-12 23:35:16.170184: Current learning rate: 0.00982 
2025-03-12 23:38:51.949752: train_loss 0.2681 
2025-03-12 23:38:51.950184: val_loss 0.5439 
2025-03-12 23:38:51.950326: Pseudo dice [0.6274, 0.3889, 0.7986, 0.6051, 0.0837] 
2025-03-12 23:38:51.950428: Epoch time: 215.78 s 
2025-03-12 23:38:51.950500: Yayy! New best EMA pseudo Dice: 0.2939 
2025-03-12 23:38:56.987660:  
2025-03-12 23:38:56.988016: Epoch 5 
2025-03-12 23:38:56.988163: Current learning rate: 0.00977 
2025-03-12 23:42:32.301925: train_loss 0.1969 
2025-03-12 23:42:32.302364: val_loss 0.4669 
2025-03-12 23:42:32.302491: Pseudo dice [0.6731, 0.4034, 0.8083, 0.5942, 0.2832] 
2025-03-12 23:42:32.302598: Epoch time: 215.32 s 
2025-03-12 23:42:32.302668: Yayy! New best EMA pseudo Dice: 0.3198 
2025-03-12 23:42:36.589411:  
2025-03-12 23:42:36.589809: Epoch 6 
2025-03-12 23:42:36.589957: Current learning rate: 0.00973 
2025-03-12 23:46:11.748766: train_loss 0.0274 
2025-03-12 23:46:11.749128: val_loss 0.2893 
2025-03-12 23:46:11.749315: Pseudo dice [0.6761, 0.4727, 0.8331, 0.7314, 0.0879] 
2025-03-12 23:46:11.749445: Epoch time: 215.16 s 
2025-03-12 23:46:11.749545: Yayy! New best EMA pseudo Dice: 0.3438 
2025-03-12 23:46:16.203985:  
2025-03-12 23:46:16.204399: Epoch 7 
2025-03-12 23:46:16.204610: Current learning rate: 0.00968 
2025-03-12 23:49:51.228179: train_loss -0.0861 
2025-03-12 23:49:51.228843: val_loss 0.3128 
2025-03-12 23:49:51.229049: Pseudo dice [0.6814, 0.4805, 0.8223, 0.7711, 0.3308] 
2025-03-12 23:49:51.229192: Epoch time: 215.03 s 
2025-03-12 23:49:51.229324: Yayy! New best EMA pseudo Dice: 0.3712 
2025-03-12 23:49:54.584610:  
2025-03-12 23:49:54.584997: Epoch 8 
2025-03-12 23:49:54.585295: Current learning rate: 0.00964 
2025-03-12 23:53:29.851911: train_loss -0.1646 
2025-03-12 23:53:29.852383: val_loss 0.1707 
2025-03-12 23:53:29.852629: Pseudo dice [0.7144, 0.5636, 0.8252, 0.7668, 0.6658] 
2025-03-12 23:53:29.852792: Epoch time: 215.28 s 
2025-03-12 23:53:29.852931: Yayy! New best EMA pseudo Dice: 0.4048 
2025-03-12 23:53:34.283234:  
2025-03-12 23:53:34.283659: Epoch 9 
2025-03-12 23:53:34.283920: Current learning rate: 0.00959 
2025-03-12 23:57:09.765346: train_loss -0.2377 
2025-03-12 23:57:09.765789: val_loss 0.0296 
2025-03-12 23:57:09.765948: Pseudo dice [0.7653, 0.6119, 0.8486, 0.8186, 0.7756] 
2025-03-12 23:57:09.766042: Epoch time: 215.49 s 
2025-03-12 23:57:09.766107: Yayy! New best EMA pseudo Dice: 0.4407 
2025-03-12 23:57:14.968776:  
2025-03-12 23:57:14.969172: Epoch 10 
2025-03-12 23:57:14.969391: Current learning rate: 0.00955 
2025-03-13 00:00:50.221260: train_loss -0.3457 
2025-03-13 00:00:50.221727: val_loss 0.0508 
2025-03-13 00:00:50.221917: Pseudo dice [0.7431, 0.6284, 0.8346, 0.8215, 0.6562] 
2025-03-13 00:00:50.222015: Epoch time: 215.26 s 
2025-03-13 00:00:50.222083: Yayy! New best EMA pseudo Dice: 0.4703 
2025-03-13 00:00:54.942190:  
2025-03-13 00:00:54.942810: Epoch 11 
2025-03-13 00:00:54.943702: Current learning rate: 0.0095 
2025-03-13 00:04:30.264528: train_loss -0.4223 
2025-03-13 00:04:30.264958: val_loss -0.0166 
2025-03-13 00:04:30.265115: Pseudo dice [0.7687, 0.6602, 0.8521, 0.8102, 0.7482] 
2025-03-13 00:04:30.265216: Epoch time: 215.33 s 
2025-03-13 00:04:30.265287: Yayy! New best EMA pseudo Dice: 0.5001 
2025-03-13 00:04:34.746123:  
2025-03-13 00:04:34.746583: Epoch 12 
2025-03-13 00:04:34.746859: Current learning rate: 0.00946 
2025-03-13 00:08:10.099816: train_loss -0.4055 
2025-03-13 00:08:10.100249: val_loss -0.0154 
2025-03-13 00:08:10.100383: Pseudo dice [0.7687, 0.6423, 0.8571, 0.8232, 0.7857] 
2025-03-13 00:08:10.100479: Epoch time: 215.36 s 
2025-03-13 00:08:10.100545: Yayy! New best EMA pseudo Dice: 0.5276 
2025-03-13 00:08:14.774836:  
2025-03-13 00:08:14.775185: Epoch 13 
2025-03-13 00:08:14.775459: Current learning rate: 0.00941 
2025-03-13 00:11:50.064319: train_loss -0.4738 
2025-03-13 00:11:50.064690: val_loss -0.0945 
2025-03-13 00:11:50.064825: Pseudo dice [0.8059, 0.6837, 0.8729, 0.8065, 0.8352] 
2025-03-13 00:11:50.064935: Epoch time: 215.29 s 
2025-03-13 00:11:50.065001: Yayy! New best EMA pseudo Dice: 0.5549 
2025-03-13 00:11:54.590093:  
2025-03-13 00:11:54.590827: Epoch 14 
2025-03-13 00:11:54.591169: Current learning rate: 0.00937 
2025-03-13 00:15:29.885117: train_loss -0.527 
2025-03-13 00:15:29.885561: val_loss -0.0718 
2025-03-13 00:15:29.885765: Pseudo dice [0.7766, 0.707, 0.876, 0.8355, 0.744] 
2025-03-13 00:15:29.885888: Epoch time: 215.3 s 
2025-03-13 00:15:29.885963: Yayy! New best EMA pseudo Dice: 0.5782 
2025-03-13 00:15:33.408274:  
2025-03-13 00:15:33.408764: Epoch 15 
2025-03-13 00:15:33.408994: Current learning rate: 0.00932 
2025-03-13 00:19:08.996792: train_loss -0.5496 
2025-03-13 00:19:08.997482: val_loss -0.1199 
2025-03-13 00:19:08.997764: Pseudo dice [0.8024, 0.7106, 0.8878, 0.8385, 0.8157] 
2025-03-13 00:19:08.997952: Epoch time: 215.6 s 
2025-03-13 00:19:08.998091: Yayy! New best EMA pseudo Dice: 0.6015 
2025-03-13 00:19:12.336697:  
2025-03-13 00:19:12.337162: Epoch 16 
2025-03-13 00:19:12.337418: Current learning rate: 0.00928 
2025-03-13 00:22:47.711980: train_loss -0.5543 
2025-03-13 00:22:47.712358: val_loss -0.083 
2025-03-13 00:22:47.712542: Pseudo dice [0.814, 0.7294, 0.8913, 0.7591, 0.6635] 
2025-03-13 00:22:47.712647: Epoch time: 215.38 s 
2025-03-13 00:22:47.712714: Yayy! New best EMA pseudo Dice: 0.6185 
2025-03-13 00:22:52.156705:  
2025-03-13 00:22:52.157027: Epoch 17 
2025-03-13 00:22:52.157267: Current learning rate: 0.00923 
2025-03-13 00:26:27.441026: train_loss -0.5262 
2025-03-13 00:26:27.441606: val_loss 0.0134 
2025-03-13 00:26:27.441991: Pseudo dice [0.7706, 0.697, 0.8451, 0.8244, 0.8879] 
2025-03-13 00:26:27.442108: Epoch time: 215.29 s 
2025-03-13 00:26:27.442181: Yayy! New best EMA pseudo Dice: 0.6371 
2025-03-13 00:26:32.348004:  
2025-03-13 00:26:32.348409: Epoch 18 
2025-03-13 00:26:32.348644: Current learning rate: 0.00919 
2025-03-13 00:30:08.520006: train_loss -0.5108 
2025-03-13 00:30:08.520350: val_loss 0.0184 
2025-03-13 00:30:08.520615: Pseudo dice [0.7539, 0.6851, 0.8478, 0.8359, 0.5397] 
2025-03-13 00:30:08.520814: Epoch time: 216.18 s 
2025-03-13 00:30:08.520917: Yayy! New best EMA pseudo Dice: 0.6467 
2025-03-13 00:30:13.352852:  
2025-03-13 00:30:13.353230: Epoch 19 
2025-03-13 00:30:13.353476: Current learning rate: 0.00914 
2025-03-13 00:33:49.428607: train_loss -0.562 
2025-03-13 00:33:49.428944: val_loss -0.0965 
2025-03-13 00:33:49.429128: Pseudo dice [0.8212, 0.7506, 0.8672, 0.8318, 0.6991] 
2025-03-13 00:33:49.429797: Epoch time: 216.08 s 
2025-03-13 00:33:49.430037: Yayy! New best EMA pseudo Dice: 0.6614 
2025-03-13 00:33:52.801295:  
2025-03-13 00:33:52.801710: Epoch 20 
2025-03-13 00:33:52.801885: Current learning rate: 0.0091 
2025-03-13 00:37:28.449363: train_loss -0.6137 
2025-03-13 00:37:28.449715: val_loss -0.2207 
2025-03-13 00:37:28.449869: Pseudo dice [0.8182, 0.7484, 0.9026, 0.8609, 0.9061] 
2025-03-13 00:37:28.449964: Epoch time: 215.65 s 
2025-03-13 00:37:28.450030: Yayy! New best EMA pseudo Dice: 0.68 
2025-03-13 00:37:33.026500:  
2025-03-13 00:37:33.026822: Epoch 21 
2025-03-13 00:37:33.027080: Current learning rate: 0.00905 
2025-03-13 00:41:08.757132: train_loss -0.6423 
2025-03-13 00:41:08.757574: val_loss -0.2127 
2025-03-13 00:41:08.757694: Pseudo dice [0.8135, 0.7584, 0.9041, 0.8757, 0.8669] 
2025-03-13 00:41:08.757792: Epoch time: 215.74 s 
2025-03-13 00:41:08.757856: Yayy! New best EMA pseudo Dice: 0.6964 
2025-03-13 00:41:13.258475:  
2025-03-13 00:41:13.258802: Epoch 22 
2025-03-13 00:41:13.258973: Current learning rate: 0.009 
2025-03-13 00:44:48.551193: train_loss -0.6708 
2025-03-13 00:44:48.551545: val_loss -0.1286 
2025-03-13 00:44:48.551695: Pseudo dice [0.8161, 0.7749, 0.8865, 0.8529, 0.8549] 
2025-03-13 00:44:48.551799: Epoch time: 215.3 s 
2025-03-13 00:44:48.551949: Yayy! New best EMA pseudo Dice: 0.7104 
2025-03-13 00:44:52.080866:  
2025-03-13 00:44:52.081366: Epoch 23 
2025-03-13 00:44:52.081723: Current learning rate: 0.00896 
2025-03-13 00:48:28.122818: train_loss -0.6551 
2025-03-13 00:48:28.123341: val_loss -0.1376 
2025-03-13 00:48:28.123728: Pseudo dice [0.7969, 0.7507, 0.8833, 0.8607, 0.9186] 
2025-03-13 00:48:28.123945: Epoch time: 216.05 s 
2025-03-13 00:48:28.124058: Yayy! New best EMA pseudo Dice: 0.7236 
2025-03-13 00:48:32.798035:  
2025-03-13 00:48:32.798413: Epoch 24 
2025-03-13 00:48:32.798632: Current learning rate: 0.00891 
2025-03-13 00:52:08.614603: train_loss -0.6802 
2025-03-13 00:52:08.614985: val_loss -0.1864 
2025-03-13 00:52:08.615162: Pseudo dice [0.8148, 0.793, 0.8992, 0.8733, 0.8057] 
2025-03-13 00:52:08.615313: Epoch time: 215.82 s 
2025-03-13 00:52:08.615420: Yayy! New best EMA pseudo Dice: 0.7349 
2025-03-13 00:52:13.868958:  
2025-03-13 00:52:13.869295: Epoch 25 
2025-03-13 00:52:13.869514: Current learning rate: 0.00887 
2025-03-13 00:55:49.596133: train_loss -0.6804 
2025-03-13 00:55:49.596984: val_loss -0.1772 
2025-03-13 00:55:49.597514: Pseudo dice [0.8273, 0.7977, 0.891, 0.8699, 0.8686] 
2025-03-13 00:55:49.598003: Epoch time: 215.73 s 
2025-03-13 00:55:49.598422: Yayy! New best EMA pseudo Dice: 0.7465 
2025-03-13 00:55:54.197180:  
2025-03-13 00:55:54.197597: Epoch 26 
2025-03-13 00:55:54.197858: Current learning rate: 0.00882 
2025-03-13 00:59:29.701082: train_loss -0.712 
2025-03-13 00:59:29.701590: val_loss -0.1918 
2025-03-13 00:59:29.701927: Pseudo dice [0.8364, 0.8061, 0.9101, 0.8824, 0.8034] 
2025-03-13 00:59:29.702104: Epoch time: 215.51 s 
2025-03-13 00:59:29.702176: Yayy! New best EMA pseudo Dice: 0.7567 
2025-03-13 00:59:33.741322:  
2025-03-13 00:59:33.741714: Epoch 27 
2025-03-13 00:59:33.741883: Current learning rate: 0.00878 
2025-03-13 01:03:09.306307: train_loss -0.7177 
2025-03-13 01:03:09.307684: val_loss -0.2233 
2025-03-13 01:03:09.308567: Pseudo dice [0.8282, 0.793, 0.9072, 0.8946, 0.8997] 
2025-03-13 01:03:09.309412: Epoch time: 215.57 s 
2025-03-13 01:03:09.310209: Yayy! New best EMA pseudo Dice: 0.7674 
2025-03-13 01:03:12.837296:  
2025-03-13 01:03:12.837647: Epoch 28 
2025-03-13 01:03:12.837924: Current learning rate: 0.00873 
2025-03-13 01:06:48.988804: train_loss -0.702 
2025-03-13 01:06:48.989384: val_loss -0.0967 
2025-03-13 01:06:48.989583: Pseudo dice [0.827, 0.7287, 0.8842, 0.8296, 0.9261] 
2025-03-13 01:06:48.989690: Epoch time: 216.16 s 
2025-03-13 01:06:48.989753: Yayy! New best EMA pseudo Dice: 0.7746 
2025-03-13 01:06:53.699394:  
2025-03-13 01:06:53.699778: Epoch 29 
2025-03-13 01:06:53.699994: Current learning rate: 0.00868 
2025-03-13 01:10:29.688857: train_loss -0.6919 
2025-03-13 01:10:29.689366: val_loss -0.1243 
2025-03-13 01:10:29.689606: Pseudo dice [0.8292, 0.7834, 0.9029, 0.8729, 0.4033] 
2025-03-13 01:10:29.689764: Epoch time: 216.0 s 
2025-03-13 01:10:31.424327:  
2025-03-13 01:10:31.424715: Epoch 30 
2025-03-13 01:10:31.424897: Current learning rate: 0.00864 
2025-03-13 01:14:06.853463: train_loss -0.6794 
2025-03-13 01:14:06.854105: val_loss -0.0865 
2025-03-13 01:14:06.854407: Pseudo dice [0.8302, 0.7212, 0.8882, 0.7678, 0.7801] 
2025-03-13 01:14:06.854615: Epoch time: 215.44 s 
2025-03-13 01:14:06.854827: Yayy! New best EMA pseudo Dice: 0.7754 
2025-03-13 01:14:11.971566:  
2025-03-13 01:14:11.971862: Epoch 31 
2025-03-13 01:14:11.972062: Current learning rate: 0.00859 
2025-03-13 01:17:47.327808: train_loss -0.3286 
2025-03-13 01:17:47.328200: val_loss 0.0367 
2025-03-13 01:17:47.328381: Pseudo dice [0.7337, 0.6336, 0.8608, 0.7825, 0.868] 
2025-03-13 01:17:47.328542: Epoch time: 215.36 s 
2025-03-13 01:17:47.328712: Yayy! New best EMA pseudo Dice: 0.7755 
2025-03-13 01:17:51.874095:  
2025-03-13 01:17:51.874410: Epoch 32 
2025-03-13 01:17:51.874639: Current learning rate: 0.00855 
2025-03-13 01:21:27.510227: train_loss -0.4489 
2025-03-13 01:21:27.510838: val_loss 0.0229 
2025-03-13 01:21:27.511164: Pseudo dice [0.7523, 0.6753, 0.8663, 0.8316, 0.6021] 
2025-03-13 01:21:27.511351: Epoch time: 215.64 s 
2025-03-13 01:21:29.386794:  
2025-03-13 01:21:29.387199: Epoch 33 
2025-03-13 01:21:29.387719: Current learning rate: 0.0085 
2025-03-13 01:25:06.074048: train_loss -0.606 
2025-03-13 01:25:06.074455: val_loss -0.1505 
2025-03-13 01:25:06.074659: Pseudo dice [0.8013, 0.7646, 0.8909, 0.8562, 0.7492] 
2025-03-13 01:25:06.074809: Epoch time: 216.7 s 
2025-03-13 01:25:06.074954: Yayy! New best EMA pseudo Dice: 0.7765 
2025-03-13 01:25:11.156647:  
2025-03-13 01:25:11.157804: Epoch 34 
2025-03-13 01:25:11.158142: Current learning rate: 0.00846 
2025-03-13 01:28:46.986097: train_loss -0.6627 
2025-03-13 01:28:46.986536: val_loss -0.1681 
2025-03-13 01:28:46.986737: Pseudo dice [0.8232, 0.7735, 0.9112, 0.8527, 0.2624] 
2025-03-13 01:28:46.986913: Epoch time: 215.84 s 
2025-03-13 01:28:48.768875:  
2025-03-13 01:28:48.769212: Epoch 35 
2025-03-13 01:28:48.769455: Current learning rate: 0.00841 
2025-03-13 01:32:24.029811: train_loss -0.6453 
2025-03-13 01:32:24.030374: val_loss -0.1929 
2025-03-13 01:32:24.030556: Pseudo dice [0.8091, 0.7847, 0.893, 0.8659, 0.8153] 
2025-03-13 01:32:24.030732: Epoch time: 215.27 s 
2025-03-13 01:32:24.030803: Yayy! New best EMA pseudo Dice: 0.7775 
2025-03-13 01:32:28.772658:  
2025-03-13 01:32:28.773031: Epoch 36 
2025-03-13 01:32:28.773202: Current learning rate: 0.00836 
2025-03-13 01:36:03.920178: train_loss -0.6592 
2025-03-13 01:36:03.920964: val_loss -0.0992 
2025-03-13 01:36:03.921433: Pseudo dice [0.7953, 0.7689, 0.8747, 0.816, 0.7778] 
2025-03-13 01:36:03.921740: Epoch time: 215.15 s 
2025-03-13 01:36:03.921844: Yayy! New best EMA pseudo Dice: 0.7804 
2025-03-13 01:36:07.378971:  
2025-03-13 01:36:07.379264: Epoch 37 
2025-03-13 01:36:07.379537: Current learning rate: 0.00832 
2025-03-13 01:39:42.574079: train_loss -0.6754 
2025-03-13 01:39:42.574739: val_loss -0.2924 
2025-03-13 01:39:42.575035: Pseudo dice [0.8403, 0.8095, 0.9096, 0.9012, 0.9457] 
2025-03-13 01:39:42.575202: Epoch time: 215.2 s 
2025-03-13 01:39:42.575272: Yayy! New best EMA pseudo Dice: 0.7905 
2025-03-13 01:39:47.323563:  
2025-03-13 01:39:47.323941: Epoch 38 
2025-03-13 01:39:47.324161: Current learning rate: 0.00827 
2025-03-13 01:43:23.045784: train_loss -0.7124 
2025-03-13 01:43:23.046371: val_loss -0.2395 
2025-03-13 01:43:23.046701: Pseudo dice [0.8478, 0.8252, 0.9041, 0.8543, 0.918] 
2025-03-13 01:43:23.046872: Epoch time: 215.73 s 
2025-03-13 01:43:23.046987: Yayy! New best EMA pseudo Dice: 0.7984 
2025-03-13 01:43:26.506021:  
2025-03-13 01:43:26.506387: Epoch 39 
2025-03-13 01:43:26.506586: Current learning rate: 0.00823 
2025-03-13 01:47:02.032959: train_loss -0.7398 
2025-03-13 01:47:02.033391: val_loss -0.1893 
2025-03-13 01:47:02.033542: Pseudo dice [0.8257, 0.8278, 0.9033, 0.8701, 0.8923] 
2025-03-13 01:47:02.033647: Epoch time: 215.53 s 
2025-03-13 01:47:02.033709: Yayy! New best EMA pseudo Dice: 0.805 
2025-03-13 01:47:06.462400:  
2025-03-13 01:47:06.462784: Epoch 40 
2025-03-13 01:47:06.463020: Current learning rate: 0.00818 
2025-03-13 01:50:41.595087: train_loss -0.7219 
2025-03-13 01:50:41.595603: val_loss -0.2249 
2025-03-13 01:50:41.595815: Pseudo dice [0.8273, 0.8165, 0.9019, 0.8803, 0.9273] 
2025-03-13 01:50:41.595915: Epoch time: 215.14 s 
2025-03-13 01:50:41.595981: Yayy! New best EMA pseudo Dice: 0.8115 
2025-03-13 01:50:46.240951:  
2025-03-13 01:50:46.241322: Epoch 41 
2025-03-13 01:50:46.241499: Current learning rate: 0.00813 
2025-03-13 01:54:21.390601: train_loss -0.7495 
2025-03-13 01:54:21.391020: val_loss -0.2453 
2025-03-13 01:54:21.391175: Pseudo dice [0.853, 0.8241, 0.9032, 0.8703, 0.9453] 
2025-03-13 01:54:21.391275: Epoch time: 215.15 s 
2025-03-13 01:54:21.391339: Yayy! New best EMA pseudo Dice: 0.8183 
2025-03-13 01:54:24.688786:  
2025-03-13 01:54:24.689118: Epoch 42 
2025-03-13 01:54:24.689350: Current learning rate: 0.00809 
2025-03-13 01:57:59.630661: train_loss -0.7502 
2025-03-13 01:57:59.631083: val_loss -0.2458 
2025-03-13 01:57:59.631278: Pseudo dice [0.8475, 0.8266, 0.9114, 0.8987, 0.8988] 
2025-03-13 01:57:59.631443: Epoch time: 214.95 s 
2025-03-13 01:57:59.631517: Yayy! New best EMA pseudo Dice: 0.8241 
2025-03-13 01:58:04.305901:  
2025-03-13 01:58:04.306291: Epoch 43 
2025-03-13 01:58:04.306746: Current learning rate: 0.00804 
2025-03-13 02:01:39.631381: train_loss -0.7642 
2025-03-13 02:01:39.632026: val_loss -0.2181 
2025-03-13 02:01:39.632313: Pseudo dice [0.8281, 0.8399, 0.9033, 0.9157, 0.8895] 
2025-03-13 02:01:39.632569: Epoch time: 215.33 s 
2025-03-13 02:01:39.632697: Yayy! New best EMA pseudo Dice: 0.8293 
2025-03-13 02:01:43.938828:  
2025-03-13 02:01:43.939106: Epoch 44 
2025-03-13 02:01:43.939396: Current learning rate: 0.008 
2025-03-13 02:05:19.200681: train_loss -0.7613 
2025-03-13 02:05:19.201058: val_loss -0.234 
2025-03-13 02:05:19.201260: Pseudo dice [0.834, 0.8425, 0.9093, 0.8932, 0.854] 
2025-03-13 02:05:19.201393: Epoch time: 215.27 s 
2025-03-13 02:05:19.201486: Yayy! New best EMA pseudo Dice: 0.833 
2025-03-13 02:05:23.730308:  
2025-03-13 02:05:23.730789: Epoch 45 
2025-03-13 02:05:23.731045: Current learning rate: 0.00795 
2025-03-13 02:08:58.954829: train_loss -0.7749 
2025-03-13 02:08:58.955338: val_loss -0.2629 
2025-03-13 02:08:58.955695: Pseudo dice [0.8461, 0.8386, 0.9147, 0.8849, 0.7567] 
2025-03-13 02:08:58.955864: Epoch time: 215.23 s 
2025-03-13 02:08:58.955932: Yayy! New best EMA pseudo Dice: 0.8345 
2025-03-13 02:09:02.438760:  
2025-03-13 02:09:02.439137: Epoch 46 
2025-03-13 02:09:02.439447: Current learning rate: 0.0079 
2025-03-13 02:12:37.738625: train_loss -0.773 
2025-03-13 02:12:37.738991: val_loss -0.2735 
2025-03-13 02:12:37.739121: Pseudo dice [0.8586, 0.8427, 0.9181, 0.8855, 0.9089] 
2025-03-13 02:12:37.739345: Epoch time: 215.3 s 
2025-03-13 02:12:37.739487: Yayy! New best EMA pseudo Dice: 0.8393 
2025-03-13 02:12:42.376380:  
2025-03-13 02:12:42.376786: Epoch 47 
2025-03-13 02:12:42.377005: Current learning rate: 0.00786 
2025-03-13 02:16:17.662468: train_loss -0.7603 
2025-03-13 02:16:17.663006: val_loss -0.2536 
2025-03-13 02:16:17.663141: Pseudo dice [0.8325, 0.8306, 0.9003, 0.8842, 0.9183] 
2025-03-13 02:16:17.663238: Epoch time: 215.29 s 
2025-03-13 02:16:17.663304: Yayy! New best EMA pseudo Dice: 0.8427 
2025-03-13 02:16:22.527590:  
2025-03-13 02:16:22.527906: Epoch 48 
2025-03-13 02:16:22.528200: Current learning rate: 0.00781 
2025-03-13 02:19:58.161883: train_loss -0.7627 
2025-03-13 02:19:58.162301: val_loss -0.2611 
2025-03-13 02:19:58.162430: Pseudo dice [0.856, 0.8591, 0.9153, 0.8829, 0.7713] 
2025-03-13 02:19:58.162526: Epoch time: 215.64 s 
2025-03-13 02:19:58.162597: Yayy! New best EMA pseudo Dice: 0.8441 
2025-03-13 02:20:02.872957:  
2025-03-13 02:20:02.873802: Epoch 49 
2025-03-13 02:20:02.874294: Current learning rate: 0.00777 
2025-03-13 02:23:38.928129: train_loss -0.7757 
2025-03-13 02:23:38.928646: val_loss -0.1929 
2025-03-13 02:23:38.928860: Pseudo dice [0.828, 0.8393, 0.9111, 0.8982, 0.9312] 
2025-03-13 02:23:38.929012: Epoch time: 216.06 s 
2025-03-13 02:23:41.759220: Yayy! New best EMA pseudo Dice: 0.8479 
2025-03-13 02:23:46.497237:  
2025-03-13 02:23:46.497485: Epoch 50 
2025-03-13 02:23:46.497682: Current learning rate: 0.00772 
2025-03-13 02:27:21.842079: train_loss -0.7723 
2025-03-13 02:27:21.842583: val_loss -0.2434 
2025-03-13 02:27:21.842846: Pseudo dice [0.853, 0.8416, 0.9159, 0.9091, 0.9372] 
2025-03-13 02:27:21.843005: Epoch time: 215.35 s 
2025-03-13 02:27:21.843154: Yayy! New best EMA pseudo Dice: 0.8522 
2025-03-13 02:27:26.330811:  
2025-03-13 02:27:26.331181: Epoch 51 
2025-03-13 02:27:26.331372: Current learning rate: 0.00767 
2025-03-13 02:31:01.842119: train_loss -0.7905 
2025-03-13 02:31:01.842583: val_loss -0.2906 
2025-03-13 02:31:01.842840: Pseudo dice [0.8665, 0.852, 0.927, 0.89, 0.8058] 
2025-03-13 02:31:01.842975: Epoch time: 215.52 s 
2025-03-13 02:31:01.843074: Yayy! New best EMA pseudo Dice: 0.8538 
2025-03-13 02:31:06.348254:  
2025-03-13 02:31:06.348745: Epoch 52 
2025-03-13 02:31:06.349114: Current learning rate: 0.00763 
2025-03-13 02:34:41.569075: train_loss -0.7965 
2025-03-13 02:34:41.569545: val_loss -0.2996 
2025-03-13 02:34:41.569773: Pseudo dice [0.8639, 0.8652, 0.9245, 0.9191, 0.9076] 
2025-03-13 02:34:41.570136: Epoch time: 215.23 s 
2025-03-13 02:34:41.570383: Yayy! New best EMA pseudo Dice: 0.8581 
2025-03-13 02:34:45.064860:  
2025-03-13 02:34:45.065241: Epoch 53 
2025-03-13 02:34:45.065525: Current learning rate: 0.00758 
2025-03-13 02:38:21.434370: train_loss -0.7974 
2025-03-13 02:38:21.435045: val_loss -0.2428 
2025-03-13 02:38:21.435358: Pseudo dice [0.8469, 0.8491, 0.921, 0.9016, 0.9209] 
2025-03-13 02:38:21.435614: Epoch time: 216.37 s 
2025-03-13 02:38:21.435698: Yayy! New best EMA pseudo Dice: 0.861 
2025-03-13 02:38:24.915997:  
2025-03-13 02:38:24.916382: Epoch 54 
2025-03-13 02:38:24.916574: Current learning rate: 0.00753 
2025-03-13 02:42:00.874414: train_loss -0.791 
2025-03-13 02:42:00.874819: val_loss -0.2149 
2025-03-13 02:42:00.875028: Pseudo dice [0.8563, 0.8528, 0.9147, 0.903, 0.806] 
2025-03-13 02:42:00.875186: Epoch time: 215.96 s 
2025-03-13 02:42:00.875258: Yayy! New best EMA pseudo Dice: 0.8616 
2025-03-13 02:42:05.853993:  
2025-03-13 02:42:05.854338: Epoch 55 
2025-03-13 02:42:05.854781: Current learning rate: 0.00749 
2025-03-13 02:45:41.827094: train_loss -0.7903 
2025-03-13 02:45:41.827463: val_loss -0.3383 
2025-03-13 02:45:41.827618: Pseudo dice [0.8719, 0.8549, 0.929, 0.9209, 0.9364] 
2025-03-13 02:45:41.827718: Epoch time: 215.98 s 
2025-03-13 02:45:41.827785: Yayy! New best EMA pseudo Dice: 0.8657 
2025-03-13 02:45:46.308939:  
2025-03-13 02:45:46.309285: Epoch 56 
2025-03-13 02:45:46.309464: Current learning rate: 0.00744 
2025-03-13 02:49:22.296861: train_loss -0.7855 
2025-03-13 02:49:22.297260: val_loss -0.1934 
2025-03-13 02:49:22.297464: Pseudo dice [0.8305, 0.8732, 0.9095, 0.8911, 0.8858] 
2025-03-13 02:49:22.297589: Epoch time: 215.99 s 
2025-03-13 02:49:22.297655: Yayy! New best EMA pseudo Dice: 0.8669 
2025-03-13 02:49:26.777691:  
2025-03-13 02:49:26.778352: Epoch 57 
2025-03-13 02:49:26.778995: Current learning rate: 0.00739 
2025-03-13 02:53:02.675743: train_loss -0.8017 
2025-03-13 02:53:02.676162: val_loss -0.318 
2025-03-13 02:53:02.676350: Pseudo dice [0.869, 0.8801, 0.9294, 0.9165, 0.883] 
2025-03-13 02:53:02.676518: Epoch time: 215.92 s 
2025-03-13 02:53:02.676590: Yayy! New best EMA pseudo Dice: 0.8698 
2025-03-13 02:53:05.928087:  
2025-03-13 02:53:05.928497: Epoch 58 
2025-03-13 02:53:05.928734: Current learning rate: 0.00735 
2025-03-13 02:56:43.198950: train_loss -0.7815 
2025-03-13 02:56:43.199503: val_loss -0.2557 
2025-03-13 02:56:43.199703: Pseudo dice [0.8558, 0.8511, 0.9176, 0.9049, 0.854] 
2025-03-13 02:56:43.199799: Epoch time: 217.28 s 
2025-03-13 02:56:43.199865: Yayy! New best EMA pseudo Dice: 0.8705 
2025-03-13 02:56:48.002373:  
2025-03-13 02:56:48.002716: Epoch 59 
2025-03-13 02:56:48.002889: Current learning rate: 0.0073 
2025-03-13 03:00:24.305105: train_loss -0.7968 
2025-03-13 03:00:24.305444: val_loss -0.2374 
2025-03-13 03:00:24.305690: Pseudo dice [0.8498, 0.8609, 0.9156, 0.8994, 0.9293] 
2025-03-13 03:00:24.305857: Epoch time: 216.31 s 
2025-03-13 03:00:24.306240: Yayy! New best EMA pseudo Dice: 0.8725 
2025-03-13 03:00:28.745414:  
2025-03-13 03:00:28.745724: Epoch 60 
2025-03-13 03:00:28.745880: Current learning rate: 0.00725 
2025-03-13 03:04:04.653702: train_loss -0.7972 
2025-03-13 03:04:04.654031: val_loss -0.2376 
2025-03-13 03:04:04.654174: Pseudo dice [0.8502, 0.8753, 0.9171, 0.9123, 0.7423] 
2025-03-13 03:04:04.654264: Epoch time: 215.91 s 
2025-03-13 03:04:07.181529:  
2025-03-13 03:04:07.182026: Epoch 61 
2025-03-13 03:04:07.182770: Current learning rate: 0.00721 
2025-03-13 03:07:43.249944: train_loss -0.8212 
2025-03-13 03:07:43.250534: val_loss -0.273 
2025-03-13 03:07:43.250858: Pseudo dice [0.8629, 0.8765, 0.9248, 0.9062, 0.8263] 
2025-03-13 03:07:43.251005: Epoch time: 216.08 s 
2025-03-13 03:07:45.017473:  
2025-03-13 03:07:45.017885: Epoch 62 
2025-03-13 03:07:45.018133: Current learning rate: 0.00716 
2025-03-13 03:11:20.901377: train_loss -0.7878 
2025-03-13 03:11:20.901846: val_loss -0.1465 
2025-03-13 03:11:20.901981: Pseudo dice [0.8251, 0.8468, 0.9124, 0.8966, 0.4924] 
2025-03-13 03:11:20.902140: Epoch time: 215.89 s 
2025-03-13 03:11:22.665794:  
2025-03-13 03:11:22.666129: Epoch 63 
2025-03-13 03:11:22.666307: Current learning rate: 0.00711 
2025-03-13 03:14:59.834316: train_loss -0.7913 
2025-03-13 03:14:59.835114: val_loss -0.2162 
2025-03-13 03:14:59.835370: Pseudo dice [0.8448, 0.8442, 0.9203, 0.9248, 0.5487] 
2025-03-13 03:14:59.835523: Epoch time: 217.17 s 
2025-03-13 03:15:01.605876:  
2025-03-13 03:15:01.606124: Epoch 64 
2025-03-13 03:15:01.606352: Current learning rate: 0.00707 
2025-03-13 03:18:38.088460: train_loss -0.7934 
2025-03-13 03:18:38.088810: val_loss -0.1815 
2025-03-13 03:18:38.088937: Pseudo dice [0.8365, 0.8461, 0.9159, 0.9269, 0.8895] 
2025-03-13 03:18:38.089039: Epoch time: 216.49 s 
2025-03-13 03:18:39.788491:  
2025-03-13 03:18:39.788847: Epoch 65 
2025-03-13 03:18:39.789019: Current learning rate: 0.00702 
2025-03-13 03:22:15.618306: train_loss -0.8066 
2025-03-13 03:22:15.618895: val_loss -0.2806 
2025-03-13 03:22:15.619068: Pseudo dice [0.8541, 0.8642, 0.9248, 0.9079, 0.8823] 
2025-03-13 03:22:15.619168: Epoch time: 215.84 s 
2025-03-13 03:22:17.326623:  
2025-03-13 03:22:17.326893: Epoch 66 
2025-03-13 03:22:17.327061: Current learning rate: 0.00697 
2025-03-13 03:25:53.256136: train_loss -0.813 
2025-03-13 03:25:53.256715: val_loss -0.3253 
2025-03-13 03:25:53.256916: Pseudo dice [0.8734, 0.8778, 0.9295, 0.9255, 0.9201] 
2025-03-13 03:25:53.257075: Epoch time: 215.94 s 
2025-03-13 03:25:54.958309:  
2025-03-13 03:25:54.958656: Epoch 67 
2025-03-13 03:25:54.958830: Current learning rate: 0.00693 
2025-03-13 03:29:31.151500: train_loss -0.8072 
2025-03-13 03:29:31.152106: val_loss -0.3403 
2025-03-13 03:29:31.152530: Pseudo dice [0.8753, 0.8753, 0.9369, 0.9282, 0.9513] 
2025-03-13 03:29:31.152725: Epoch time: 216.2 s 
2025-03-13 03:29:31.152829: Yayy! New best EMA pseudo Dice: 0.8729 
2025-03-13 03:29:35.698924:  
2025-03-13 03:29:35.699541: Epoch 68 
2025-03-13 03:29:35.699969: Current learning rate: 0.00688 
2025-03-13 03:33:12.775852: train_loss -0.8072 
2025-03-13 03:33:12.776350: val_loss -0.2573 
2025-03-13 03:33:12.776697: Pseudo dice [0.8604, 0.8839, 0.9255, 0.9263, 0.7185] 
2025-03-13 03:33:12.776803: Epoch time: 217.08 s 
2025-03-13 03:33:14.673665:  
2025-03-13 03:33:14.674055: Epoch 69 
2025-03-13 03:33:14.674274: Current learning rate: 0.00683 
2025-03-13 03:36:50.926068: train_loss -0.8174 
2025-03-13 03:36:50.926445: val_loss -0.3229 
2025-03-13 03:36:50.926676: Pseudo dice [0.8775, 0.8805, 0.934, 0.925, 0.787] 
2025-03-13 03:36:50.926844: Epoch time: 216.26 s 
2025-03-13 03:36:52.663408:  
2025-03-13 03:36:52.663680: Epoch 70 
2025-03-13 03:36:52.663870: Current learning rate: 0.00679 
2025-03-13 03:40:28.445416: train_loss -0.8182 
2025-03-13 03:40:28.445845: val_loss -0.2954 
2025-03-13 03:40:28.446091: Pseudo dice [0.8734, 0.8769, 0.9324, 0.9263, 0.7283] 
2025-03-13 03:40:28.446291: Epoch time: 215.79 s 
2025-03-13 03:40:30.247800:  
2025-03-13 03:40:30.248116: Epoch 71 
2025-03-13 03:40:30.248277: Current learning rate: 0.00674 
2025-03-13 03:44:06.272104: train_loss -0.8074 
2025-03-13 03:44:06.272516: val_loss -0.264 
2025-03-13 03:44:06.272712: Pseudo dice [0.8508, 0.8779, 0.9253, 0.9202, 0.9317] 
2025-03-13 03:44:06.272856: Epoch time: 216.03 s 
2025-03-13 03:44:06.272925: Yayy! New best EMA pseudo Dice: 0.8752 
2025-03-13 03:44:10.770372:  
2025-03-13 03:44:10.770746: Epoch 72 
2025-03-13 03:44:10.770940: Current learning rate: 0.00669 
2025-03-13 03:47:46.764145: train_loss -0.8253 
2025-03-13 03:47:46.764581: val_loss -0.2593 
2025-03-13 03:47:46.764787: Pseudo dice [0.8614, 0.8816, 0.9235, 0.9243, 0.6788] 
2025-03-13 03:47:46.764912: Epoch time: 216.0 s 
2025-03-13 03:47:48.556782:  
2025-03-13 03:47:48.557205: Epoch 73 
2025-03-13 03:47:48.557625: Current learning rate: 0.00665 
2025-03-13 03:51:26.308815: train_loss -0.8141 
2025-03-13 03:51:26.309470: val_loss -0.3101 
2025-03-13 03:51:26.309741: Pseudo dice [0.8526, 0.887, 0.9253, 0.9233, 0.7819] 
2025-03-13 03:51:26.309942: Epoch time: 217.76 s 
2025-03-13 03:51:28.138978:  
2025-03-13 03:51:28.139211: Epoch 74 
2025-03-13 03:51:28.139741: Current learning rate: 0.0066 
2025-03-13 03:55:04.663003: train_loss -0.8228 
2025-03-13 03:55:04.663372: val_loss -0.2227 
2025-03-13 03:55:04.663594: Pseudo dice [0.8588, 0.8809, 0.9184, 0.9206, 0.9161] 
2025-03-13 03:55:04.663959: Epoch time: 216.53 s 
2025-03-13 03:55:04.664144: Yayy! New best EMA pseudo Dice: 0.8757 
2025-03-13 03:55:09.072752:  
2025-03-13 03:55:09.073094: Epoch 75 
2025-03-13 03:55:09.073356: Current learning rate: 0.00655 
2025-03-13 03:58:45.120374: train_loss -0.8079 
2025-03-13 03:58:45.120783: val_loss -0.2203 
2025-03-13 03:58:45.121114: Pseudo dice [0.841, 0.8913, 0.9198, 0.9106, 0.7834] 
2025-03-13 03:58:45.121329: Epoch time: 216.05 s 
2025-03-13 03:58:46.904307:  
2025-03-13 03:58:46.904649: Epoch 76 
2025-03-13 03:58:46.904892: Current learning rate: 0.0065 
2025-03-13 04:02:23.044730: train_loss -0.8311 
2025-03-13 04:02:23.045199: val_loss -0.2751 
2025-03-13 04:02:23.045428: Pseudo dice [0.855, 0.8854, 0.9304, 0.9264, 0.7511] 
2025-03-13 04:02:23.045587: Epoch time: 216.15 s 
2025-03-13 04:02:24.848398:  
2025-03-13 04:02:24.848727: Epoch 77 
2025-03-13 04:02:24.848907: Current learning rate: 0.00646 
2025-03-13 04:06:00.627958: train_loss -0.8187 
2025-03-13 04:06:00.628293: val_loss -0.2907 
2025-03-13 04:06:00.628432: Pseudo dice [0.866, 0.885, 0.926, 0.9137, 0.8653] 
2025-03-13 04:06:00.628526: Epoch time: 215.79 s 
2025-03-13 04:06:00.628599: Yayy! New best EMA pseudo Dice: 0.8762 
2025-03-13 04:06:04.146719:  
2025-03-13 04:06:04.146994: Epoch 78 
2025-03-13 04:06:04.147222: Current learning rate: 0.00641 
2025-03-13 04:09:40.694133: train_loss -0.8211 
2025-03-13 04:09:40.694672: val_loss -0.2835 
2025-03-13 04:09:40.694885: Pseudo dice [0.8562, 0.8836, 0.9262, 0.9239, 0.8874] 
2025-03-13 04:09:40.694986: Epoch time: 216.55 s 
2025-03-13 04:09:40.695051: Yayy! New best EMA pseudo Dice: 0.8781 
2025-03-13 04:09:45.285667:  
2025-03-13 04:09:45.286041: Epoch 79 
2025-03-13 04:09:45.286365: Current learning rate: 0.00636 
2025-03-13 04:13:22.114044: train_loss -0.8237 
2025-03-13 04:13:22.114523: val_loss -0.2899 
2025-03-13 04:13:22.114728: Pseudo dice [0.8752, 0.883, 0.9247, 0.9059, 0.7364] 
2025-03-13 04:13:22.114919: Epoch time: 216.84 s 
2025-03-13 04:13:23.865297:  
2025-03-13 04:13:23.865590: Epoch 80 
2025-03-13 04:13:23.865786: Current learning rate: 0.00631 
2025-03-13 04:16:59.944709: train_loss -0.8298 
2025-03-13 04:16:59.945341: val_loss -0.2999 
2025-03-13 04:16:59.945651: Pseudo dice [0.8726, 0.8923, 0.9296, 0.9168, 0.6764] 
2025-03-13 04:16:59.945827: Epoch time: 216.09 s 
2025-03-13 04:17:01.783518:  
2025-03-13 04:17:01.783787: Epoch 81 
2025-03-13 04:17:01.784013: Current learning rate: 0.00627 
2025-03-13 04:20:37.747325: train_loss -0.8059 
2025-03-13 04:20:37.747824: val_loss -0.1639 
2025-03-13 04:20:37.748071: Pseudo dice [0.8242, 0.8527, 0.9026, 0.914, 0.8154] 
2025-03-13 04:20:37.748223: Epoch time: 215.97 s 
2025-03-13 04:20:39.490196:  
2025-03-13 04:20:39.490564: Epoch 82 
2025-03-13 04:20:39.490807: Current learning rate: 0.00622 
2025-03-13 04:24:15.357301: train_loss -0.7895 
2025-03-13 04:24:15.357660: val_loss -0.3145 
2025-03-13 04:24:15.357847: Pseudo dice [0.8669, 0.8657, 0.9248, 0.9201, 0.896] 
2025-03-13 04:24:15.357998: Epoch time: 215.87 s 
2025-03-13 04:24:17.014526:  
2025-03-13 04:24:17.014798: Epoch 83 
2025-03-13 04:24:17.015065: Current learning rate: 0.00617 
2025-03-13 04:27:53.768501: train_loss -0.8048 
2025-03-13 04:27:53.768913: val_loss -0.2955 
2025-03-13 04:27:53.769130: Pseudo dice [0.8613, 0.876, 0.9213, 0.9369, 0.8515] 
2025-03-13 04:27:53.769298: Epoch time: 216.76 s 
2025-03-13 04:27:55.600737:  
2025-03-13 04:27:55.601085: Epoch 84 
2025-03-13 04:27:55.601245: Current learning rate: 0.00612 
2025-03-13 04:31:31.505132: train_loss -0.7778 
2025-03-13 04:31:31.505507: val_loss -0.0613 
2025-03-13 04:31:31.505694: Pseudo dice [0.8055, 0.8296, 0.8876, 0.8955, 0.0397] 
2025-03-13 04:31:31.505854: Epoch time: 215.91 s 
2025-03-13 04:31:33.180319:  
2025-03-13 04:31:33.180561: Epoch 85 
2025-03-13 04:31:33.180807: Current learning rate: 0.00608 
2025-03-13 04:35:08.883286: train_loss -0.7277 
2025-03-13 04:35:08.883851: val_loss -0.0776 
2025-03-13 04:35:08.884073: Pseudo dice [0.747, 0.8089, 0.893, 0.8707, 0.911] 
2025-03-13 04:35:08.884266: Epoch time: 215.71 s 
2025-03-13 04:35:10.574638:  
2025-03-13 04:35:10.575065: Epoch 86 
2025-03-13 04:35:10.575280: Current learning rate: 0.00603 
2025-03-13 04:38:46.828439: train_loss -0.7101 
2025-03-13 04:38:46.828781: val_loss -0.2026 
2025-03-13 04:38:46.828931: Pseudo dice [0.8503, 0.8203, 0.8957, 0.871, 0.9076] 
2025-03-13 04:38:46.829029: Epoch time: 216.27 s 
2025-03-13 04:38:48.530937:  
2025-03-13 04:38:48.531224: Epoch 87 
2025-03-13 04:38:48.531436: Current learning rate: 0.00598 
2025-03-13 04:42:24.459013: train_loss -0.7234 
2025-03-13 04:42:24.459506: val_loss -0.2078 
2025-03-13 04:42:24.459760: Pseudo dice [0.8196, 0.8185, 0.914, 0.9197, 0.9198] 
2025-03-13 04:42:24.459927: Epoch time: 215.93 s 
2025-03-13 04:42:26.255414:  
2025-03-13 04:42:26.255710: Epoch 88 
2025-03-13 04:42:26.255923: Current learning rate: 0.00593 
2025-03-13 04:46:03.393178: train_loss -0.7293 
2025-03-13 04:46:03.393846: val_loss -0.1524 
2025-03-13 04:46:03.394384: Pseudo dice [0.8113, 0.8085, 0.8848, 0.7966, 0.8683] 
2025-03-13 04:46:03.394703: Epoch time: 217.14 s 
2025-03-13 04:46:05.139336:  
2025-03-13 04:46:05.139652: Epoch 89 
2025-03-13 04:46:05.139844: Current learning rate: 0.00589 
2025-03-13 04:49:40.951365: train_loss -0.69 
2025-03-13 04:49:40.952036: val_loss -0.3418 
2025-03-13 04:49:40.952317: Pseudo dice [0.8573, 0.8335, 0.9251, 0.8933, 0.9692] 
2025-03-13 04:49:40.952499: Epoch time: 215.82 s 
2025-03-13 04:49:42.602142:  
2025-03-13 04:49:42.602389: Epoch 90 
2025-03-13 04:49:42.602513: Current learning rate: 0.00584 
2025-03-13 04:53:18.138392: train_loss -0.768 
2025-03-13 04:53:18.138791: val_loss -0.3041 
2025-03-13 04:53:18.138956: Pseudo dice [0.8589, 0.8678, 0.9224, 0.9063, 0.9523] 
2025-03-13 04:53:18.139051: Epoch time: 215.54 s 
2025-03-13 04:53:19.954654:  
2025-03-13 04:53:19.955012: Epoch 91 
2025-03-13 04:53:19.955273: Current learning rate: 0.00579 
2025-03-13 04:56:55.580193: train_loss -0.7063 
2025-03-13 04:56:55.580808: val_loss -0.2092 
2025-03-13 04:56:55.581015: Pseudo dice [0.8322, 0.7857, 0.899, 0.8829, 0.8839] 
2025-03-13 04:56:55.581177: Epoch time: 215.63 s 
2025-03-13 04:56:57.243254:  
2025-03-13 04:56:57.243657: Epoch 92 
2025-03-13 04:56:57.243908: Current learning rate: 0.00574 
2025-03-13 05:00:32.739404: train_loss -0.7785 
2025-03-13 05:00:32.740539: val_loss -0.2571 
2025-03-13 05:00:32.741198: Pseudo dice [0.8564, 0.8398, 0.9114, 0.9051, 0.9476] 
2025-03-13 05:00:32.743573: Epoch time: 215.5 s 
2025-03-13 05:00:35.180489:  
2025-03-13 05:00:35.180826: Epoch 93 
2025-03-13 05:00:35.181135: Current learning rate: 0.0057 
2025-03-13 05:04:11.610123: train_loss -0.7931 
2025-03-13 05:04:11.610673: val_loss -0.2658 
2025-03-13 05:04:11.610844: Pseudo dice [0.8581, 0.8732, 0.9241, 0.9122, 0.9416] 
2025-03-13 05:04:11.610937: Epoch time: 216.43 s 
2025-03-13 05:04:13.283091:  
2025-03-13 05:04:13.284083: Epoch 94 
2025-03-13 05:04:13.284302: Current learning rate: 0.00565 
2025-03-13 05:07:48.852106: train_loss -0.8054 
2025-03-13 05:07:48.852445: val_loss -0.2729 
2025-03-13 05:07:48.852769: Pseudo dice [0.8487, 0.8702, 0.9101, 0.9336, 0.9597] 
2025-03-13 05:07:48.852975: Epoch time: 215.58 s 
2025-03-13 05:07:50.500325:  
2025-03-13 05:07:50.500892: Epoch 95 
2025-03-13 05:07:50.501371: Current learning rate: 0.0056 
2025-03-13 05:11:25.946460: train_loss -0.7902 
2025-03-13 05:11:25.946966: val_loss -0.3166 
2025-03-13 05:11:25.947169: Pseudo dice [0.8528, 0.8786, 0.9211, 0.9387, 0.9474] 
2025-03-13 05:11:25.947307: Epoch time: 215.45 s 
2025-03-13 05:11:27.670112:  
2025-03-13 05:11:27.670446: Epoch 96 
2025-03-13 05:11:27.670650: Current learning rate: 0.00555 
2025-03-13 05:15:03.097076: train_loss -0.8099 
2025-03-13 05:15:03.097454: val_loss -0.2784 
2025-03-13 05:15:03.097628: Pseudo dice [0.8459, 0.875, 0.9217, 0.9219, 0.9718] 
2025-03-13 05:15:03.097753: Epoch time: 215.43 s 
2025-03-13 05:15:03.097820: Yayy! New best EMA pseudo Dice: 0.8805 
2025-03-13 05:15:07.704838:  
2025-03-13 05:15:07.705196: Epoch 97 
2025-03-13 05:15:07.705384: Current learning rate: 0.0055 
2025-03-13 05:18:43.447576: train_loss -0.8178 
2025-03-13 05:18:43.448026: val_loss -0.283 
2025-03-13 05:18:43.448195: Pseudo dice [0.8703, 0.8746, 0.9265, 0.9205, 0.9381] 
2025-03-13 05:18:43.448289: Epoch time: 215.75 s 
2025-03-13 05:18:43.448398: Yayy! New best EMA pseudo Dice: 0.8831 
2025-03-13 05:18:47.885731:  
2025-03-13 05:18:47.886040: Epoch 98 
2025-03-13 05:18:47.886206: Current learning rate: 0.00546 
2025-03-13 05:22:24.558634: train_loss -0.8061 
2025-03-13 05:22:24.559351: val_loss -0.3336 
2025-03-13 05:22:24.559629: Pseudo dice [0.8833, 0.8908, 0.9304, 0.9139, 0.9353] 
2025-03-13 05:22:24.559778: Epoch time: 216.68 s 
2025-03-13 05:22:24.559888: Yayy! New best EMA pseudo Dice: 0.8859 
2025-03-13 05:22:28.963433:  
2025-03-13 05:22:28.963712: Epoch 99 
2025-03-13 05:22:28.963939: Current learning rate: 0.00541 
2025-03-13 05:26:04.932507: train_loss -0.824 
2025-03-13 05:26:04.932955: val_loss -0.3272 
2025-03-13 05:26:04.933218: Pseudo dice [0.8712, 0.8947, 0.9321, 0.9333, 0.9375] 
2025-03-13 05:26:04.933316: Epoch time: 215.97 s 
2025-03-13 05:26:08.771528: Yayy! New best EMA pseudo Dice: 0.8886 
2025-03-13 05:26:13.198579:  
2025-03-13 05:26:13.198924: Epoch 100 
2025-03-13 05:26:13.199157: Current learning rate: 0.00536 
2025-03-13 05:29:48.495073: train_loss -0.8308 
2025-03-13 05:29:48.495566: val_loss -0.3524 
2025-03-13 05:29:48.495785: Pseudo dice [0.8807, 0.9056, 0.9304, 0.9364, 0.9485] 
2025-03-13 05:29:48.495939: Epoch time: 215.3 s 
2025-03-13 05:29:48.496006: Yayy! New best EMA pseudo Dice: 0.8918 
2025-03-13 05:29:52.934034:  
2025-03-13 05:29:52.934400: Epoch 101 
2025-03-13 05:29:52.934557: Current learning rate: 0.00531 
2025-03-13 05:33:28.434489: train_loss -0.8229 
2025-03-13 05:33:28.434926: val_loss -0.3152 
2025-03-13 05:33:28.435059: Pseudo dice [0.8733, 0.8923, 0.9292, 0.9444, 0.9387] 
2025-03-13 05:33:28.435153: Epoch time: 215.51 s 
2025-03-13 05:33:28.435215: Yayy! New best EMA pseudo Dice: 0.8942 
2025-03-13 05:33:33.974317:  
2025-03-13 05:33:33.974733: Epoch 102 
2025-03-13 05:33:33.974958: Current learning rate: 0.00526 
2025-03-13 05:37:10.059766: train_loss -0.8138 
2025-03-13 05:37:10.060230: val_loss -0.3421 
2025-03-13 05:37:10.060434: Pseudo dice [0.873, 0.8827, 0.9361, 0.9265, 0.9495] 
2025-03-13 05:37:10.060613: Epoch time: 216.09 s 
2025-03-13 05:37:10.060680: Yayy! New best EMA pseudo Dice: 0.8961 
2025-03-13 05:37:13.565419:  
2025-03-13 05:37:13.565658: Epoch 103 
2025-03-13 05:37:13.565815: Current learning rate: 0.00521 
2025-03-13 05:40:50.070668: train_loss -0.8217 
2025-03-13 05:40:50.071148: val_loss -0.198 
2025-03-13 05:40:50.071340: Pseudo dice [0.8461, 0.869, 0.9139, 0.9269, 0.945] 
2025-03-13 05:40:50.071490: Epoch time: 216.52 s 
2025-03-13 05:40:50.071642: Yayy! New best EMA pseudo Dice: 0.8965 
2025-03-13 05:40:54.433818:  
2025-03-13 05:40:54.434141: Epoch 104 
2025-03-13 05:40:54.434306: Current learning rate: 0.00517 
2025-03-13 05:44:30.346314: train_loss -0.8043 
2025-03-13 05:44:30.347164: val_loss -0.3429 
2025-03-13 05:44:30.347533: Pseudo dice [0.8768, 0.8465, 0.9199, 0.9146, 0.8888] 
2025-03-13 05:44:30.347654: Epoch time: 215.92 s 
2025-03-13 05:44:32.135802:  
2025-03-13 05:44:32.136217: Epoch 105 
2025-03-13 05:44:32.136800: Current learning rate: 0.00512 
2025-03-13 05:48:07.722437: train_loss -0.7803 
2025-03-13 05:48:07.722993: val_loss -0.2822 
2025-03-13 05:48:07.723204: Pseudo dice [0.8634, 0.874, 0.9216, 0.9179, 0.6183] 
2025-03-13 05:48:07.723375: Epoch time: 215.59 s 
2025-03-13 05:48:09.509711:  
2025-03-13 05:48:09.510076: Epoch 106 
2025-03-13 05:48:09.510514: Current learning rate: 0.00507 
2025-03-13 05:51:44.946182: train_loss -0.8089 
2025-03-13 05:51:44.946592: val_loss -0.2811 
2025-03-13 05:51:44.946884: Pseudo dice [0.8436, 0.8871, 0.9236, 0.922, 0.9055] 
2025-03-13 05:51:44.947023: Epoch time: 215.44 s 
2025-03-13 05:51:46.606739:  
2025-03-13 05:51:46.607053: Epoch 107 
2025-03-13 05:51:46.607235: Current learning rate: 0.00502 
2025-03-13 05:55:22.310820: train_loss -0.7977 
2025-03-13 05:55:22.311404: val_loss -0.3135 
2025-03-13 05:55:22.311709: Pseudo dice [0.8691, 0.8611, 0.9253, 0.9062, 0.9374] 
2025-03-13 05:55:22.311824: Epoch time: 215.71 s 
2025-03-13 05:55:24.068585:  
2025-03-13 05:55:24.068931: Epoch 108 
2025-03-13 05:55:24.069185: Current learning rate: 0.00497 
2025-03-13 05:59:00.799913: train_loss -0.8076 
2025-03-13 05:59:00.800299: val_loss -0.3673 
2025-03-13 05:59:00.800514: Pseudo dice [0.8767, 0.8913, 0.9343, 0.9142, 0.8409] 
2025-03-13 05:59:00.800628: Epoch time: 216.74 s 
2025-03-13 05:59:02.831532:  
2025-03-13 05:59:02.831865: Epoch 109 
2025-03-13 05:59:02.832045: Current learning rate: 0.00492 
2025-03-13 06:02:38.158126: train_loss -0.8116 
2025-03-13 06:02:38.158543: val_loss -0.3437 
2025-03-13 06:02:38.158754: Pseudo dice [0.8811, 0.8977, 0.9307, 0.9103, 0.9465] 
2025-03-13 06:02:38.158914: Epoch time: 215.33 s 
2025-03-13 06:02:39.888538:  
2025-03-13 06:02:39.888878: Epoch 110 
2025-03-13 06:02:39.889088: Current learning rate: 0.00487 
2025-03-13 06:06:15.445613: train_loss -0.7405 
2025-03-13 06:06:15.446295: val_loss -0.29 
2025-03-13 06:06:15.446673: Pseudo dice [0.8574, 0.8539, 0.9196, 0.9097, 0.7383] 
2025-03-13 06:06:15.446811: Epoch time: 215.56 s 
2025-03-13 06:06:17.178843:  
2025-03-13 06:06:17.179833: Epoch 111 
2025-03-13 06:06:17.179973: Current learning rate: 0.00483 
2025-03-13 06:09:53.134841: train_loss -0.7665 
2025-03-13 06:09:53.135592: val_loss -0.2852 
2025-03-13 06:09:53.135895: Pseudo dice [0.8797, 0.8665, 0.9091, 0.9049, 0.9466] 
2025-03-13 06:09:53.136084: Epoch time: 215.96 s 
2025-03-13 06:09:54.785219:  
2025-03-13 06:09:54.785519: Epoch 112 
2025-03-13 06:09:54.785820: Current learning rate: 0.00478 
2025-03-13 06:13:30.414909: train_loss -0.7974 
2025-03-13 06:13:30.415321: val_loss -0.2589 
2025-03-13 06:13:30.415498: Pseudo dice [0.8548, 0.8524, 0.9139, 0.9357, 0.9346] 
2025-03-13 06:13:30.415671: Epoch time: 215.64 s 
2025-03-13 06:13:32.220680:  
2025-03-13 06:13:32.220968: Epoch 113 
2025-03-13 06:13:32.221170: Current learning rate: 0.00473 
2025-03-13 06:17:08.844788: train_loss -0.8049 
2025-03-13 06:17:08.845340: val_loss -0.3511 
2025-03-13 06:17:08.845522: Pseudo dice [0.8689, 0.8958, 0.936, 0.9363, 0.9402] 
2025-03-13 06:17:08.845628: Epoch time: 216.63 s 
2025-03-13 06:17:10.546799:  
2025-03-13 06:17:10.547122: Epoch 114 
2025-03-13 06:17:10.547364: Current learning rate: 0.00468 
2025-03-13 06:20:46.161174: train_loss -0.8293 
2025-03-13 06:20:46.161762: val_loss -0.3324 
2025-03-13 06:20:46.161971: Pseudo dice [0.8781, 0.8972, 0.9277, 0.9393, 0.9372] 
2025-03-13 06:20:46.162157: Epoch time: 215.62 s 
2025-03-13 06:20:47.873833:  
2025-03-13 06:20:47.874174: Epoch 115 
2025-03-13 06:20:47.874474: Current learning rate: 0.00463 
2025-03-13 06:24:23.446038: train_loss -0.8266 
2025-03-13 06:24:23.446431: val_loss -0.2663 
2025-03-13 06:24:23.446566: Pseudo dice [0.8639, 0.8907, 0.9228, 0.9368, 0.9342] 
2025-03-13 06:24:23.446665: Epoch time: 215.58 s 
2025-03-13 06:24:23.446730: Yayy! New best EMA pseudo Dice: 0.8977 
2025-03-13 06:24:27.945391:  
2025-03-13 06:24:27.945725: Epoch 116 
2025-03-13 06:24:27.946038: Current learning rate: 0.00458 
2025-03-13 06:28:03.632084: train_loss -0.8363 
2025-03-13 06:28:03.632495: val_loss -0.3168 
2025-03-13 06:28:03.632632: Pseudo dice [0.8641, 0.8911, 0.9299, 0.9277, 0.9452] 
2025-03-13 06:28:03.632728: Epoch time: 215.7 s 
2025-03-13 06:28:03.632793: Yayy! New best EMA pseudo Dice: 0.8991 
2025-03-13 06:28:06.976619:  
2025-03-13 06:28:06.976849: Epoch 117 
2025-03-13 06:28:06.977121: Current learning rate: 0.00453 
2025-03-13 06:31:43.497746: train_loss -0.8074 
2025-03-13 06:31:43.498356: val_loss -0.2747 
2025-03-13 06:31:43.498766: Pseudo dice [0.8659, 0.8473, 0.9129, 0.8872, 0.9113] 
2025-03-13 06:31:43.499046: Epoch time: 216.53 s 
2025-03-13 06:31:45.312716:  
2025-03-13 06:31:45.313133: Epoch 118 
2025-03-13 06:31:45.313317: Current learning rate: 0.00448 
2025-03-13 06:35:22.030998: train_loss -0.8123 
2025-03-13 06:35:22.031532: val_loss -0.3977 
2025-03-13 06:35:22.031836: Pseudo dice [0.8859, 0.8955, 0.9358, 0.9252, 0.9182] 
2025-03-13 06:35:22.031993: Epoch time: 216.72 s 
2025-03-13 06:35:22.032062: Yayy! New best EMA pseudo Dice: 0.8991 
2025-03-13 06:35:25.476069:  
2025-03-13 06:35:25.476424: Epoch 119 
2025-03-13 06:35:25.476608: Current learning rate: 0.00443 
2025-03-13 06:39:01.025623: train_loss -0.8256 
2025-03-13 06:39:01.025979: val_loss -0.3553 
2025-03-13 06:39:01.026178: Pseudo dice [0.8914, 0.8847, 0.9346, 0.9403, 0.9546] 
2025-03-13 06:39:01.026338: Epoch time: 215.55 s 
2025-03-13 06:39:01.026452: Yayy! New best EMA pseudo Dice: 0.9013 
2025-03-13 06:39:06.293341:  
2025-03-13 06:39:06.293789: Epoch 120 
2025-03-13 06:39:06.293968: Current learning rate: 0.00438 
2025-03-13 06:42:41.977838: train_loss -0.8274 
2025-03-13 06:42:41.978193: val_loss -0.413 
2025-03-13 06:42:41.978324: Pseudo dice [0.8963, 0.8976, 0.9424, 0.9427, 0.9372] 
2025-03-13 06:42:41.978419: Epoch time: 215.69 s 
2025-03-13 06:42:41.978484: Yayy! New best EMA pseudo Dice: 0.9035 
2025-03-13 06:42:45.440565:  
2025-03-13 06:42:45.440969: Epoch 121 
2025-03-13 06:42:45.441237: Current learning rate: 0.00433 
2025-03-13 06:46:21.151004: train_loss -0.8244 
2025-03-13 06:46:21.151445: val_loss -0.367 
2025-03-13 06:46:21.151712: Pseudo dice [0.8793, 0.8932, 0.9393, 0.9363, 0.9712] 
2025-03-13 06:46:21.151860: Epoch time: 215.72 s 
2025-03-13 06:46:21.151929: Yayy! New best EMA pseudo Dice: 0.9056 
2025-03-13 06:46:26.284202:  
2025-03-13 06:46:26.284606: Epoch 122 
2025-03-13 06:46:26.285053: Current learning rate: 0.00429 
2025-03-13 06:50:02.750406: train_loss -0.8301 
2025-03-13 06:50:02.750757: val_loss -0.3908 
2025-03-13 06:50:02.750899: Pseudo dice [0.8918, 0.8996, 0.9418, 0.9492, 0.9482] 
2025-03-13 06:50:02.750994: Epoch time: 216.47 s 
2025-03-13 06:50:02.751060: Yayy! New best EMA pseudo Dice: 0.9076 
2025-03-13 06:50:08.186170:  
2025-03-13 06:50:08.186504: Epoch 123 
2025-03-13 06:50:08.186693: Current learning rate: 0.00424 
2025-03-13 06:53:45.099164: train_loss -0.8236 
2025-03-13 06:53:45.099734: val_loss -0.3043 
2025-03-13 06:53:45.100006: Pseudo dice [0.8611, 0.8969, 0.9264, 0.9438, 0.9317] 
2025-03-13 06:53:45.100155: Epoch time: 216.92 s 
2025-03-13 06:53:45.100277: Yayy! New best EMA pseudo Dice: 0.908 
2025-03-13 06:53:50.132329:  
2025-03-13 06:53:50.132638: Epoch 124 
2025-03-13 06:53:50.133038: Current learning rate: 0.00419 
2025-03-13 06:57:25.802239: train_loss -0.8308 
2025-03-13 06:57:25.802783: val_loss -0.3655 
2025-03-13 06:57:25.803070: Pseudo dice [0.8751, 0.9127, 0.9366, 0.9431, 0.9391] 
2025-03-13 06:57:25.803230: Epoch time: 215.67 s 
2025-03-13 06:57:25.803325: Yayy! New best EMA pseudo Dice: 0.9094 
2025-03-13 06:57:29.692629:  
2025-03-13 06:57:29.692936: Epoch 125 
2025-03-13 06:57:29.693151: Current learning rate: 0.00414 
2025-03-13 07:01:05.467817: train_loss -0.8296 
2025-03-13 07:01:05.468151: val_loss -0.3663 
2025-03-13 07:01:05.468294: Pseudo dice [0.8801, 0.8878, 0.9356, 0.9447, 0.9441] 
2025-03-13 07:01:05.468396: Epoch time: 215.78 s 
2025-03-13 07:01:05.468470: Yayy! New best EMA pseudo Dice: 0.9103 
2025-03-13 07:01:10.790860:  
2025-03-13 07:01:10.791172: Epoch 126 
2025-03-13 07:01:10.791646: Current learning rate: 0.00409 
2025-03-13 07:04:46.411824: train_loss -0.8314 
2025-03-13 07:04:46.412271: val_loss -0.3747 
2025-03-13 07:04:46.412455: Pseudo dice [0.8905, 0.8852, 0.9354, 0.9338, 0.961] 
2025-03-13 07:04:46.412631: Epoch time: 215.63 s 
2025-03-13 07:04:46.412703: Yayy! New best EMA pseudo Dice: 0.9114 
2025-03-13 07:04:51.862489:  
2025-03-13 07:04:51.862894: Epoch 127 
2025-03-13 07:04:51.863117: Current learning rate: 0.00404 
2025-03-13 07:08:28.246546: train_loss -0.8214 
2025-03-13 07:08:28.247044: val_loss -0.3444 
2025-03-13 07:08:28.247194: Pseudo dice [0.8692, 0.9024, 0.9298, 0.9508, 0.9326] 
2025-03-13 07:08:28.247293: Epoch time: 216.39 s 
2025-03-13 07:08:28.247365: Yayy! New best EMA pseudo Dice: 0.9119 
2025-03-13 07:08:34.266040:  
2025-03-13 07:08:34.266402: Epoch 128 
2025-03-13 07:08:34.266607: Current learning rate: 0.00399 
2025-03-13 07:12:10.740421: train_loss -0.8333 
2025-03-13 07:12:10.740873: val_loss -0.4124 
2025-03-13 07:12:10.741059: Pseudo dice [0.9004, 0.915, 0.9445, 0.938, 0.9414] 
2025-03-13 07:12:10.741221: Epoch time: 216.48 s 
2025-03-13 07:12:10.741344: Yayy! New best EMA pseudo Dice: 0.9135 
2025-03-13 07:12:16.558928:  
2025-03-13 07:12:16.559325: Epoch 129 
2025-03-13 07:12:16.559512: Current learning rate: 0.00394 
2025-03-13 07:15:52.008881: train_loss -0.8362 
2025-03-13 07:15:52.009323: val_loss -0.3121 
2025-03-13 07:15:52.009581: Pseudo dice [0.8842, 0.9036, 0.9303, 0.9375, 0.9567] 
2025-03-13 07:15:52.009750: Epoch time: 215.45 s 
2025-03-13 07:15:52.009874: Yayy! New best EMA pseudo Dice: 0.9144 
2025-03-13 07:15:57.084249:  
2025-03-13 07:15:57.084595: Epoch 130 
2025-03-13 07:15:57.084947: Current learning rate: 0.00389 
2025-03-13 07:19:32.649435: train_loss -0.8329 
2025-03-13 07:19:32.650008: val_loss -0.3666 
2025-03-13 07:19:32.650236: Pseudo dice [0.8938, 0.9062, 0.9345, 0.9496, 0.8196] 
2025-03-13 07:19:32.650353: Epoch time: 215.57 s 
2025-03-13 07:19:34.372099:  
2025-03-13 07:19:34.372341: Epoch 131 
2025-03-13 07:19:34.372539: Current learning rate: 0.00384 
2025-03-13 07:23:09.904354: train_loss -0.8461 
2025-03-13 07:23:09.904846: val_loss -0.331 
2025-03-13 07:23:09.905027: Pseudo dice [0.8812, 0.9095, 0.9336, 0.9479, 0.9474] 
2025-03-13 07:23:09.905432: Epoch time: 215.54 s 
2025-03-13 07:23:11.863190:  
2025-03-13 07:23:11.863468: Epoch 132 
2025-03-13 07:23:11.863741: Current learning rate: 0.00379 
2025-03-13 07:26:47.934873: train_loss -0.8484 
2025-03-13 07:26:47.935244: val_loss -0.3323 
2025-03-13 07:26:47.935372: Pseudo dice [0.8633, 0.8989, 0.9295, 0.9418, 0.9527] 
2025-03-13 07:26:47.935461: Epoch time: 216.08 s 
2025-03-13 07:26:47.935526: Yayy! New best EMA pseudo Dice: 0.9144 
2025-03-13 07:26:54.917839:  
2025-03-13 07:26:54.918174: Epoch 133 
2025-03-13 07:26:54.918425: Current learning rate: 0.00374 
2025-03-13 07:30:31.417984: train_loss -0.8244 
2025-03-13 07:30:31.418640: val_loss -0.3599 
2025-03-13 07:30:31.418883: Pseudo dice [0.8905, 0.9178, 0.9301, 0.9357, 0.9472] 
2025-03-13 07:30:31.419055: Epoch time: 216.5 s 
2025-03-13 07:30:31.419189: Yayy! New best EMA pseudo Dice: 0.9154 
2025-03-13 07:30:36.587228:  
2025-03-13 07:30:36.587585: Epoch 134 
2025-03-13 07:30:36.587769: Current learning rate: 0.00369 
2025-03-13 07:34:12.376023: train_loss -0.8322 
2025-03-13 07:34:12.376351: val_loss -0.3675 
2025-03-13 07:34:12.376484: Pseudo dice [0.8895, 0.9121, 0.9389, 0.9455, 0.9111] 
2025-03-13 07:34:12.376682: Epoch time: 215.79 s 
2025-03-13 07:34:12.376817: Yayy! New best EMA pseudo Dice: 0.9158 
2025-03-13 07:34:18.368462:  
2025-03-13 07:34:18.368820: Epoch 135 
2025-03-13 07:34:18.368985: Current learning rate: 0.00364 
2025-03-13 07:37:54.392161: train_loss -0.8417 
2025-03-13 07:37:54.392677: val_loss -0.3725 
2025-03-13 07:37:54.392816: Pseudo dice [0.889, 0.9111, 0.9352, 0.9426, 0.9156] 
2025-03-13 07:37:54.392921: Epoch time: 216.03 s 
2025-03-13 07:37:54.392992: Yayy! New best EMA pseudo Dice: 0.9161 
2025-03-13 07:37:59.815766:  
2025-03-13 07:37:59.816103: Epoch 136 
2025-03-13 07:37:59.816253: Current learning rate: 0.00359 
2025-03-13 07:41:35.782036: train_loss -0.836 
2025-03-13 07:41:35.782496: val_loss -0.3604 
2025-03-13 07:41:35.782629: Pseudo dice [0.881, 0.9104, 0.9353, 0.9464, 0.9319] 
2025-03-13 07:41:35.782719: Epoch time: 215.97 s 
2025-03-13 07:41:35.782783: Yayy! New best EMA pseudo Dice: 0.9166 
2025-03-13 07:41:41.007609:  
2025-03-13 07:41:41.007935: Epoch 137 
2025-03-13 07:41:41.008220: Current learning rate: 0.00354 
2025-03-13 07:45:17.328079: train_loss -0.8511 
2025-03-13 07:45:17.328422: val_loss -0.3906 
2025-03-13 07:45:17.328696: Pseudo dice [0.892, 0.9163, 0.939, 0.9478, 0.9435] 
2025-03-13 07:45:17.328890: Epoch time: 216.33 s 
2025-03-13 07:45:17.328961: Yayy! New best EMA pseudo Dice: 0.9177 
2025-03-13 07:45:22.871615:  
2025-03-13 07:45:22.871981: Epoch 138 
2025-03-13 07:45:22.872279: Current learning rate: 0.00349 
2025-03-13 07:48:59.807766: train_loss -0.8423 
2025-03-13 07:48:59.808230: val_loss -0.3407 
2025-03-13 07:48:59.808410: Pseudo dice [0.8852, 0.8941, 0.9347, 0.9486, 0.944] 
2025-03-13 07:48:59.808582: Epoch time: 216.94 s 
2025-03-13 07:48:59.808724: Yayy! New best EMA pseudo Dice: 0.9181 
2025-03-13 07:49:05.023151:  
2025-03-13 07:49:05.023500: Epoch 139 
2025-03-13 07:49:05.023679: Current learning rate: 0.00343 
2025-03-13 07:52:40.587440: train_loss -0.851 
2025-03-13 07:52:40.588042: val_loss -0.3574 
2025-03-13 07:52:40.588288: Pseudo dice [0.887, 0.9086, 0.9328, 0.945, 0.9564] 
2025-03-13 07:52:40.588459: Epoch time: 215.57 s 
2025-03-13 07:52:40.588540: Yayy! New best EMA pseudo Dice: 0.9189 
2025-03-13 07:52:45.837614:  
2025-03-13 07:52:45.837940: Epoch 140 
2025-03-13 07:52:45.838204: Current learning rate: 0.00338 
2025-03-13 07:56:21.976697: train_loss -0.8309 
2025-03-13 07:56:21.977138: val_loss -0.4325 
2025-03-13 07:56:21.977339: Pseudo dice [0.9005, 0.912, 0.942, 0.9518, 0.8994] 
2025-03-13 07:56:21.977531: Epoch time: 216.14 s 
2025-03-13 07:56:21.977642: Yayy! New best EMA pseudo Dice: 0.9191 
2025-03-13 07:56:27.152755:  
2025-03-13 07:56:27.153087: Epoch 141 
2025-03-13 07:56:27.153312: Current learning rate: 0.00333 
2025-03-13 08:00:03.029041: train_loss -0.8345 
2025-03-13 08:00:03.029500: val_loss -0.4069 
2025-03-13 08:00:03.029636: Pseudo dice [0.9004, 0.9138, 0.944, 0.9408, 0.9232] 
2025-03-13 08:00:03.029730: Epoch time: 215.88 s 
2025-03-13 08:00:03.029796: Yayy! New best EMA pseudo Dice: 0.9196 
2025-03-13 08:00:06.554688:  
2025-03-13 08:00:06.555180: Epoch 142 
2025-03-13 08:00:06.555432: Current learning rate: 0.00328 
2025-03-13 08:03:43.147440: train_loss -0.8473 
2025-03-13 08:03:43.147851: val_loss -0.3875 
2025-03-13 08:03:43.147995: Pseudo dice [0.8969, 0.9116, 0.9433, 0.9433, 0.8581] 
2025-03-13 08:03:43.148107: Epoch time: 216.6 s 
2025-03-13 08:03:44.948410:  
2025-03-13 08:03:44.948711: Epoch 143 
2025-03-13 08:03:44.948950: Current learning rate: 0.00323 
2025-03-13 08:07:21.689224: train_loss -0.8344 
2025-03-13 08:07:21.689565: val_loss -0.337 
2025-03-13 08:07:21.689715: Pseudo dice [0.8889, 0.9107, 0.9383, 0.9401, 0.7464] 
2025-03-13 08:07:21.689831: Epoch time: 216.75 s 
2025-03-13 08:07:23.461581:  
2025-03-13 08:07:23.461826: Epoch 144 
2025-03-13 08:07:23.462082: Current learning rate: 0.00318 
2025-03-13 08:10:59.114942: train_loss -0.8484 
2025-03-13 08:10:59.115375: val_loss -0.3571 
2025-03-13 08:10:59.115662: Pseudo dice [0.8919, 0.8994, 0.9415, 0.9424, 0.9337] 
2025-03-13 08:10:59.115781: Epoch time: 215.66 s 
2025-03-13 08:11:01.002292:  
2025-03-13 08:11:01.002639: Epoch 145 
2025-03-13 08:11:01.002805: Current learning rate: 0.00313 
2025-03-13 08:14:36.741054: train_loss -0.8367 
2025-03-13 08:14:36.741464: val_loss -0.406 
2025-03-13 08:14:36.741648: Pseudo dice [0.9008, 0.9088, 0.9453, 0.9478, 0.913] 
2025-03-13 08:14:36.741754: Epoch time: 215.74 s 
2025-03-13 08:14:38.515364:  
2025-03-13 08:14:38.515793: Epoch 146 
2025-03-13 08:14:38.516175: Current learning rate: 0.00308 
2025-03-13 08:18:14.671911: train_loss -0.8383 
2025-03-13 08:18:14.672435: val_loss -0.3094 
2025-03-13 08:18:14.672634: Pseudo dice [0.8819, 0.9014, 0.932, 0.9343, 0.7686] 
2025-03-13 08:18:14.672746: Epoch time: 216.16 s 
2025-03-13 08:18:16.418867:  
2025-03-13 08:18:16.419160: Epoch 147 
2025-03-13 08:18:16.419333: Current learning rate: 0.00303 
2025-03-13 08:21:52.356557: train_loss -0.8425 
2025-03-13 08:21:52.356924: val_loss -0.3059 
2025-03-13 08:21:52.357064: Pseudo dice [0.8838, 0.9096, 0.9267, 0.9473, 0.9122] 
2025-03-13 08:21:52.357212: Epoch time: 215.94 s 
2025-03-13 08:21:54.264119:  
2025-03-13 08:21:54.264545: Epoch 148 
2025-03-13 08:21:54.264716: Current learning rate: 0.00297 
2025-03-13 08:25:31.253313: train_loss -0.8444 
2025-03-13 08:25:31.253704: val_loss -0.4217 
2025-03-13 08:25:31.253952: Pseudo dice [0.8965, 0.921, 0.9474, 0.947, 0.9364] 
2025-03-13 08:25:31.254140: Epoch time: 217.01 s 
2025-03-13 08:25:32.986594:  
2025-03-13 08:25:32.986849: Epoch 149 
2025-03-13 08:25:32.987028: Current learning rate: 0.00292 
2025-03-13 08:29:08.603824: train_loss -0.8404 
2025-03-13 08:29:08.604274: val_loss -0.3688 
2025-03-13 08:29:08.604494: Pseudo dice [0.8946, 0.9113, 0.9407, 0.939, 0.9511] 
2025-03-13 08:29:08.604632: Epoch time: 215.62 s 
2025-03-13 08:29:14.273042:  
2025-03-13 08:29:14.273462: Epoch 150 
2025-03-13 08:29:14.273632: Current learning rate: 0.00287 
2025-03-13 08:32:49.988228: train_loss -0.8506 
2025-03-13 08:32:49.988711: val_loss -0.4202 
2025-03-13 08:32:49.988962: Pseudo dice [0.9028, 0.9202, 0.9445, 0.9517, 0.921] 
2025-03-13 08:32:49.989155: Epoch time: 215.72 s 
2025-03-13 08:32:51.875614:  
2025-03-13 08:32:51.875915: Epoch 151 
2025-03-13 08:32:51.876182: Current learning rate: 0.00282 
2025-03-13 08:36:27.673942: train_loss -0.8413 
2025-03-13 08:36:27.674285: val_loss -0.3309 
2025-03-13 08:36:27.674413: Pseudo dice [0.8801, 0.9063, 0.937, 0.942, 0.9246] 
2025-03-13 08:36:27.674506: Epoch time: 215.8 s 
2025-03-13 08:36:29.452825:  
2025-03-13 08:36:29.453137: Epoch 152 
2025-03-13 08:36:29.453306: Current learning rate: 0.00277 
2025-03-13 08:40:05.455342: train_loss -0.8335 
2025-03-13 08:40:05.455714: val_loss -0.3799 
2025-03-13 08:40:05.455832: Pseudo dice [0.8907, 0.9207, 0.9446, 0.9503, 0.879] 
2025-03-13 08:40:05.455924: Epoch time: 216.01 s 
2025-03-13 08:40:08.023560:  
2025-03-13 08:40:08.023889: Epoch 153 
2025-03-13 08:40:08.024111: Current learning rate: 0.00272 
2025-03-13 08:43:45.343067: train_loss -0.8409 
2025-03-13 08:43:45.343538: val_loss -0.3204 
2025-03-13 08:43:45.343806: Pseudo dice [0.8916, 0.9009, 0.9353, 0.929, 0.9292] 
2025-03-13 08:43:45.343959: Epoch time: 217.32 s 
2025-03-13 08:43:47.107868:  
2025-03-13 08:43:47.108232: Epoch 154 
2025-03-13 08:43:47.108406: Current learning rate: 0.00266 
2025-03-13 08:47:22.924484: train_loss -0.8441 
2025-03-13 08:47:22.925057: val_loss -0.2697 
2025-03-13 08:47:22.925222: Pseudo dice [0.8717, 0.9071, 0.9317, 0.9505, 0.9299] 
2025-03-13 08:47:22.925372: Epoch time: 215.82 s 
2025-03-13 08:47:24.748121:  
2025-03-13 08:47:24.748493: Epoch 155 
2025-03-13 08:47:24.748648: Current learning rate: 0.00261 
2025-03-13 08:51:00.594714: train_loss -0.8525 
2025-03-13 08:51:00.595027: val_loss -0.4023 
2025-03-13 08:51:00.595195: Pseudo dice [0.9045, 0.9212, 0.9462, 0.9511, 0.9001] 
2025-03-13 08:51:00.595292: Epoch time: 215.85 s 
2025-03-13 08:51:02.435545:  
2025-03-13 08:51:02.435870: Epoch 156 
2025-03-13 08:51:02.436072: Current learning rate: 0.00256 
2025-03-13 08:54:38.304493: train_loss -0.854 
2025-03-13 08:54:38.304919: val_loss -0.3926 
2025-03-13 08:54:38.305043: Pseudo dice [0.9032, 0.9208, 0.942, 0.9371, 0.9199] 
2025-03-13 08:54:38.305137: Epoch time: 215.87 s 
2025-03-13 08:54:40.088135:  
2025-03-13 08:54:40.088495: Epoch 157 
2025-03-13 08:54:40.088682: Current learning rate: 0.00251 
2025-03-13 08:58:15.805700: train_loss -0.8457 
2025-03-13 08:58:15.806236: val_loss -0.398 
2025-03-13 08:58:15.806572: Pseudo dice [0.9018, 0.9226, 0.9465, 0.9466, 0.8519] 
2025-03-13 08:58:15.806730: Epoch time: 215.72 s 
2025-03-13 08:58:17.619751:  
2025-03-13 08:58:17.620088: Epoch 158 
2025-03-13 08:58:17.620268: Current learning rate: 0.00245 
2025-03-13 09:01:53.942803: train_loss -0.8474 
2025-03-13 09:01:53.943317: val_loss -0.3036 
2025-03-13 09:01:53.943642: Pseudo dice [0.8793, 0.9229, 0.9305, 0.9451, 0.9643] 
2025-03-13 09:01:53.943817: Epoch time: 216.33 s 
2025-03-13 09:01:55.762711:  
2025-03-13 09:01:55.763049: Epoch 159 
2025-03-13 09:01:55.763228: Current learning rate: 0.0024 
2025-03-13 09:05:32.056468: train_loss -0.8514 
2025-03-13 09:05:32.056967: val_loss -0.3915 
2025-03-13 09:05:32.057188: Pseudo dice [0.8966, 0.9168, 0.9442, 0.9505, 0.965] 
2025-03-13 09:05:32.057291: Epoch time: 216.3 s 
2025-03-13 09:05:32.057358: Yayy! New best EMA pseudo Dice: 0.921 
2025-03-13 09:05:38.026393:  
2025-03-13 09:05:38.026715: Epoch 160 
2025-03-13 09:05:38.026854: Current learning rate: 0.00235 
2025-03-13 09:09:13.602383: train_loss -0.8393 
2025-03-13 09:09:13.602970: val_loss -0.2984 
2025-03-13 09:09:13.603160: Pseudo dice [0.8785, 0.9027, 0.9354, 0.9466, 0.9368] 
2025-03-13 09:09:13.603320: Epoch time: 215.59 s 
2025-03-13 09:09:15.444725:  
2025-03-13 09:09:15.445012: Epoch 161 
2025-03-13 09:09:15.445219: Current learning rate: 0.0023 
2025-03-13 09:12:51.143338: train_loss -0.8468 
2025-03-13 09:12:51.143685: val_loss -0.4226 
2025-03-13 09:12:51.143830: Pseudo dice [0.9139, 0.9225, 0.9476, 0.9561, 0.955] 
2025-03-13 09:12:51.143924: Epoch time: 215.7 s 
2025-03-13 09:12:51.143992: Yayy! New best EMA pseudo Dice: 0.9227 
2025-03-13 09:12:54.598325:  
2025-03-13 09:12:54.598663: Epoch 162 
2025-03-13 09:12:54.598861: Current learning rate: 0.00224 
2025-03-13 09:16:30.332944: train_loss -0.8356 
2025-03-13 09:16:30.333465: val_loss -0.3304 
2025-03-13 09:16:30.333758: Pseudo dice [0.8925, 0.9104, 0.9373, 0.9236, 0.5756] 
2025-03-13 09:16:30.333909: Epoch time: 215.74 s 
2025-03-13 09:16:32.102419:  
2025-03-13 09:16:32.102663: Epoch 163 
2025-03-13 09:16:32.102776: Current learning rate: 0.00219 
2025-03-13 09:20:09.454609: train_loss -0.8448 
2025-03-13 09:20:09.455173: val_loss -0.3638 
2025-03-13 09:20:09.455896: Pseudo dice [0.8821, 0.9121, 0.9342, 0.9396, 0.8692] 
2025-03-13 09:20:09.456212: Epoch time: 217.36 s 
2025-03-13 09:20:11.415431:  
2025-03-13 09:20:11.415768: Epoch 164 
2025-03-13 09:20:11.415963: Current learning rate: 0.00214 
2025-03-13 09:23:47.546069: train_loss -0.8496 
2025-03-13 09:23:47.546576: val_loss -0.3391 
2025-03-13 09:23:47.546814: Pseudo dice [0.8862, 0.9096, 0.9369, 0.9287, 0.7613] 
2025-03-13 09:23:47.546907: Epoch time: 216.14 s 
2025-03-13 09:23:49.270107:  
2025-03-13 09:23:49.270444: Epoch 165 
2025-03-13 09:23:49.270610: Current learning rate: 0.00208 
2025-03-13 09:27:25.429622: train_loss -0.8505 
2025-03-13 09:27:25.429963: val_loss -0.2089 
2025-03-13 09:27:25.430134: Pseudo dice [0.8491, 0.9076, 0.9261, 0.9484, 0.8349] 
2025-03-13 09:27:25.430369: Epoch time: 216.17 s 
2025-03-13 09:27:27.183020:  
2025-03-13 09:27:27.183253: Epoch 166 
2025-03-13 09:27:27.183441: Current learning rate: 0.00203 
2025-03-13 09:31:03.000759: train_loss -0.8435 
2025-03-13 09:31:03.001197: val_loss -0.3141 
2025-03-13 09:31:03.001330: Pseudo dice [0.8701, 0.9205, 0.9302, 0.9527, 0.8748] 
2025-03-13 09:31:03.001428: Epoch time: 215.82 s 
2025-03-13 09:31:04.743383:  
2025-03-13 09:31:04.743728: Epoch 167 
2025-03-13 09:31:04.743879: Current learning rate: 0.00198 
2025-03-13 09:34:40.562964: train_loss -0.85 
2025-03-13 09:34:40.563411: val_loss -0.3691 
2025-03-13 09:34:40.563599: Pseudo dice [0.8979, 0.9188, 0.9417, 0.9614, 0.9046] 
2025-03-13 09:34:40.563703: Epoch time: 215.83 s 
2025-03-13 09:34:42.385802:  
2025-03-13 09:34:42.386156: Epoch 168 
2025-03-13 09:34:42.386324: Current learning rate: 0.00192 
2025-03-13 09:38:19.149488: train_loss -0.8505 
2025-03-13 09:38:19.149966: val_loss -0.3352 
2025-03-13 09:38:19.150187: Pseudo dice [0.8841, 0.9163, 0.9331, 0.9499, 0.8342] 
2025-03-13 09:38:19.150396: Epoch time: 216.77 s 
2025-03-13 09:38:20.990255:  
2025-03-13 09:38:20.990667: Epoch 169 
2025-03-13 09:38:20.990843: Current learning rate: 0.00187 
2025-03-13 09:41:57.755871: train_loss -0.8479 
2025-03-13 09:41:57.756267: val_loss -0.3425 
2025-03-13 09:41:57.756392: Pseudo dice [0.8822, 0.9159, 0.9376, 0.9437, 0.9127] 
2025-03-13 09:41:57.756483: Epoch time: 216.77 s 
2025-03-13 09:41:59.501905:  
2025-03-13 09:41:59.502185: Epoch 170 
2025-03-13 09:41:59.502764: Current learning rate: 0.00181 
2025-03-13 09:45:35.513317: train_loss -0.8465 
2025-03-13 09:45:35.513827: val_loss -0.3097 
2025-03-13 09:45:35.514072: Pseudo dice [0.8882, 0.9166, 0.9357, 0.9387, 0.9306] 
2025-03-13 09:45:35.514198: Epoch time: 216.02 s 
2025-03-13 09:45:37.249826:  
2025-03-13 09:45:37.250255: Epoch 171 
2025-03-13 09:45:37.250503: Current learning rate: 0.00176 
2025-03-13 09:49:13.036236: train_loss -0.8552 
2025-03-13 09:49:13.036776: val_loss -0.3991 
2025-03-13 09:49:13.036979: Pseudo dice [0.8981, 0.9197, 0.9472, 0.9454, 0.7922] 
2025-03-13 09:49:13.037143: Epoch time: 215.79 s 
2025-03-13 09:49:14.855288:  
2025-03-13 09:49:14.855655: Epoch 172 
2025-03-13 09:49:14.855858: Current learning rate: 0.0017 
2025-03-13 09:52:51.325831: train_loss -0.8526 
2025-03-13 09:52:51.326289: val_loss -0.3198 
2025-03-13 09:52:51.326580: Pseudo dice [0.8789, 0.926, 0.9354, 0.9537, 0.6397] 
2025-03-13 09:52:51.326781: Epoch time: 216.48 s 
2025-03-13 09:52:53.083789:  
2025-03-13 09:52:53.084234: Epoch 173 
2025-03-13 09:52:53.084441: Current learning rate: 0.00165 
2025-03-13 09:56:29.519985: train_loss -0.8479 
2025-03-13 09:56:29.520403: val_loss -0.3907 
2025-03-13 09:56:29.520655: Pseudo dice [0.888, 0.9187, 0.9428, 0.9546, 0.923] 
2025-03-13 09:56:29.520839: Epoch time: 216.44 s 
2025-03-13 09:56:31.453001:  
2025-03-13 09:56:31.453310: Epoch 174 
2025-03-13 09:56:31.453509: Current learning rate: 0.00159 
2025-03-13 10:00:08.435013: train_loss -0.8531 
2025-03-13 10:00:08.435565: val_loss -0.3656 
2025-03-13 10:00:08.435872: Pseudo dice [0.8945, 0.918, 0.9412, 0.958, 0.9358] 
2025-03-13 10:00:08.436088: Epoch time: 216.99 s 
2025-03-13 10:00:10.233237:  
2025-03-13 10:00:10.233487: Epoch 175 
2025-03-13 10:00:10.233675: Current learning rate: 0.00154 
2025-03-13 10:03:45.875607: train_loss -0.8501 
2025-03-13 10:03:45.875938: val_loss -0.3128 
2025-03-13 10:03:45.876079: Pseudo dice [0.8691, 0.9163, 0.936, 0.9509, 0.8911] 
2025-03-13 10:03:45.876173: Epoch time: 215.65 s 
2025-03-13 10:03:47.740608:  
2025-03-13 10:03:47.740888: Epoch 176 
2025-03-13 10:03:47.741077: Current learning rate: 0.00148 
2025-03-13 10:07:23.522858: train_loss -0.8461 
2025-03-13 10:07:23.523260: val_loss -0.3398 
2025-03-13 10:07:23.523437: Pseudo dice [0.8772, 0.9072, 0.9361, 0.9375, 0.8969] 
2025-03-13 10:07:23.523792: Epoch time: 215.79 s 
2025-03-13 10:07:25.254038:  
2025-03-13 10:07:25.254370: Epoch 177 
2025-03-13 10:07:25.254580: Current learning rate: 0.00143 
2025-03-13 10:11:01.096956: train_loss -0.8503 
2025-03-13 10:11:01.097472: val_loss -0.3578 
2025-03-13 10:11:01.097753: Pseudo dice [0.889, 0.9094, 0.9391, 0.9516, 0.9457] 
2025-03-13 10:11:01.097910: Epoch time: 215.85 s 
2025-03-13 10:11:02.867682:  
2025-03-13 10:11:02.868008: Epoch 178 
2025-03-13 10:11:02.868207: Current learning rate: 0.00137 
2025-03-13 10:14:39.029049: train_loss -0.8423 
2025-03-13 10:14:39.029482: val_loss -0.3599 
2025-03-13 10:14:39.029713: Pseudo dice [0.8899, 0.9243, 0.9376, 0.9486, 0.9355] 
2025-03-13 10:14:39.029840: Epoch time: 216.17 s 
2025-03-13 10:14:41.425828:  
2025-03-13 10:14:41.426214: Epoch 179 
2025-03-13 10:14:41.426437: Current learning rate: 0.00132 
2025-03-13 10:18:18.720787: train_loss -0.8539 
2025-03-13 10:18:18.721341: val_loss -0.4197 
2025-03-13 10:18:18.721579: Pseudo dice [0.8958, 0.9322, 0.9429, 0.9562, 0.9486] 
2025-03-13 10:18:18.721687: Epoch time: 217.3 s 
2025-03-13 10:18:20.706840:  
2025-03-13 10:18:20.708733: Epoch 180 
2025-03-13 10:18:20.709224: Current learning rate: 0.00126 
2025-03-13 10:21:56.867227: train_loss -0.8513 
2025-03-13 10:21:56.867651: val_loss -0.3281 
2025-03-13 10:21:56.867830: Pseudo dice [0.8831, 0.9163, 0.9373, 0.9525, 0.9102] 
2025-03-13 10:21:56.868009: Epoch time: 216.17 s 
2025-03-13 10:21:58.642487:  
2025-03-13 10:21:58.642798: Epoch 181 
2025-03-13 10:21:58.643044: Current learning rate: 0.0012 
2025-03-13 10:25:34.907383: train_loss -0.8482 
2025-03-13 10:25:34.907920: val_loss -0.3949 
2025-03-13 10:25:34.908142: Pseudo dice [0.9015, 0.9207, 0.944, 0.9521, 0.953] 
2025-03-13 10:25:34.908246: Epoch time: 216.27 s 
2025-03-13 10:25:36.800025:  
2025-03-13 10:25:36.800369: Epoch 182 
2025-03-13 10:25:36.800629: Current learning rate: 0.00115 
2025-03-13 10:29:13.166726: train_loss -0.8611 
2025-03-13 10:29:13.167119: val_loss -0.3608 
2025-03-13 10:29:13.167243: Pseudo dice [0.8864, 0.922, 0.9413, 0.9541, 0.934] 
2025-03-13 10:29:13.167336: Epoch time: 216.37 s 
2025-03-13 10:29:14.960251:  
2025-03-13 10:29:14.960471: Epoch 183 
2025-03-13 10:29:14.960666: Current learning rate: 0.00109 
2025-03-13 10:32:51.306344: train_loss -0.8465 
2025-03-13 10:32:51.306758: val_loss -0.3564 
2025-03-13 10:32:51.306895: Pseudo dice [0.8876, 0.9187, 0.9396, 0.9436, 0.8885] 
2025-03-13 10:32:51.307009: Epoch time: 216.35 s 
2025-03-13 10:32:53.083133:  
2025-03-13 10:32:53.083441: Epoch 184 
2025-03-13 10:32:53.083617: Current learning rate: 0.00103 
2025-03-13 10:36:29.714815: train_loss -0.856 
2025-03-13 10:36:29.716286: val_loss -0.289 
2025-03-13 10:36:29.716735: Pseudo dice [0.8757, 0.9143, 0.9345, 0.9414, 0.9037] 
2025-03-13 10:36:29.717129: Epoch time: 216.64 s 
2025-03-13 10:36:31.598734:  
2025-03-13 10:36:31.599086: Epoch 185 
2025-03-13 10:36:31.599308: Current learning rate: 0.00097 
2025-03-13 10:40:08.607778: train_loss -0.8475 
2025-03-13 10:40:08.609237: val_loss -0.3488 
2025-03-13 10:40:08.610627: Pseudo dice [0.8915, 0.9182, 0.9413, 0.9429, 0.8916] 
2025-03-13 10:40:08.611866: Epoch time: 217.02 s 
2025-03-13 10:40:10.456729:  
2025-03-13 10:40:10.457077: Epoch 186 
2025-03-13 10:40:10.457242: Current learning rate: 0.00091 
2025-03-13 10:43:46.720849: train_loss -0.8544 
2025-03-13 10:43:46.721366: val_loss -0.3541 
2025-03-13 10:43:46.721487: Pseudo dice [0.8854, 0.9229, 0.938, 0.953, 0.9151] 
2025-03-13 10:43:46.721587: Epoch time: 216.27 s 
2025-03-13 10:43:48.468338:  
2025-03-13 10:43:48.468686: Epoch 187 
2025-03-13 10:43:48.468925: Current learning rate: 0.00085 
2025-03-13 10:47:24.780576: train_loss -0.851 
2025-03-13 10:47:24.780983: val_loss -0.3376 
2025-03-13 10:47:24.781190: Pseudo dice [0.8905, 0.911, 0.9396, 0.9383, 0.9417] 
2025-03-13 10:47:24.781360: Epoch time: 216.32 s 
2025-03-13 10:47:26.566977:  
2025-03-13 10:47:26.567335: Epoch 188 
2025-03-13 10:47:26.567629: Current learning rate: 0.00079 
2025-03-13 10:51:02.834244: train_loss -0.8527 
2025-03-13 10:51:02.834841: val_loss -0.3329 
2025-03-13 10:51:02.835140: Pseudo dice [0.8943, 0.9135, 0.9404, 0.953, 0.8809] 
2025-03-13 10:51:02.835309: Epoch time: 216.28 s 
2025-03-13 10:51:04.607784:  
2025-03-13 10:51:04.608145: Epoch 189 
2025-03-13 10:51:04.608375: Current learning rate: 0.00074 
2025-03-13 10:54:41.166697: train_loss -0.8566 
2025-03-13 10:54:41.167117: val_loss -0.3558 
2025-03-13 10:54:41.167243: Pseudo dice [0.8952, 0.9201, 0.9402, 0.9412, 0.8714] 
2025-03-13 10:54:41.167617: Epoch time: 216.57 s 
2025-03-13 10:54:43.038127:  
2025-03-13 10:54:43.038384: Epoch 190 
2025-03-13 10:54:43.038626: Current learning rate: 0.00067 
2025-03-13 10:58:19.456600: train_loss -0.8475 
2025-03-13 10:58:19.457049: val_loss -0.3685 
2025-03-13 10:58:19.457239: Pseudo dice [0.8996, 0.9342, 0.9435, 0.9517, 0.8776] 
2025-03-13 10:58:19.457373: Epoch time: 216.42 s 
2025-03-13 10:58:21.218790:  
2025-03-13 10:58:21.266805: Epoch 191 
2025-03-13 10:58:21.267121: Current learning rate: 0.00061 
2025-03-13 11:01:57.263627: train_loss -0.8508 
2025-03-13 11:01:57.264241: val_loss -0.3434 
2025-03-13 11:01:57.264478: Pseudo dice [0.8887, 0.9187, 0.9396, 0.9511, 0.9252] 
2025-03-13 11:01:57.264703: Epoch time: 216.05 s 
2025-03-13 11:01:59.078686:  
2025-03-13 11:01:59.078999: Epoch 192 
2025-03-13 11:01:59.079204: Current learning rate: 0.00055 
2025-03-13 11:05:36.344928: train_loss -0.8513 
2025-03-13 11:05:36.345365: val_loss -0.2989 
2025-03-13 11:05:36.345566: Pseudo dice [0.8816, 0.907, 0.9331, 0.9477, 0.9431] 
2025-03-13 11:05:36.345665: Epoch time: 217.27 s 
2025-03-13 11:05:38.157628:  
2025-03-13 11:05:38.158010: Epoch 193 
2025-03-13 11:05:38.158266: Current learning rate: 0.00049 
2025-03-13 11:09:14.287266: train_loss -0.8518 
2025-03-13 11:09:14.287841: val_loss -0.3503 
2025-03-13 11:09:14.288018: Pseudo dice [0.8814, 0.9313, 0.9411, 0.9607, 0.9206] 
2025-03-13 11:09:14.288628: Epoch time: 216.14 s 
2025-03-13 11:09:16.153062:  
2025-03-13 11:09:16.153287: Epoch 194 
2025-03-13 11:09:16.153447: Current learning rate: 0.00043 
2025-03-13 11:12:52.477368: train_loss -0.8601 
2025-03-13 11:12:52.477751: val_loss -0.3735 
2025-03-13 11:12:52.477941: Pseudo dice [0.8876, 0.9107, 0.9415, 0.9564, 0.9552] 
2025-03-13 11:12:52.478115: Epoch time: 216.33 s 
2025-03-13 11:12:54.337646:  
2025-03-13 11:12:54.338019: Epoch 195 
2025-03-13 11:12:54.338197: Current learning rate: 0.00036 
2025-03-13 11:16:31.096959: train_loss -0.8561 
2025-03-13 11:16:31.097385: val_loss -0.3754 
2025-03-13 11:16:31.097585: Pseudo dice [0.8883, 0.9249, 0.9441, 0.9561, 0.9468] 
2025-03-13 11:16:31.097736: Epoch time: 216.77 s 
2025-03-13 11:16:33.044255:  
2025-03-13 11:16:33.044644: Epoch 196 
2025-03-13 11:16:33.044816: Current learning rate: 0.0003 
2025-03-13 11:20:09.253748: train_loss -0.8502 
2025-03-13 11:20:09.254273: val_loss -0.3338 
2025-03-13 11:20:09.254593: Pseudo dice [0.877, 0.9227, 0.9394, 0.9607, 0.9438] 
2025-03-13 11:20:09.254846: Epoch time: 216.22 s 
2025-03-13 11:20:09.255012: Yayy! New best EMA pseudo Dice: 0.923 
2025-03-13 11:20:14.445334:  
2025-03-13 11:20:14.445688: Epoch 197 
2025-03-13 11:20:14.445914: Current learning rate: 0.00023 
2025-03-13 11:23:50.610122: train_loss -0.8559 
2025-03-13 11:23:50.610556: val_loss -0.3107 
2025-03-13 11:23:50.610736: Pseudo dice [0.8813, 0.9224, 0.9398, 0.9522, 0.8467] 
2025-03-13 11:23:50.610868: Epoch time: 216.17 s 
2025-03-13 11:23:52.416374:  
2025-03-13 11:23:52.416720: Epoch 198 
2025-03-13 11:23:52.416893: Current learning rate: 0.00016 
2025-03-13 11:27:28.957175: train_loss -0.8522 
2025-03-13 11:27:28.957814: val_loss -0.2947 
2025-03-13 11:27:28.958102: Pseudo dice [0.8797, 0.9051, 0.9339, 0.9367, 0.8814] 
2025-03-13 11:27:28.958261: Epoch time: 216.55 s 
2025-03-13 11:27:30.757035:  
2025-03-13 11:27:30.757286: Epoch 199 
2025-03-13 11:27:30.757509: Current learning rate: 8e-05 
2025-03-13 11:31:07.196175: train_loss -0.8421 
2025-03-13 11:31:07.196816: val_loss -0.319 
2025-03-13 11:31:07.197046: Pseudo dice [0.8855, 0.9109, 0.9343, 0.9443, 0.943] 
2025-03-13 11:31:07.197179: Epoch time: 216.44 s 
2025-03-13 11:31:11.669839: Training done. 
2025-03-13 11:31:12.505804: Using splits from existing split file: /data/hotaru/projects/nnUNet/nnunetv2/dataset/nnUNet_preprocessed/Dataset001_puma/splits_final.json 
2025-03-13 11:31:12.517637: The split file contains 5 splits. 
2025-03-13 11:31:12.517841: Desired fold for training: 5 
2025-03-13 11:31:12.517978: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-03-13 11:31:12.526952: This random 80:20 split has 322 training and 81 validation cases. 
2025-03-13 11:31:12.529578: predicting aug_0_training_set_metastatic_roi_005 
2025-03-13 11:31:12.534619: aug_0_training_set_metastatic_roi_005, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:31.609428: predicting aug_0_training_set_metastatic_roi_037 
2025-03-13 11:32:31.612361: aug_0_training_set_metastatic_roi_037, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:31.995610: predicting aug_0_training_set_metastatic_roi_056 
2025-03-13 11:32:31.997522: aug_0_training_set_metastatic_roi_056, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:32.369445: predicting aug_0_training_set_metastatic_roi_064 
2025-03-13 11:32:32.371626: aug_0_training_set_metastatic_roi_064, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:32.771506: predicting aug_0_training_set_primary_roi_008 
2025-03-13 11:32:32.773800: aug_0_training_set_primary_roi_008, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:33.156928: predicting aug_0_training_set_primary_roi_038 
2025-03-13 11:32:33.159084: aug_0_training_set_primary_roi_038, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:33.534091: predicting aug_0_training_set_primary_roi_049 
2025-03-13 11:32:33.535977: aug_0_training_set_primary_roi_049, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:33.905060: predicting aug_0_training_set_primary_roi_059 
2025-03-13 11:32:33.906991: aug_0_training_set_primary_roi_059, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:34.280437: predicting aug_0_training_set_primary_roi_089 
2025-03-13 11:32:34.282466: aug_0_training_set_primary_roi_089, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:34.662963: predicting aug_0_training_set_primary_roi_090 
2025-03-13 11:32:34.665261: aug_0_training_set_primary_roi_090, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:35.047950: predicting aug_0_training_set_primary_roi_095 
2025-03-13 11:32:35.050159: aug_0_training_set_primary_roi_095, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:35.434760: predicting aug_0_training_set_primary_roi_096 
2025-03-13 11:32:35.436698: aug_0_training_set_primary_roi_096, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:35.812939: predicting aug_1_training_set_primary_roi_006 
2025-03-13 11:32:35.814776: aug_1_training_set_primary_roi_006, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:36.186707: predicting aug_1_training_set_primary_roi_008 
2025-03-13 11:32:36.188278: aug_1_training_set_primary_roi_008, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:36.557660: predicting aug_1_training_set_primary_roi_029 
2025-03-13 11:32:36.559132: aug_1_training_set_primary_roi_029, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:36.929287: predicting aug_1_training_set_primary_roi_030 
2025-03-13 11:32:36.931067: aug_1_training_set_primary_roi_030, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:37.305319: predicting aug_1_training_set_primary_roi_038 
2025-03-13 11:32:37.307232: aug_1_training_set_primary_roi_038, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:37.687306: predicting aug_1_training_set_primary_roi_039 
2025-03-13 11:32:37.689967: aug_1_training_set_primary_roi_039, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:38.072430: predicting aug_1_training_set_primary_roi_049 
2025-03-13 11:32:38.074382: aug_1_training_set_primary_roi_049, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:38.445269: predicting aug_1_training_set_primary_roi_059 
2025-03-13 11:32:38.447260: aug_1_training_set_primary_roi_059, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:38.821301: predicting aug_1_training_set_primary_roi_070 
2025-03-13 11:32:38.823189: aug_1_training_set_primary_roi_070, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:39.202460: predicting aug_1_training_set_primary_roi_081 
2025-03-13 11:32:39.205740: aug_1_training_set_primary_roi_081, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:39.594710: predicting aug_1_training_set_primary_roi_089 
2025-03-13 11:32:39.597184: aug_1_training_set_primary_roi_089, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:39.975592: predicting aug_1_training_set_primary_roi_100 
2025-03-13 11:32:39.977686: aug_1_training_set_primary_roi_100, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:40.349051: predicting aug_1_training_set_primary_roi_101 
2025-03-13 11:32:40.350956: aug_1_training_set_primary_roi_101, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:40.729440: predicting aug_1_training_set_primary_roi_103 
2025-03-13 11:32:40.731755: aug_1_training_set_primary_roi_103, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:41.134074: predicting aug_2_training_set_metastatic_roi_032 
2025-03-13 11:32:41.136017: aug_2_training_set_metastatic_roi_032, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:41.510282: predicting aug_2_training_set_metastatic_roi_102 
2025-03-13 11:32:41.512064: aug_2_training_set_metastatic_roi_102, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:41.893450: predicting aug_2_training_set_primary_roi_010 
2025-03-13 11:32:41.900733: aug_2_training_set_primary_roi_010, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:42.314793: predicting aug_2_training_set_primary_roi_020 
2025-03-13 11:32:42.317029: aug_2_training_set_primary_roi_020, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:42.691318: predicting aug_2_training_set_primary_roi_027 
2025-03-13 11:32:42.692940: aug_2_training_set_primary_roi_027, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:43.062698: predicting aug_2_training_set_primary_roi_030 
2025-03-13 11:32:43.064481: aug_2_training_set_primary_roi_030, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:43.440174: predicting aug_2_training_set_primary_roi_051 
2025-03-13 11:32:43.441898: aug_2_training_set_primary_roi_051, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:43.817991: predicting aug_2_training_set_primary_roi_059 
2025-03-13 11:32:43.819568: aug_2_training_set_primary_roi_059, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:44.194525: predicting aug_2_training_set_primary_roi_081 
2025-03-13 11:32:44.196223: aug_2_training_set_primary_roi_081, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:44.572600: predicting aug_2_training_set_primary_roi_083 
2025-03-13 11:32:44.574116: aug_2_training_set_primary_roi_083, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:44.944704: predicting aug_2_training_set_primary_roi_089 
2025-03-13 11:32:44.946413: aug_2_training_set_primary_roi_089, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:45.332533: predicting aug_2_training_set_primary_roi_100 
2025-03-13 11:32:45.335067: aug_2_training_set_primary_roi_100, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:45.722896: predicting training_set_metastatic_roi_016 
2025-03-13 11:32:45.725422: training_set_metastatic_roi_016, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:46.109566: predicting training_set_metastatic_roi_021 
2025-03-13 11:32:46.111521: training_set_metastatic_roi_021, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:46.487494: predicting training_set_metastatic_roi_023 
2025-03-13 11:32:46.489278: training_set_metastatic_roi_023, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:46.860348: predicting training_set_metastatic_roi_024 
2025-03-13 11:32:46.862193: training_set_metastatic_roi_024, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:47.236181: predicting training_set_metastatic_roi_025 
2025-03-13 11:32:47.237975: training_set_metastatic_roi_025, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:47.609674: predicting training_set_metastatic_roi_029 
2025-03-13 11:32:47.611334: training_set_metastatic_roi_029, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:47.985800: predicting training_set_metastatic_roi_030 
2025-03-13 11:32:47.987601: training_set_metastatic_roi_030, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:48.359777: predicting training_set_metastatic_roi_033 
2025-03-13 11:32:48.361659: training_set_metastatic_roi_033, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:48.734515: predicting training_set_metastatic_roi_039 
2025-03-13 11:32:48.736271: training_set_metastatic_roi_039, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:49.110239: predicting training_set_metastatic_roi_052 
2025-03-13 11:32:49.112751: training_set_metastatic_roi_052, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:49.491089: predicting training_set_metastatic_roi_053 
2025-03-13 11:32:49.492821: training_set_metastatic_roi_053, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:49.869142: predicting training_set_metastatic_roi_057 
2025-03-13 11:32:49.870839: training_set_metastatic_roi_057, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:50.258946: predicting training_set_metastatic_roi_060 
2025-03-13 11:32:50.261026: training_set_metastatic_roi_060, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:50.653136: predicting training_set_metastatic_roi_061 
2025-03-13 11:32:50.654689: training_set_metastatic_roi_061, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:51.036111: predicting training_set_metastatic_roi_085 
2025-03-13 11:32:51.038053: training_set_metastatic_roi_085, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:51.422466: predicting training_set_metastatic_roi_096 
2025-03-13 11:32:51.424058: training_set_metastatic_roi_096, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:51.811077: predicting training_set_metastatic_roi_100 
2025-03-13 11:32:51.812601: training_set_metastatic_roi_100, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:52.184798: predicting training_set_primary_roi_001 
2025-03-13 11:32:52.186395: training_set_primary_roi_001, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:52.559803: predicting training_set_primary_roi_006 
2025-03-13 11:32:52.561457: training_set_primary_roi_006, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:52.939991: predicting training_set_primary_roi_007 
2025-03-13 11:32:52.941871: training_set_primary_roi_007, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:53.322459: predicting training_set_primary_roi_009 
2025-03-13 11:32:53.324178: training_set_primary_roi_009, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:53.699688: predicting training_set_primary_roi_011 
2025-03-13 11:32:53.702380: training_set_primary_roi_011, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:54.115130: predicting training_set_primary_roi_013 
2025-03-13 11:32:54.117051: training_set_primary_roi_013, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:54.490929: predicting training_set_primary_roi_014 
2025-03-13 11:32:54.492738: training_set_primary_roi_014, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:54.885725: predicting training_set_primary_roi_015 
2025-03-13 11:32:54.891212: training_set_primary_roi_015, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:55.292123: predicting training_set_primary_roi_018 
2025-03-13 11:32:55.294137: training_set_primary_roi_018, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:55.668141: predicting training_set_primary_roi_021 
2025-03-13 11:32:55.670216: training_set_primary_roi_021, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:56.045575: predicting training_set_primary_roi_024 
2025-03-13 11:32:56.047450: training_set_primary_roi_024, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:56.422036: predicting training_set_primary_roi_026 
2025-03-13 11:32:56.424095: training_set_primary_roi_026, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:56.804875: predicting training_set_primary_roi_039 
2025-03-13 11:32:56.807145: training_set_primary_roi_039, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:57.185882: predicting training_set_primary_roi_043 
2025-03-13 11:32:57.187906: training_set_primary_roi_043, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:57.573450: predicting training_set_primary_roi_048 
2025-03-13 11:32:57.575903: training_set_primary_roi_048, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:57.959188: predicting training_set_primary_roi_051 
2025-03-13 11:32:57.960975: training_set_primary_roi_051, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:58.333187: predicting training_set_primary_roi_055 
2025-03-13 11:32:58.335354: training_set_primary_roi_055, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:58.710324: predicting training_set_primary_roi_061 
2025-03-13 11:32:58.712308: training_set_primary_roi_061, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:59.085942: predicting training_set_primary_roi_063 
2025-03-13 11:32:59.087681: training_set_primary_roi_063, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:59.461296: predicting training_set_primary_roi_066 
2025-03-13 11:32:59.463024: training_set_primary_roi_066, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:32:59.848174: predicting training_set_primary_roi_069 
2025-03-13 11:32:59.849862: training_set_primary_roi_069, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:00.226921: predicting training_set_primary_roi_071 
2025-03-13 11:33:00.228650: training_set_primary_roi_071, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:00.605659: predicting training_set_primary_roi_080 
2025-03-13 11:33:00.607629: training_set_primary_roi_080, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:00.982589: predicting training_set_primary_roi_081 
2025-03-13 11:33:00.984292: training_set_primary_roi_081, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:01.374645: predicting training_set_primary_roi_088 
2025-03-13 11:33:01.376234: training_set_primary_roi_088, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:01.752715: predicting training_set_primary_roi_100 
2025-03-13 11:33:01.754601: training_set_primary_roi_100, shape torch.Size([3, 1, 1024, 1024]), rank 0 
2025-03-13 11:33:09.012533: Validation complete 
2025-03-13 11:33:09.012743: Mean Validation Dice:  0.6443122616670035 


#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-11 18:03:21.933004: do_dummy_2d_data_aug: False 
2025-03-11 18:03:21.936030: Using splits from existing split file: /data/hotaru/projects/nnUNet/nnunetv2/dataset/nnUNet_preprocessed/Dataset001_puma/splits_final.json 
2025-03-11 18:03:21.936906: The split file contains 5 splits. 
2025-03-11 18:03:21.937084: Desired fold for training: 5 
2025-03-11 18:03:21.937204: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-03-11 18:03:21.940834: This random 80:20 split has 164 training and 41 validation cases. 
2025-03-11 18:03:37.987545: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 13, 'patch_size': [896, 768], 'median_image_size_in_voxels': [1024.0, 1024.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_puma', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 1024, 1024], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 158.0860595703125, 'median': 166.0, 'min': 0.0, 'percentile_00_5': 30.0, 'percentile_99_5': 246.0, 'std': 50.084224700927734}, '1': {'max': 255.0, 'mean': 106.11481475830078, 'median': 101.0, 'min': 0.0, 'percentile_00_5': 15.0, 'percentile_99_5': 225.0, 'std': 49.32137680053711}, '2': {'max': 255.0, 'mean': 178.19647216796875, 'median': 182.0, 'min': 0.0, 'percentile_00_5': 46.0, 'percentile_99_5': 244.0, 'std': 34.66091537475586}}} 
 
2025-03-11 18:03:40.364661: unpacking dataset... 
2025-03-11 18:03:48.595389: unpacking done... 
2025-03-11 18:03:48.602531: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-03-11 18:03:48.775939:  
2025-03-11 18:03:48.776272: Epoch 0 
2025-03-11 18:03:48.776994: Current learning rate: 0.01 
2025-03-11 18:10:44.997281: train_loss 0.8423 
2025-03-11 18:10:44.997690: val_loss 1.0022 
2025-03-11 18:10:44.997813: Pseudo dice [0.0194, 0.0, 0.7626, 0.0, 0.0] 
2025-03-11 18:10:44.997911: Epoch time: 416.23 s 
2025-03-11 18:10:44.997976: Yayy! New best EMA pseudo Dice: 0.1564 
2025-03-11 18:10:47.863048:  
2025-03-11 18:10:47.863397: Epoch 1 
2025-03-11 18:10:47.863664: Current learning rate: 0.00999 
2025-03-11 18:14:21.938080: train_loss 0.5368 
2025-03-11 18:14:21.938429: val_loss 0.8766 
2025-03-11 18:14:21.938567: Pseudo dice [0.4269, 0.0, 0.7949, 0.0, 0.0] 
2025-03-11 18:14:21.938665: Epoch time: 214.08 s 
2025-03-11 18:14:21.938732: Yayy! New best EMA pseudo Dice: 0.1652 
2025-03-11 18:14:26.307321:  
2025-03-11 18:14:26.307645: Epoch 2 
2025-03-11 18:14:26.308162: Current learning rate: 0.00998 
2025-03-11 18:18:01.348812: train_loss 0.4414 
2025-03-11 18:18:01.349318: val_loss 0.7598 
2025-03-11 18:18:01.349852: Pseudo dice [0.5486, 0.0, 0.8449, 0.0, 0.0] 
2025-03-11 18:18:01.350146: Epoch time: 215.05 s 
2025-03-11 18:18:01.350328: Yayy! New best EMA pseudo Dice: 0.1765 
2025-03-11 18:18:06.049506:  
2025-03-11 18:18:06.049990: Epoch 3 
2025-03-11 18:18:06.050232: Current learning rate: 0.00997 
2025-03-11 18:21:40.839762: train_loss 0.3398 
2025-03-11 18:21:40.840179: val_loss 0.6629 
2025-03-11 18:21:40.840444: Pseudo dice [0.5931, 0.0, 0.8805, 0.0, 0.0] 
2025-03-11 18:21:40.840713: Epoch time: 214.8 s 
2025-03-11 18:21:40.840798: Yayy! New best EMA pseudo Dice: 0.1884 
2025-03-11 18:21:45.277776:  
2025-03-11 18:21:45.278185: Epoch 4 
2025-03-11 18:21:45.278410: Current learning rate: 0.00996 
2025-03-11 18:25:21.541660: train_loss 0.1978 
2025-03-11 18:25:21.542096: val_loss 0.7912 
2025-03-11 18:25:21.542261: Pseudo dice [0.5568, 0.0, 0.8426, 0.2045, 0.0] 
2025-03-11 18:25:21.542362: Epoch time: 216.27 s 
2025-03-11 18:25:21.542431: Yayy! New best EMA pseudo Dice: 0.2016 
2025-03-11 18:25:26.128540:  
2025-03-11 18:25:26.129062: Epoch 5 
2025-03-11 18:25:26.129466: Current learning rate: 0.00995 
2025-03-11 18:29:01.294116: train_loss 0.0888 
2025-03-11 18:29:01.294543: val_loss 0.6971 
2025-03-11 18:29:01.294673: Pseudo dice [0.5846, 0.0, 0.8578, 0.2579, 0.0] 
2025-03-11 18:29:01.294774: Epoch time: 215.17 s 
2025-03-11 18:29:01.294837: Yayy! New best EMA pseudo Dice: 0.2154 
2025-03-11 18:29:05.722888:  
2025-03-11 18:29:05.723245: Epoch 6 
2025-03-11 18:29:05.723479: Current learning rate: 0.00995 
2025-03-11 18:32:40.956788: train_loss -0.0099 
2025-03-11 18:32:40.957231: val_loss 0.6813 
2025-03-11 18:32:40.957358: Pseudo dice [0.5732, 0.0, 0.8793, 0.0869, 0.0] 
2025-03-11 18:32:40.957455: Epoch time: 215.24 s 
2025-03-11 18:32:40.957556: Yayy! New best EMA pseudo Dice: 0.2247 
2025-03-11 18:32:45.380970:  
2025-03-11 18:32:45.381462: Epoch 7 
2025-03-11 18:32:45.381736: Current learning rate: 0.00994 
2025-03-11 18:36:20.632576: train_loss -0.0466 
2025-03-11 18:36:20.633002: val_loss 0.6529 
2025-03-11 18:36:20.633131: Pseudo dice [0.6317, 0.0002, 0.8611, 0.1256, 0.0] 
2025-03-11 18:36:20.633230: Epoch time: 215.26 s 
2025-03-11 18:36:20.633299: Yayy! New best EMA pseudo Dice: 0.2346 
2025-03-11 18:36:25.132678:  
2025-03-11 18:36:25.132935: Epoch 8 
2025-03-11 18:36:25.133176: Current learning rate: 0.00993 
2025-03-11 18:40:00.260503: train_loss -0.115 
2025-03-11 18:40:00.260914: val_loss 0.5327 
2025-03-11 18:40:00.261108: Pseudo dice [0.6282, 0.1603, 0.8935, 0.2929, 0.0] 
2025-03-11 18:40:00.261246: Epoch time: 215.13 s 
2025-03-11 18:40:00.261318: Yayy! New best EMA pseudo Dice: 0.2506 
2025-03-11 18:40:04.743768:  
2025-03-11 18:40:04.744184: Epoch 9 
2025-03-11 18:40:04.744421: Current learning rate: 0.00992 
2025-03-11 18:43:40.728392: train_loss -0.2186 
2025-03-11 18:43:40.728823: val_loss 0.7434 
2025-03-11 18:43:40.728943: Pseudo dice [0.6136, 0.0834, 0.8787, 0.1081, 0.0] 
2025-03-11 18:43:40.729038: Epoch time: 215.99 s 
2025-03-11 18:43:40.729106: Yayy! New best EMA pseudo Dice: 0.2592 
2025-03-11 18:43:45.224910:  
2025-03-11 18:43:45.225411: Epoch 10 
2025-03-11 18:43:45.225765: Current learning rate: 0.00991 
2025-03-11 18:47:20.643066: train_loss -0.275 
2025-03-11 18:47:20.643527: val_loss 0.746 
2025-03-11 18:47:20.643766: Pseudo dice [0.5653, 0.1279, 0.8687, 0.2784, 0.0057] 
2025-03-11 18:47:20.644013: Epoch time: 215.43 s 
2025-03-11 18:47:20.644210: Yayy! New best EMA pseudo Dice: 0.2702 
2025-03-11 18:47:24.998202:  
2025-03-11 18:47:24.998513: Epoch 11 
2025-03-11 18:47:24.998763: Current learning rate: 0.0099 
2025-03-11 18:51:00.210329: train_loss -0.333 
2025-03-11 18:51:00.211279: val_loss 0.6796 
2025-03-11 18:51:00.211711: Pseudo dice [0.6213, 0.089, 0.895, 0.2335, 0.0556] 
2025-03-11 18:51:00.212075: Epoch time: 215.22 s 
2025-03-11 18:51:00.212270: Yayy! New best EMA pseudo Dice: 0.2811 
2025-03-11 18:51:04.611257:  
2025-03-11 18:51:04.611596: Epoch 12 
2025-03-11 18:51:04.611804: Current learning rate: 0.00989 
2025-03-11 18:54:39.823869: train_loss -0.3722 
2025-03-11 18:54:39.824178: val_loss 0.774 
2025-03-11 18:54:39.824371: Pseudo dice [0.6682, 0.1036, 0.8773, 0.0759, 0.0071] 
2025-03-11 18:54:39.824507: Epoch time: 215.22 s 
2025-03-11 18:54:39.824582: Yayy! New best EMA pseudo Dice: 0.2876 
2025-03-11 18:54:44.231378:  
2025-03-11 18:54:44.231700: Epoch 13 
2025-03-11 18:54:44.231873: Current learning rate: 0.00988 
2025-03-11 18:58:19.582354: train_loss -0.3825 
2025-03-11 18:58:19.582799: val_loss 0.9476 
2025-03-11 18:58:19.582980: Pseudo dice [0.5946, 0.0843, 0.8605, 0.207, 0.1667] 
2025-03-11 18:58:19.583340: Epoch time: 215.36 s 
2025-03-11 18:58:19.583691: Yayy! New best EMA pseudo Dice: 0.2971 
2025-03-11 18:58:24.414032:  
2025-03-11 18:58:24.414475: Epoch 14 
2025-03-11 18:58:24.414780: Current learning rate: 0.00987 
2025-03-11 19:01:59.693938: train_loss -0.4494 
2025-03-11 19:01:59.694887: val_loss 0.9418 
2025-03-11 19:01:59.695204: Pseudo dice [0.6296, 0.0383, 0.8653, 0.11, 0.0428] 
2025-03-11 19:01:59.695426: Epoch time: 215.29 s 
2025-03-11 19:01:59.695567: Yayy! New best EMA pseudo Dice: 0.3011 
2025-03-11 19:02:04.338425:  
2025-03-11 19:02:04.338770: Epoch 15 
2025-03-11 19:02:04.339029: Current learning rate: 0.00986 
2025-03-11 19:05:40.348368: train_loss -0.4384 
2025-03-11 19:05:40.348770: val_loss 0.9666 
2025-03-11 19:05:40.348887: Pseudo dice [0.5786, 0.1071, 0.8753, 0.1561, 0.07] 
2025-03-11 19:05:40.348989: Epoch time: 216.02 s 
2025-03-11 19:05:40.349056: Yayy! New best EMA pseudo Dice: 0.3068 
2025-03-11 19:05:45.089668:  
2025-03-11 19:05:45.090128: Epoch 16 
2025-03-11 19:05:45.090290: Current learning rate: 0.00986 
2025-03-11 19:09:20.652537: train_loss -0.4375 
2025-03-11 19:09:20.652879: val_loss 0.8742 
2025-03-11 19:09:20.653012: Pseudo dice [0.6418, 0.1162, 0.8907, 0.1702, 0.0251] 
2025-03-11 19:09:20.653272: Epoch time: 215.57 s 
2025-03-11 19:09:20.653401: Yayy! New best EMA pseudo Dice: 0.313 
2025-03-11 19:09:25.356130:  
2025-03-11 19:09:25.356580: Epoch 17 
2025-03-11 19:09:25.356752: Current learning rate: 0.00985 
2025-03-11 19:13:00.551342: train_loss -0.4923 
2025-03-11 19:13:00.551832: val_loss 0.9902 
2025-03-11 19:13:00.552157: Pseudo dice [0.638, 0.042, 0.8906, 0.1452, 0.0198] 
2025-03-11 19:13:00.552382: Epoch time: 215.2 s 
2025-03-11 19:13:00.552455: Yayy! New best EMA pseudo Dice: 0.3164 
2025-03-11 19:13:05.065734:  
2025-03-11 19:13:05.066084: Epoch 18 
2025-03-11 19:13:05.066289: Current learning rate: 0.00984 
2025-03-11 19:16:40.347580: train_loss -0.5091 
2025-03-11 19:16:40.347944: val_loss 1.0717 
2025-03-11 19:16:40.348080: Pseudo dice [0.6368, 0.0328, 0.8895, 0.1078, 0.0086] 
2025-03-11 19:16:40.348185: Epoch time: 215.29 s 
2025-03-11 19:16:40.348254: Yayy! New best EMA pseudo Dice: 0.3183 
2025-03-11 19:16:45.116102:  
2025-03-11 19:16:45.116556: Epoch 19 
2025-03-11 19:16:45.116754: Current learning rate: 0.00983 
2025-03-11 19:20:20.290644: train_loss -0.5373 
2025-03-11 19:20:20.291213: val_loss 1.1619 
2025-03-11 19:20:20.291395: Pseudo dice [0.5698, 0.0608, 0.8785, 0.0903, 0.0197] 
2025-03-11 19:20:20.291634: Epoch time: 215.18 s 
2025-03-11 19:20:20.292055: Yayy! New best EMA pseudo Dice: 0.3188 
2025-03-11 19:20:24.721097:  
2025-03-11 19:20:24.721452: Epoch 20 
2025-03-11 19:20:24.721803: Current learning rate: 0.00982 
2025-03-11 19:24:00.264388: train_loss -0.5426 
2025-03-11 19:24:00.264834: val_loss 1.0646 
2025-03-11 19:24:00.264958: Pseudo dice [0.6592, 0.0404, 0.8813, 0.2137, 0.0032] 
2025-03-11 19:24:00.265063: Epoch time: 215.55 s 
2025-03-11 19:24:00.265129: Yayy! New best EMA pseudo Dice: 0.3229 
2025-03-11 19:24:04.882393:  
2025-03-11 19:24:04.882855: Epoch 21 
2025-03-11 19:24:04.883077: Current learning rate: 0.00981 
2025-03-11 19:27:39.456129: train_loss -0.5679 
2025-03-11 19:27:39.456736: val_loss 0.9519 
2025-03-11 19:27:39.456977: Pseudo dice [0.6502, 0.0951, 0.8921, 0.1911, 0.0052] 
2025-03-11 19:27:39.457195: Epoch time: 214.58 s 
2025-03-11 19:27:39.457338: Yayy! New best EMA pseudo Dice: 0.3273 
2025-03-11 19:27:43.789589:  
2025-03-11 19:27:43.789958: Epoch 22 
2025-03-11 19:27:43.790140: Current learning rate: 0.0098 
2025-03-11 19:31:18.601327: train_loss -0.5924 
2025-03-11 19:31:18.601682: val_loss 1.2275 
2025-03-11 19:31:18.601851: Pseudo dice [0.615, 0.1233, 0.8813, 0.0777, 0.0012] 
2025-03-11 19:31:18.602056: Epoch time: 214.82 s 
2025-03-11 19:31:18.602132: Yayy! New best EMA pseudo Dice: 0.3285 
2025-03-11 19:31:23.261498:  
2025-03-11 19:31:23.261974: Epoch 23 
2025-03-11 19:31:23.262573: Current learning rate: 0.00979 
2025-03-11 19:34:58.296506: train_loss -0.5924 
2025-03-11 19:34:58.296861: val_loss 1.218 
2025-03-11 19:34:58.297033: Pseudo dice [0.6016, 0.0825, 0.8712, 0.2117, 0.014] 
2025-03-11 19:34:58.297451: Epoch time: 215.04 s 
2025-03-11 19:34:58.297830: Yayy! New best EMA pseudo Dice: 0.3313 
2025-03-11 19:35:02.669368:  
2025-03-11 19:35:02.669711: Epoch 24 
2025-03-11 19:35:02.669951: Current learning rate: 0.00978 
2025-03-11 19:38:37.495901: train_loss -0.6079 
2025-03-11 19:38:37.496228: val_loss 1.3413 
2025-03-11 19:38:37.496352: Pseudo dice [0.5638, 0.0971, 0.8681, 0.2275, 0.0074] 
2025-03-11 19:38:37.496560: Epoch time: 214.83 s 
2025-03-11 19:38:37.496685: Yayy! New best EMA pseudo Dice: 0.3334 
2025-03-11 19:38:42.993297:  
2025-03-11 19:38:42.993850: Epoch 25 
2025-03-11 19:38:42.994066: Current learning rate: 0.00977 
2025-03-11 19:42:18.073756: train_loss -0.6032 
2025-03-11 19:42:18.074120: val_loss 1.2176 
2025-03-11 19:42:18.074254: Pseudo dice [0.6262, 0.07, 0.9042, 0.1443, 0.0002] 
2025-03-11 19:42:18.074356: Epoch time: 215.09 s 
2025-03-11 19:42:18.074434: Yayy! New best EMA pseudo Dice: 0.335 
2025-03-11 19:42:22.577636:  
2025-03-11 19:42:22.578014: Epoch 26 
2025-03-11 19:42:22.578305: Current learning rate: 0.00977 
2025-03-11 19:45:58.074806: train_loss -0.6347 
2025-03-11 19:45:58.075249: val_loss 0.9671 
2025-03-11 19:45:58.075425: Pseudo dice [0.653, 0.0506, 0.8969, 0.1415, 0.0001] 
2025-03-11 19:45:58.075625: Epoch time: 215.5 s 
2025-03-11 19:45:58.075795: Yayy! New best EMA pseudo Dice: 0.3363 
2025-03-11 19:46:02.540282:  
2025-03-11 19:46:02.540682: Epoch 27 
2025-03-11 19:46:02.540847: Current learning rate: 0.00976 
2025-03-11 19:49:37.783111: train_loss -0.6366 
2025-03-11 19:49:37.783493: val_loss 1.2078 
2025-03-11 19:49:37.783625: Pseudo dice [0.6506, 0.1169, 0.8828, 0.3229, 0.0259] 
2025-03-11 19:49:37.783730: Epoch time: 215.25 s 
2025-03-11 19:49:37.783797: Yayy! New best EMA pseudo Dice: 0.3427 
2025-03-11 19:49:42.348830:  
2025-03-11 19:49:42.349163: Epoch 28 
2025-03-11 19:49:42.349368: Current learning rate: 0.00975 
2025-03-11 19:53:17.368264: train_loss -0.651 
2025-03-11 19:53:17.368762: val_loss 1.1308 
2025-03-11 19:53:17.369071: Pseudo dice [0.6594, 0.1141, 0.8992, 0.2877, 0.019] 
2025-03-11 19:53:17.369233: Epoch time: 215.02 s 
2025-03-11 19:53:17.369348: Yayy! New best EMA pseudo Dice: 0.348 
2025-03-11 19:53:22.181005:  
2025-03-11 19:53:22.181435: Epoch 29 
2025-03-11 19:53:22.181602: Current learning rate: 0.00974 
2025-03-11 19:56:57.198403: train_loss -0.6077 
2025-03-11 19:56:57.198852: val_loss 1.1417 
2025-03-11 19:56:57.198969: Pseudo dice [0.5894, 0.0438, 0.9035, 0.2101, 0.0059] 
2025-03-11 19:56:57.199070: Epoch time: 215.02 s 
2025-03-11 19:56:57.199135: Yayy! New best EMA pseudo Dice: 0.3483 
2025-03-11 19:57:01.853861:  
2025-03-11 19:57:01.854114: Epoch 30 
2025-03-11 19:57:01.854358: Current learning rate: 0.00973 
2025-03-11 20:00:37.210061: train_loss -0.6054 
2025-03-11 20:00:37.210569: val_loss 1.0323 
2025-03-11 20:00:37.210764: Pseudo dice [0.6574, 0.0515, 0.9052, 0.3144, 0.0193] 
2025-03-11 20:00:37.210916: Epoch time: 215.36 s 
2025-03-11 20:00:37.211034: Yayy! New best EMA pseudo Dice: 0.3524 
2025-03-11 20:00:43.102293:  
2025-03-11 20:00:43.102661: Epoch 31 
2025-03-11 20:00:43.102834: Current learning rate: 0.00972 
2025-03-11 20:04:18.240461: train_loss -0.6357 
2025-03-11 20:04:18.240801: val_loss 1.1239 
2025-03-11 20:04:18.240916: Pseudo dice [0.6185, 0.1216, 0.8849, 0.2753, 0.0008] 
2025-03-11 20:04:18.241015: Epoch time: 215.14 s 
2025-03-11 20:04:18.241082: Yayy! New best EMA pseudo Dice: 0.3552 
2025-03-11 20:04:23.417227:  
2025-03-11 20:04:23.417584: Epoch 32 
2025-03-11 20:04:23.417790: Current learning rate: 0.00971 
2025-03-11 20:07:58.508185: train_loss -0.6646 
2025-03-11 20:07:58.508730: val_loss 1.1944 
2025-03-11 20:07:58.509018: Pseudo dice [0.6766, 0.0468, 0.8972, 0.1513, 0.0007] 
2025-03-11 20:07:58.509174: Epoch time: 215.1 s 
2025-03-11 20:08:00.248943:  
2025-03-11 20:08:00.249248: Epoch 33 
2025-03-11 20:08:00.249475: Current learning rate: 0.0097 
2025-03-11 20:11:35.330481: train_loss -0.6369 
2025-03-11 20:11:35.330858: val_loss 1.3 
2025-03-11 20:11:35.331071: Pseudo dice [0.599, 0.0464, 0.8683, 0.2999, 0.0003] 
2025-03-11 20:11:35.331392: Epoch time: 215.09 s 
2025-03-11 20:11:35.331532: Yayy! New best EMA pseudo Dice: 0.3559 
2025-03-11 20:11:38.856085:  
2025-03-11 20:11:38.856424: Epoch 34 
2025-03-11 20:11:38.856601: Current learning rate: 0.00969 
2025-03-11 20:15:14.616532: train_loss -0.6365 
2025-03-11 20:15:14.616979: val_loss 1.2686 
2025-03-11 20:15:14.617312: Pseudo dice [0.6079, 0.0412, 0.8746, 0.1267, 0.0073] 
2025-03-11 20:15:14.617541: Epoch time: 215.77 s 
2025-03-11 20:15:16.388385:  
2025-03-11 20:15:16.388780: Epoch 35 
2025-03-11 20:15:16.388979: Current learning rate: 0.00968 
2025-03-11 20:18:51.730803: train_loss -0.6461 
2025-03-11 20:18:51.731209: val_loss 1.1206 
2025-03-11 20:18:51.731347: Pseudo dice [0.6239, 0.0436, 0.8894, 0.3868, 0.0289] 
2025-03-11 20:18:51.731447: Epoch time: 215.35 s 
2025-03-11 20:18:51.731514: Yayy! New best EMA pseudo Dice: 0.3575 
2025-03-11 20:18:56.261512:  
2025-03-11 20:18:56.261871: Epoch 36 
2025-03-11 20:18:56.262357: Current learning rate: 0.00968 
2025-03-11 20:22:31.190346: train_loss -0.6592 
2025-03-11 20:22:31.190791: val_loss 1.3015 
2025-03-11 20:22:31.191146: Pseudo dice [0.6139, 0.0308, 0.8927, 0.2705, 0.023] 
2025-03-11 20:22:31.191343: Epoch time: 214.93 s 
2025-03-11 20:22:31.191420: Yayy! New best EMA pseudo Dice: 0.3584 
2025-03-11 20:22:35.692026:  
2025-03-11 20:22:35.692312: Epoch 37 
2025-03-11 20:22:35.692559: Current learning rate: 0.00967 
2025-03-11 20:26:10.447916: train_loss -0.6575 
2025-03-11 20:26:10.448429: val_loss 1.1293 
2025-03-11 20:26:10.448609: Pseudo dice [0.6012, 0.0124, 0.8984, 0.122, 0.0677] 
2025-03-11 20:26:10.448771: Epoch time: 214.76 s 
2025-03-11 20:26:12.844115:  
2025-03-11 20:26:12.844402: Epoch 38 
2025-03-11 20:26:12.844678: Current learning rate: 0.00966 
2025-03-11 20:29:47.964888: train_loss -0.6618 
2025-03-11 20:29:47.965282: val_loss 1.3542 
2025-03-11 20:29:47.965452: Pseudo dice [0.6207, 0.0419, 0.8932, 0.2815, 0.0001] 
2025-03-11 20:29:47.965604: Epoch time: 215.13 s 
2025-03-11 20:29:49.746106:  
2025-03-11 20:29:49.746437: Epoch 39 
2025-03-11 20:29:49.746675: Current learning rate: 0.00965 
2025-03-11 20:33:24.727581: train_loss -0.6613 
2025-03-11 20:33:24.727974: val_loss 1.3315 
2025-03-11 20:33:24.728220: Pseudo dice [0.6097, 0.0658, 0.8814, 0.3239, 0.0008] 
2025-03-11 20:33:24.728373: Epoch time: 214.99 s 
2025-03-11 20:33:24.728442: Yayy! New best EMA pseudo Dice: 0.3596 
2025-03-11 20:33:29.573944:  
2025-03-11 20:33:29.574357: Epoch 40 
2025-03-11 20:33:29.574584: Current learning rate: 0.00964 
2025-03-11 20:37:04.236152: train_loss -0.6555 
2025-03-11 20:37:04.236597: val_loss 1.3881 
2025-03-11 20:37:04.236762: Pseudo dice [0.6186, 0.0875, 0.8858, 0.194, 0.0015] 
2025-03-11 20:37:04.236867: Epoch time: 214.67 s 
2025-03-11 20:37:05.999299:  
2025-03-11 20:37:05.999680: Epoch 41 
2025-03-11 20:37:05.999826: Current learning rate: 0.00963 
2025-03-11 20:40:40.997575: train_loss -0.6638 
2025-03-11 20:40:40.997920: val_loss 1.1721 
2025-03-11 20:40:40.998054: Pseudo dice [0.6856, 0.0484, 0.8896, 0.3, 0.0047] 
2025-03-11 20:40:40.998157: Epoch time: 215.0 s 
2025-03-11 20:40:40.998223: Yayy! New best EMA pseudo Dice: 0.362 
2025-03-11 20:40:45.179723:  
2025-03-11 20:40:45.180049: Epoch 42 
2025-03-11 20:40:45.180267: Current learning rate: 0.00962 
2025-03-11 20:44:20.183096: train_loss -0.6842 
2025-03-11 20:44:20.183526: val_loss 1.4197 
2025-03-11 20:44:20.183784: Pseudo dice [0.6304, 0.0699, 0.8862, 0.1216, 0.0] 
2025-03-11 20:44:20.183996: Epoch time: 215.01 s 
2025-03-11 20:44:21.800564:  
2025-03-11 20:44:21.800871: Epoch 43 
2025-03-11 20:44:21.801131: Current learning rate: 0.00961 
2025-03-11 20:47:57.208237: train_loss -0.6925 
2025-03-11 20:47:57.208673: val_loss 1.2598 
2025-03-11 20:47:57.208843: Pseudo dice [0.6595, 0.0807, 0.8988, 0.3323, 0.0203] 
2025-03-11 20:47:57.209003: Epoch time: 215.41 s 
2025-03-11 20:47:57.209073: Yayy! New best EMA pseudo Dice: 0.3638 
2025-03-11 20:48:01.482881:  
2025-03-11 20:48:01.483260: Epoch 44 
2025-03-11 20:48:01.483446: Current learning rate: 0.0096 
2025-03-11 20:51:37.182784: train_loss -0.6856 
2025-03-11 20:51:37.183203: val_loss 1.273 
2025-03-11 20:51:37.183451: Pseudo dice [0.6271, 0.0918, 0.8956, 0.3105, 0.0015] 
2025-03-11 20:51:37.184068: Epoch time: 215.7 s 
2025-03-11 20:51:37.184427: Yayy! New best EMA pseudo Dice: 0.3659 
2025-03-11 20:51:41.596933:  
2025-03-11 20:51:41.597278: Epoch 45 
2025-03-11 20:51:41.597474: Current learning rate: 0.00959 
2025-03-11 20:55:16.823135: train_loss -0.6057 
2025-03-11 20:55:16.823578: val_loss 1.2024 
2025-03-11 20:55:16.823846: Pseudo dice [0.571, 0.0737, 0.8802, 0.2785, 0.0039] 
2025-03-11 20:55:16.824109: Epoch time: 215.23 s 
2025-03-11 20:55:18.498127:  
2025-03-11 20:55:18.498468: Epoch 46 
2025-03-11 20:55:18.498650: Current learning rate: 0.00959 
2025-03-11 20:58:53.913863: train_loss -0.6169 
2025-03-11 20:58:53.914392: val_loss 1.3733 
2025-03-11 20:58:53.914570: Pseudo dice [0.6325, 0.0659, 0.8893, 0.2081, 0.002] 
2025-03-11 20:58:53.914726: Epoch time: 215.42 s 
2025-03-11 20:58:55.549538:  
2025-03-11 20:58:55.549884: Epoch 47 
2025-03-11 20:58:55.550094: Current learning rate: 0.00958 
2025-03-11 21:02:31.213639: train_loss -0.6659 
2025-03-11 21:02:31.214180: val_loss 1.0511 
2025-03-11 21:02:31.214392: Pseudo dice [0.6634, 0.0769, 0.9015, 0.3586, 0.1773] 
2025-03-11 21:02:31.214581: Epoch time: 215.67 s 
2025-03-11 21:02:31.214751: Yayy! New best EMA pseudo Dice: 0.372 
2025-03-11 21:02:35.984656:  
2025-03-11 21:02:35.985020: Epoch 48 
2025-03-11 21:02:35.985314: Current learning rate: 0.00957 
2025-03-11 21:06:12.186656: train_loss -0.6565 
2025-03-11 21:06:12.187172: val_loss 1.334 
2025-03-11 21:06:12.187375: Pseudo dice [0.6205, 0.0693, 0.8883, 0.2037, 0.0304] 
2025-03-11 21:06:12.187733: Epoch time: 216.21 s 
2025-03-11 21:06:13.993186:  
2025-03-11 21:06:13.993523: Epoch 49 
2025-03-11 21:06:13.993756: Current learning rate: 0.00956 
2025-03-11 21:09:49.747493: train_loss -0.6719 
2025-03-11 21:09:49.747884: val_loss 1.3634 
2025-03-11 21:09:49.748237: Pseudo dice [0.6259, 0.0867, 0.8835, 0.2814, 0.0057] 
2025-03-11 21:09:49.748409: Epoch time: 215.76 s 
2025-03-11 21:09:52.979955:  
2025-03-11 21:09:52.980257: Epoch 50 
2025-03-11 21:09:52.980382: Current learning rate: 0.00955 
2025-03-11 21:13:29.157250: train_loss -0.6709 
2025-03-11 21:13:29.157621: val_loss 1.3238 
2025-03-11 21:13:29.157753: Pseudo dice [0.6142, 0.0333, 0.8739, 0.082, 0.0361] 
2025-03-11 21:13:29.157853: Epoch time: 216.18 s 
2025-03-11 21:13:30.983312:  
2025-03-11 21:13:30.983652: Epoch 51 
2025-03-11 21:13:30.983814: Current learning rate: 0.00954 
2025-03-11 21:17:07.377893: train_loss -0.6478 
2025-03-11 21:17:07.378330: val_loss 1.1833 
2025-03-11 21:17:07.378482: Pseudo dice [0.626, 0.1537, 0.8826, 0.277, 0.0794] 
2025-03-11 21:17:07.378590: Epoch time: 216.4 s 
2025-03-11 21:17:09.080762:  
2025-03-11 21:17:09.081057: Epoch 52 
2025-03-11 21:17:09.081297: Current learning rate: 0.00953 
2025-03-11 21:20:44.586126: train_loss -0.6681 
2025-03-11 21:20:44.587027: val_loss 1.4232 
2025-03-11 21:20:44.587592: Pseudo dice [0.6224, 0.07, 0.8793, 0.2308, 0.1241] 
2025-03-11 21:20:44.587884: Epoch time: 215.51 s 
2025-03-11 21:20:44.588076: Yayy! New best EMA pseudo Dice: 0.3723 
2025-03-11 21:20:49.499729:  
2025-03-11 21:20:49.500207: Epoch 53 
2025-03-11 21:20:49.500479: Current learning rate: 0.00952 
2025-03-11 21:24:25.149880: train_loss -0.6827 
2025-03-11 21:24:25.150211: val_loss 1.0828 
2025-03-11 21:24:25.150387: Pseudo dice [0.6481, 0.0525, 0.8922, 0.247, 0.1951] 
2025-03-11 21:24:25.150547: Epoch time: 215.66 s 
2025-03-11 21:24:25.150736: Yayy! New best EMA pseudo Dice: 0.3758 
2025-03-11 21:24:30.299847:  
2025-03-11 21:24:30.300159: Epoch 54 
2025-03-11 21:24:30.300449: Current learning rate: 0.00951 
2025-03-11 21:28:05.868253: train_loss -0.7043 
2025-03-11 21:28:05.868685: val_loss 1.2717 
2025-03-11 21:28:05.868862: Pseudo dice [0.6251, 0.0122, 0.8989, 0.2988, 0.0016] 
2025-03-11 21:28:05.868967: Epoch time: 215.57 s 
2025-03-11 21:28:07.684160:  
2025-03-11 21:28:07.684470: Epoch 55 
2025-03-11 21:28:07.684709: Current learning rate: 0.0095 
2025-03-11 21:31:43.421884: train_loss -0.6793 
2025-03-11 21:31:43.422285: val_loss 1.3937 
2025-03-11 21:31:43.422564: Pseudo dice [0.6228, 0.0579, 0.8826, 0.2636, 0.0247] 
2025-03-11 21:31:43.422858: Epoch time: 215.74 s 
2025-03-11 21:31:45.130118:  
2025-03-11 21:31:45.130475: Epoch 56 
2025-03-11 21:31:45.130666: Current learning rate: 0.00949 
2025-03-11 21:35:21.172444: train_loss -0.71 
2025-03-11 21:35:21.172922: val_loss 1.3983 
2025-03-11 21:35:21.173091: Pseudo dice [0.629, 0.0408, 0.8852, 0.1868, 0.0085] 
2025-03-11 21:35:21.173254: Epoch time: 216.05 s 
2025-03-11 21:35:22.815536:  
2025-03-11 21:35:22.815891: Epoch 57 
2025-03-11 21:35:22.816067: Current learning rate: 0.00949 
2025-03-11 21:38:58.210897: train_loss -0.7211 
2025-03-11 21:38:58.211376: val_loss 1.4265 
2025-03-11 21:38:58.211671: Pseudo dice [0.6283, 0.1411, 0.8561, 0.2526, 0.0122] 
2025-03-11 21:38:58.211829: Epoch time: 215.4 s 
2025-03-11 21:39:00.018876:  
2025-03-11 21:39:00.019140: Epoch 58 
2025-03-11 21:39:00.019369: Current learning rate: 0.00948 
2025-03-11 21:42:35.802910: train_loss -0.653 
2025-03-11 21:42:35.803282: val_loss 1.3676 
2025-03-11 21:42:35.803462: Pseudo dice [0.6134, 0.1409, 0.8742, 0.3561, 0.0489] 
2025-03-11 21:42:35.803570: Epoch time: 215.79 s 
2025-03-11 21:42:35.803636: Yayy! New best EMA pseudo Dice: 0.376 
2025-03-11 21:42:40.359185:  
2025-03-11 21:42:40.359500: Epoch 59 
2025-03-11 21:42:40.359683: Current learning rate: 0.00947 
2025-03-11 21:46:16.039634: train_loss -0.682 
2025-03-11 21:46:16.040018: val_loss 1.4595 
2025-03-11 21:46:16.040224: Pseudo dice [0.6021, 0.0341, 0.8911, 0.2507, 0.0032] 
2025-03-11 21:46:16.040336: Epoch time: 215.69 s 
2025-03-11 21:46:17.760114:  
2025-03-11 21:46:17.760512: Epoch 60 
2025-03-11 21:46:17.760702: Current learning rate: 0.00946 
2025-03-11 21:49:53.329210: train_loss -0.6905 
2025-03-11 21:49:53.329654: val_loss 1.7313 
2025-03-11 21:49:53.329997: Pseudo dice [0.5955, 0.0946, 0.8564, 0.0919, 0.0002] 
2025-03-11 21:49:53.330249: Epoch time: 215.58 s 
2025-03-11 21:49:54.983745:  
2025-03-11 21:49:54.984031: Epoch 61 
2025-03-11 21:49:54.984250: Current learning rate: 0.00945 
2025-03-11 21:53:30.880274: train_loss -0.6803 
2025-03-11 21:53:30.880702: val_loss 1.3823 
2025-03-11 21:53:30.881015: Pseudo dice [0.5752, 0.0843, 0.8765, 0.3841, 0.0002] 
2025-03-11 21:53:30.881213: Epoch time: 215.9 s 
2025-03-11 21:53:32.587561:  
2025-03-11 21:53:32.587912: Epoch 62 
2025-03-11 21:53:32.588080: Current learning rate: 0.00944 
2025-03-11 21:57:08.263827: train_loss -0.6944 
2025-03-11 21:57:08.264301: val_loss 1.3705 
2025-03-11 21:57:08.264524: Pseudo dice [0.5856, 0.1015, 0.8828, 0.2072, 0.0051] 
2025-03-11 21:57:08.265029: Epoch time: 215.68 s 
2025-03-11 21:57:09.956462:  
2025-03-11 21:57:09.956856: Epoch 63 
2025-03-11 21:57:09.957066: Current learning rate: 0.00943 
2025-03-11 22:00:46.243444: train_loss -0.6809 
2025-03-11 22:00:46.243826: val_loss 1.4939 
2025-03-11 22:00:46.244010: Pseudo dice [0.667, 0.0718, 0.8742, 0.0418, 0.0091] 
2025-03-11 22:00:46.244137: Epoch time: 216.29 s 
2025-03-11 22:00:47.926977:  
2025-03-11 22:00:47.927291: Epoch 64 
2025-03-11 22:00:47.927547: Current learning rate: 0.00942 
2025-03-11 22:04:23.739563: train_loss -0.674 
2025-03-11 22:04:23.740093: val_loss 1.4641 
2025-03-11 22:04:23.740330: Pseudo dice [0.5885, 0.0641, 0.8926, 0.1056, 0.279] 
2025-03-11 22:04:23.740510: Epoch time: 215.82 s 
2025-03-11 22:04:25.407372:  
2025-03-11 22:04:25.407677: Epoch 65 
2025-03-11 22:04:25.407876: Current learning rate: 0.00941 
2025-03-11 22:08:01.091726: train_loss -0.7001 
2025-03-11 22:08:01.092106: val_loss 1.3221 
2025-03-11 22:08:01.092228: Pseudo dice [0.5978, 0.0394, 0.8916, 0.2681, 0.2482] 
2025-03-11 22:08:01.092330: Epoch time: 215.69 s 
2025-03-11 22:08:02.799405:  
2025-03-11 22:08:02.799712: Epoch 66 
2025-03-11 22:08:02.799887: Current learning rate: 0.0094 
2025-03-11 22:11:38.660616: train_loss -0.7199 
2025-03-11 22:11:38.661031: val_loss 1.5118 
2025-03-11 22:11:38.661196: Pseudo dice [0.5873, 0.0411, 0.8797, 0.0613, 0.0011] 
2025-03-11 22:11:38.661388: Epoch time: 215.87 s 
2025-03-11 22:11:40.350965:  
2025-03-11 22:11:40.351342: Epoch 67 
2025-03-11 22:11:40.351539: Current learning rate: 0.00939 
2025-03-11 22:15:16.133075: train_loss -0.7106 
2025-03-11 22:15:16.133411: val_loss 1.3759 
2025-03-11 22:15:16.133543: Pseudo dice [0.5855, 0.0568, 0.8934, 0.2162, 0.0042] 
2025-03-11 22:15:16.133662: Epoch time: 215.79 s 
2025-03-11 22:15:17.971898:  
2025-03-11 22:15:17.972445: Epoch 68 
2025-03-11 22:15:17.972799: Current learning rate: 0.00939 
2025-03-11 22:18:53.964271: train_loss -0.703 
2025-03-11 22:18:53.964745: val_loss 1.3791 
2025-03-11 22:18:53.965041: Pseudo dice [0.6124, 0.037, 0.8903, 0.1004, 0.0001] 
2025-03-11 22:18:53.965214: Epoch time: 216.0 s 
2025-03-11 22:18:55.675698:  
2025-03-11 22:18:55.676072: Epoch 69 
2025-03-11 22:18:55.676273: Current learning rate: 0.00938 
2025-03-11 22:22:31.736504: train_loss -0.7157 
2025-03-11 22:22:31.736979: val_loss 1.2349 
2025-03-11 22:22:31.737288: Pseudo dice [0.6864, 0.0341, 0.887, 0.1416, 0.0269] 
2025-03-11 22:22:31.737505: Epoch time: 216.07 s 
2025-03-11 22:22:34.203588:  
2025-03-11 22:22:34.203933: Epoch 70 
2025-03-11 22:22:34.204112: Current learning rate: 0.00937 
2025-03-11 22:26:10.057995: train_loss -0.7188 
2025-03-11 22:26:10.058393: val_loss 1.4651 
2025-03-11 22:26:10.058587: Pseudo dice [0.6212, 0.0559, 0.8961, 0.2023, 0.057] 
2025-03-11 22:26:10.058695: Epoch time: 215.86 s 
2025-03-11 22:26:11.810136:  
2025-03-11 22:26:11.810512: Epoch 71 
2025-03-11 22:26:11.810676: Current learning rate: 0.00936 
2025-03-11 22:29:47.764271: train_loss -0.7192 
2025-03-11 22:29:47.764781: val_loss 1.5633 
2025-03-11 22:29:47.764953: Pseudo dice [0.6026, 0.0472, 0.8839, 0.214, 0.0544] 
2025-03-11 22:29:47.765102: Epoch time: 215.96 s 
2025-03-11 22:29:49.502610:  
2025-03-11 22:29:49.502911: Epoch 72 
2025-03-11 22:29:49.503108: Current learning rate: 0.00935 
2025-03-11 22:33:25.476266: train_loss -0.7079 
2025-03-11 22:33:25.476746: val_loss 1.1636 
2025-03-11 22:33:25.476933: Pseudo dice [0.6676, 0.0648, 0.9045, 0.298, 0.0654] 
2025-03-11 22:33:25.477113: Epoch time: 215.98 s 
2025-03-11 22:33:27.242318:  
2025-03-11 22:33:27.242619: Epoch 73 
2025-03-11 22:33:27.242789: Current learning rate: 0.00934 
2025-03-11 22:37:03.084084: train_loss -0.7094 
2025-03-11 22:37:03.084616: val_loss 1.4688 
2025-03-11 22:37:03.084837: Pseudo dice [0.6446, 0.0694, 0.8846, 0.3309, 0.0504] 
2025-03-11 22:37:03.085057: Epoch time: 215.85 s 
2025-03-11 22:37:04.880700:  
2025-03-11 22:37:04.881108: Epoch 74 
2025-03-11 22:37:04.881378: Current learning rate: 0.00933 
2025-03-11 22:40:40.922036: train_loss -0.7246 
2025-03-11 22:40:40.922399: val_loss 1.4063 
2025-03-11 22:40:40.922582: Pseudo dice [0.6514, 0.0821, 0.8817, 0.2914, 0.0116] 
2025-03-11 22:40:40.922742: Epoch time: 216.05 s 
2025-03-11 22:40:42.623877:  
2025-03-11 22:40:42.624178: Epoch 75 
2025-03-11 22:40:42.624458: Current learning rate: 0.00932 
2025-03-11 22:44:18.556454: train_loss -0.7299 
2025-03-11 22:44:18.556872: val_loss 1.5735 
2025-03-11 22:44:18.557011: Pseudo dice [0.6224, 0.0504, 0.8808, 0.2748, 0.0007] 
2025-03-11 22:44:18.557111: Epoch time: 215.94 s 
2025-03-11 22:44:20.367309:  
2025-03-11 22:44:20.367703: Epoch 76 
2025-03-11 22:44:20.368080: Current learning rate: 0.00931 
2025-03-11 22:47:56.544591: train_loss -0.7545 
2025-03-11 22:47:56.545036: val_loss 1.2985 
2025-03-11 22:47:56.545174: Pseudo dice [0.6556, 0.0256, 0.8928, 0.1158, 0.0348] 
2025-03-11 22:47:56.545278: Epoch time: 216.18 s 
2025-03-11 22:47:58.279223:  
2025-03-11 22:47:58.279653: Epoch 77 
2025-03-11 22:47:58.279872: Current learning rate: 0.0093 
2025-03-11 22:51:34.246054: train_loss -0.664 
2025-03-11 22:51:34.246440: val_loss 1.3397 
2025-03-11 22:51:34.246638: Pseudo dice [0.5945, 0.0222, 0.8817, 0.1428, 0.3038] 
2025-03-11 22:51:34.246913: Epoch time: 215.97 s 
2025-03-11 22:51:36.023888:  
2025-03-11 22:51:36.024349: Epoch 78 
2025-03-11 22:51:36.024570: Current learning rate: 0.0093 
2025-03-11 22:55:11.956563: train_loss -0.7119 
2025-03-11 22:55:11.957061: val_loss 1.3762 
2025-03-11 22:55:11.957319: Pseudo dice [0.6301, 0.1388, 0.8907, 0.1087, 0.0677] 
2025-03-11 22:55:11.957494: Epoch time: 215.94 s 
2025-03-11 22:55:13.786731:  
2025-03-11 22:55:13.787062: Epoch 79 
2025-03-11 22:55:13.787240: Current learning rate: 0.00929 
2025-03-11 22:58:49.823022: train_loss -0.7269 
2025-03-11 22:58:49.823425: val_loss 1.5502 
2025-03-11 22:58:49.823603: Pseudo dice [0.6258, 0.0819, 0.8872, 0.127, 0.0002] 
2025-03-11 22:58:49.823768: Epoch time: 216.04 s 
2025-03-11 22:58:51.621691:  
2025-03-11 22:58:51.622025: Epoch 80 
2025-03-11 22:58:51.622200: Current learning rate: 0.00928 
2025-03-11 23:02:27.685870: train_loss -0.7397 
2025-03-11 23:02:27.686306: val_loss 1.5436 
2025-03-11 23:02:27.686538: Pseudo dice [0.6388, 0.0742, 0.8858, 0.2609, 0.0042] 
2025-03-11 23:02:27.686731: Epoch time: 216.07 s 
2025-03-11 23:02:29.445508:  
2025-03-11 23:02:29.445885: Epoch 81 
2025-03-11 23:02:29.446096: Current learning rate: 0.00927 
2025-03-11 23:06:05.287521: train_loss -0.7404 
2025-03-11 23:06:05.287832: val_loss 1.5977 
2025-03-11 23:06:05.288018: Pseudo dice [0.6269, 0.0705, 0.872, 0.1194, 0.0099] 
2025-03-11 23:06:05.288213: Epoch time: 215.85 s 
2025-03-11 23:06:07.030885:  
2025-03-11 23:06:07.031234: Epoch 82 
2025-03-11 23:06:07.031391: Current learning rate: 0.00926 
2025-03-11 23:09:42.970002: train_loss -0.7509 
2025-03-11 23:09:42.970574: val_loss 1.2304 
2025-03-11 23:09:42.970776: Pseudo dice [0.6498, 0.1055, 0.9085, 0.1805, 0.0047] 
2025-03-11 23:09:42.970905: Epoch time: 215.95 s 
2025-03-11 23:09:44.604163:  
2025-03-11 23:09:44.604485: Epoch 83 
2025-03-11 23:09:44.604700: Current learning rate: 0.00925 
2025-03-11 23:13:21.063679: train_loss -0.7165 
2025-03-11 23:13:21.064173: val_loss 1.4186 
2025-03-11 23:13:21.064477: Pseudo dice [0.6419, 0.0303, 0.8861, 0.1886, 0.0004] 
2025-03-11 23:13:21.064670: Epoch time: 216.47 s 
2025-03-11 23:13:22.734370:  
2025-03-11 23:13:22.734700: Epoch 84 
2025-03-11 23:13:22.734907: Current learning rate: 0.00924 
2025-03-11 23:16:58.744243: train_loss -0.7402 
2025-03-11 23:16:58.744689: val_loss 1.4959 
2025-03-11 23:16:58.744929: Pseudo dice [0.6149, 0.0813, 0.8875, 0.2233, 0.0057] 
2025-03-11 23:16:58.745053: Epoch time: 216.02 s 
2025-03-11 23:17:00.378654:  
2025-03-11 23:17:00.379065: Epoch 85 
2025-03-11 23:17:00.379324: Current learning rate: 0.00923 
2025-03-11 23:20:36.347152: train_loss -0.7397 
2025-03-11 23:20:36.347649: val_loss 1.3065 
2025-03-11 23:20:36.347837: Pseudo dice [0.63, 0.0764, 0.8965, 0.1325, 0.0226] 
2025-03-11 23:20:36.347943: Epoch time: 215.98 s 
2025-03-11 23:20:38.024773:  
2025-03-11 23:20:38.025118: Epoch 86 
2025-03-11 23:20:38.025298: Current learning rate: 0.00922 
2025-03-11 23:24:14.066484: train_loss -0.7437 
2025-03-11 23:24:14.067051: val_loss 1.36 
2025-03-11 23:24:14.067202: Pseudo dice [0.6226, 0.082, 0.9044, 0.1854, 0.0049] 
2025-03-11 23:24:14.067461: Epoch time: 216.05 s 
2025-03-11 23:24:15.820597:  
2025-03-11 23:24:15.820944: Epoch 87 
2025-03-11 23:24:15.821113: Current learning rate: 0.00921 
2025-03-11 23:27:51.706462: train_loss -0.7485 
2025-03-11 23:27:51.706874: val_loss 1.532 
2025-03-11 23:27:51.706992: Pseudo dice [0.634, 0.0426, 0.89, 0.1996, 0.0007] 
2025-03-11 23:27:51.707102: Epoch time: 215.89 s 
2025-03-11 23:27:53.367540:  
2025-03-11 23:27:53.367953: Epoch 88 
2025-03-11 23:27:53.368212: Current learning rate: 0.0092 
2025-03-11 23:31:29.395683: train_loss -0.7374 
2025-03-11 23:31:29.396160: val_loss 1.7874 
2025-03-11 23:31:29.396349: Pseudo dice [0.609, 0.0521, 0.8762, 0.1829, 0.0021] 
2025-03-11 23:31:29.396452: Epoch time: 216.03 s 
2025-03-11 23:31:31.245384:  
2025-03-11 23:31:31.245917: Epoch 89 
2025-03-11 23:31:31.246162: Current learning rate: 0.0092 
2025-03-11 23:35:07.278932: train_loss -0.6915 
2025-03-11 23:35:07.279346: val_loss 1.4074 
2025-03-11 23:35:07.279469: Pseudo dice [0.6443, 0.0347, 0.883, 0.1424, 0.0] 
2025-03-11 23:35:07.279580: Epoch time: 216.05 s 
2025-03-11 23:35:09.806184:  
2025-03-11 23:35:09.806543: Epoch 90 
2025-03-11 23:35:09.806763: Current learning rate: 0.00919 
2025-03-11 23:38:45.892054: train_loss -0.6875 
2025-03-11 23:38:45.892520: val_loss 1.1891 
2025-03-11 23:38:45.892854: Pseudo dice [0.6269, 0.0559, 0.8552, 0.3764, 0.0312] 
2025-03-11 23:38:45.893039: Epoch time: 216.09 s 
2025-03-11 23:38:47.539068:  
2025-03-11 23:38:47.539419: Epoch 91 
2025-03-11 23:38:47.539541: Current learning rate: 0.00918 
2025-03-11 23:42:23.728606: train_loss -0.656 
2025-03-11 23:42:23.729077: val_loss 1.0776 
2025-03-11 23:42:23.729371: Pseudo dice [0.6042, 0.0993, 0.8801, 0.2733, 0.0084] 
2025-03-11 23:42:23.729599: Epoch time: 216.2 s 
2025-03-11 23:42:25.357547:  
2025-03-11 23:42:25.357915: Epoch 92 
2025-03-11 23:42:25.358119: Current learning rate: 0.00917 
2025-03-11 23:46:01.323619: train_loss -0.6881 
2025-03-11 23:46:01.324060: val_loss 1.2549 
2025-03-11 23:46:01.324193: Pseudo dice [0.6269, 0.0526, 0.8956, 0.2996, 0.0423] 
2025-03-11 23:46:01.324298: Epoch time: 215.97 s 
2025-03-11 23:46:03.132540:  
2025-03-11 23:46:03.132979: Epoch 93 
2025-03-11 23:46:03.133234: Current learning rate: 0.00916 
2025-03-11 23:49:39.044350: train_loss -0.717 
2025-03-11 23:49:39.044867: val_loss 1.4138 
2025-03-11 23:49:39.045109: Pseudo dice [0.5827, 0.0654, 0.8913, 0.0991, 0.0041] 
2025-03-11 23:49:39.045281: Epoch time: 215.92 s 
2025-03-11 23:49:40.671857:  
2025-03-11 23:49:40.672217: Epoch 94 
2025-03-11 23:49:40.672435: Current learning rate: 0.00915 
2025-03-11 23:53:16.701226: train_loss -0.7282 
2025-03-11 23:53:16.701604: val_loss 1.4019 
2025-03-11 23:53:16.701725: Pseudo dice [0.6223, 0.016, 0.873, 0.0827, 0.0489] 
2025-03-11 23:53:16.701827: Epoch time: 216.04 s 
2025-03-11 23:53:18.345439:  
2025-03-11 23:53:18.345741: Epoch 95 
2025-03-11 23:53:18.345990: Current learning rate: 0.00914 
2025-03-11 23:56:54.189368: train_loss -0.7391 
2025-03-11 23:56:54.189860: val_loss 1.4253 
2025-03-11 23:56:54.190061: Pseudo dice [0.6147, 0.0316, 0.892, 0.1717, 0.0021] 
2025-03-11 23:56:54.190295: Epoch time: 215.85 s 
2025-03-11 23:56:55.903642:  
2025-03-11 23:56:55.903917: Epoch 96 
2025-03-11 23:56:55.904145: Current learning rate: 0.00913 
2025-03-12 00:00:31.871239: train_loss -0.737 
2025-03-12 00:00:31.871696: val_loss 1.5046 
2025-03-12 00:00:31.871859: Pseudo dice [0.6263, 0.0316, 0.8919, 0.1653, 0.0037] 
2025-03-12 00:00:31.872000: Epoch time: 215.97 s 
2025-03-12 00:00:34.292439:  
2025-03-12 00:00:34.292831: Epoch 97 
2025-03-12 00:00:34.293145: Current learning rate: 0.00912 
2025-03-12 00:04:10.168385: train_loss -0.7336 
2025-03-12 00:04:10.168905: val_loss 1.6066 
2025-03-12 00:04:10.169143: Pseudo dice [0.6427, 0.0306, 0.8862, 0.0555, 0.0024] 
2025-03-12 00:04:10.169335: Epoch time: 215.88 s 
2025-03-12 00:04:11.894719:  
2025-03-12 00:04:11.895108: Epoch 98 
2025-03-12 00:04:11.895370: Current learning rate: 0.00911 
2025-03-12 00:07:47.932594: train_loss -0.7386 
2025-03-12 00:07:47.933013: val_loss 1.2872 
2025-03-12 00:07:47.933225: Pseudo dice [0.616, 0.0895, 0.8729, 0.1824, 0.0524] 
2025-03-12 00:07:47.933362: Epoch time: 216.04 s 
2025-03-12 00:07:49.690800:  
2025-03-12 00:07:49.691189: Epoch 99 
2025-03-12 00:07:49.691448: Current learning rate: 0.0091 
2025-03-12 00:11:24.980327: train_loss -0.7443 
2025-03-12 00:11:24.980701: val_loss 1.5233 
2025-03-12 00:11:24.980877: Pseudo dice [0.6228, 0.1112, 0.8837, 0.1045, 0.0017] 
2025-03-12 00:11:24.981095: Epoch time: 215.3 s 
2025-03-12 00:11:29.558340:  
2025-03-12 00:11:29.558790: Epoch 100 
2025-03-12 00:11:29.559019: Current learning rate: 0.0091 
2025-03-12 00:15:04.235534: train_loss -0.7255 
2025-03-12 00:15:04.235952: val_loss 1.5768 
2025-03-12 00:15:04.236127: Pseudo dice [0.6002, 0.051, 0.8849, 0.3039, 0.0] 
2025-03-12 00:15:04.236286: Epoch time: 214.68 s 
2025-03-12 00:15:05.916396:  
2025-03-12 00:15:05.916652: Epoch 101 
2025-03-12 00:15:05.916768: Current learning rate: 0.00909 
2025-03-12 00:18:40.661793: train_loss -0.7282 
2025-03-12 00:18:40.662220: val_loss 1.749 
2025-03-12 00:18:40.662570: Pseudo dice [0.5509, 0.0203, 0.8509, 0.3095, 0.0466] 
2025-03-12 00:18:40.662923: Epoch time: 214.75 s 
2025-03-12 00:18:42.325320:  
2025-03-12 00:18:42.325693: Epoch 102 
2025-03-12 00:18:42.325869: Current learning rate: 0.00908 
2025-03-12 00:22:17.426416: train_loss -0.692 
2025-03-12 00:22:17.426769: val_loss 1.4874 
2025-03-12 00:22:17.426899: Pseudo dice [0.6155, 0.0193, 0.8753, 0.2446, 0.0] 
2025-03-12 00:22:17.427115: Epoch time: 215.11 s 
2025-03-12 00:22:19.092350:  
2025-03-12 00:22:19.092736: Epoch 103 
2025-03-12 00:22:19.092975: Current learning rate: 0.00907 
2025-03-12 00:25:54.364542: train_loss -0.7277 
2025-03-12 00:25:54.364957: val_loss 1.4625 
2025-03-12 00:25:54.365141: Pseudo dice [0.6101, 0.0592, 0.8738, 0.2389, 0.029] 
2025-03-12 00:25:54.365311: Epoch time: 215.28 s 
2025-03-12 00:25:56.064979:  
2025-03-12 00:25:56.065286: Epoch 104 
2025-03-12 00:25:56.065524: Current learning rate: 0.00906 
2025-03-12 00:29:31.263912: train_loss -0.728 
2025-03-12 00:29:31.264339: val_loss 1.4003 
2025-03-12 00:29:31.264569: Pseudo dice [0.684, 0.0816, 0.876, 0.2951, 0.0042] 
2025-03-12 00:29:31.265100: Epoch time: 215.2 s 
2025-03-12 00:29:32.930987:  
2025-03-12 00:29:32.931295: Epoch 105 
2025-03-12 00:29:32.931546: Current learning rate: 0.00905 
2025-03-12 00:33:08.132526: train_loss -0.7337 
2025-03-12 00:33:08.133091: val_loss 1.7303 
2025-03-12 00:33:08.133613: Pseudo dice [0.5963, 0.0839, 0.8652, 0.2319, 0.0001] 
2025-03-12 00:33:08.133774: Epoch time: 215.21 s 
2025-03-12 00:33:09.789568:  
2025-03-12 00:33:09.789890: Epoch 106 
2025-03-12 00:33:09.790112: Current learning rate: 0.00904 
2025-03-12 00:36:44.956906: train_loss -0.7186 
2025-03-12 00:36:44.957306: val_loss 1.3706 
2025-03-12 00:36:44.957597: Pseudo dice [0.6129, 0.0433, 0.887, 0.1782, 0.0273] 
2025-03-12 00:36:44.957755: Epoch time: 215.17 s 
2025-03-12 00:36:46.651381:  
2025-03-12 00:36:46.651759: Epoch 107 
2025-03-12 00:36:46.651916: Current learning rate: 0.00903 
2025-03-12 00:40:22.127491: train_loss -0.7312 
2025-03-12 00:40:22.127806: val_loss 1.3993 
2025-03-12 00:40:22.127994: Pseudo dice [0.6615, 0.0476, 0.8846, 0.2583, 0.0003] 
2025-03-12 00:40:22.128149: Epoch time: 215.48 s 
2025-03-12 00:40:23.833753:  
2025-03-12 00:40:23.834108: Epoch 108 
2025-03-12 00:40:23.834358: Current learning rate: 0.00902 
2025-03-12 00:43:59.304380: train_loss -0.7396 
2025-03-12 00:43:59.304788: val_loss 1.3202 
2025-03-12 00:43:59.304981: Pseudo dice [0.6549, 0.0701, 0.8809, 0.2328, 0.0468] 
2025-03-12 00:43:59.305116: Epoch time: 215.48 s 
2025-03-12 00:44:01.038619:  
2025-03-12 00:44:01.038981: Epoch 109 
2025-03-12 00:44:01.039102: Current learning rate: 0.00901 
2025-03-12 00:47:36.386545: train_loss -0.7452 
2025-03-12 00:47:36.386978: val_loss 1.6627 
2025-03-12 00:47:36.387141: Pseudo dice [0.6463, 0.0402, 0.8788, 0.2547, 0.0608] 
2025-03-12 00:47:36.387292: Epoch time: 215.35 s 
2025-03-12 00:47:38.094589:  
2025-03-12 00:47:38.095295: Epoch 110 
2025-03-12 00:47:38.095517: Current learning rate: 0.009 
2025-03-12 00:51:13.963504: train_loss -0.7442 
2025-03-12 00:51:13.963941: val_loss 1.3618 
2025-03-12 00:51:13.964209: Pseudo dice [0.6219, 0.095, 0.8845, 0.2863, 0.0022] 
2025-03-12 00:51:13.964426: Epoch time: 215.88 s 
2025-03-12 00:51:15.681061:  
2025-03-12 00:51:15.681383: Epoch 111 
2025-03-12 00:51:15.681675: Current learning rate: 0.009 
2025-03-12 00:54:51.038290: train_loss -0.7425 
2025-03-12 00:54:51.038638: val_loss 1.868 
2025-03-12 00:54:51.038768: Pseudo dice [0.6042, 0.0201, 0.8633, 0.2578, 0.0] 
2025-03-12 00:54:51.038862: Epoch time: 215.36 s 
2025-03-12 00:54:52.693317:  
2025-03-12 00:54:52.693653: Epoch 112 
2025-03-12 00:54:52.693871: Current learning rate: 0.00899 
2025-03-12 00:58:28.129146: train_loss -0.7481 
2025-03-12 00:58:28.129708: val_loss 1.3872 
2025-03-12 00:58:28.130190: Pseudo dice [0.6258, 0.0481, 0.883, 0.2582, 0.0] 
2025-03-12 00:58:28.130574: Epoch time: 215.44 s 
2025-03-12 00:58:29.809661:  
2025-03-12 00:58:29.809939: Epoch 113 
2025-03-12 00:58:29.810115: Current learning rate: 0.00898 
2025-03-12 01:02:04.932720: train_loss -0.7525 
2025-03-12 01:02:04.933056: val_loss 1.5485 
2025-03-12 01:02:04.933188: Pseudo dice [0.5823, 0.027, 0.8818, 0.3781, 0.0026] 
2025-03-12 01:02:04.933347: Epoch time: 215.13 s 
2025-03-12 01:02:06.587782:  
2025-03-12 01:02:06.588165: Epoch 114 
2025-03-12 01:02:06.588422: Current learning rate: 0.00897 
2025-03-12 01:05:41.746781: train_loss -0.7403 
2025-03-12 01:05:41.747161: val_loss 1.5135 
2025-03-12 01:05:41.747307: Pseudo dice [0.5826, 0.0787, 0.8429, 0.3067, 0.002] 
2025-03-12 01:05:41.747411: Epoch time: 215.17 s 
2025-03-12 01:05:43.420926:  
2025-03-12 01:05:43.421251: Epoch 115 
2025-03-12 01:05:43.421419: Current learning rate: 0.00896 
2025-03-12 01:09:18.619693: train_loss -0.7222 
2025-03-12 01:09:18.620102: val_loss 1.552 
2025-03-12 01:09:18.620261: Pseudo dice [0.6038, 0.0263, 0.8826, 0.2903, 0.0002] 
2025-03-12 01:09:18.620363: Epoch time: 215.2 s 
2025-03-12 01:09:20.363941:  
2025-03-12 01:09:20.364331: Epoch 116 
2025-03-12 01:09:20.364536: Current learning rate: 0.00895 
2025-03-12 01:12:55.435508: train_loss -0.7418 
2025-03-12 01:12:55.435970: val_loss 1.4961 
2025-03-12 01:12:55.436166: Pseudo dice [0.6059, 0.0516, 0.8745, 0.3114, 0.0] 
2025-03-12 01:12:55.436362: Epoch time: 215.08 s 
2025-03-12 01:12:57.168715:  
2025-03-12 01:12:57.169070: Epoch 117 
2025-03-12 01:12:57.169254: Current learning rate: 0.00894 
2025-03-12 01:16:32.804988: train_loss -0.7346 
2025-03-12 01:16:32.805339: val_loss 1.5508 
2025-03-12 01:16:32.805506: Pseudo dice [0.6111, 0.037, 0.883, 0.0907, 0.02] 
2025-03-12 01:16:32.805723: Epoch time: 215.64 s 
2025-03-12 01:16:34.528303:  
2025-03-12 01:16:34.528695: Epoch 118 
2025-03-12 01:16:34.529036: Current learning rate: 0.00893 
2025-03-12 01:20:09.593127: train_loss -0.7472 
2025-03-12 01:20:09.593534: val_loss 1.5951 
2025-03-12 01:20:09.593791: Pseudo dice [0.6282, 0.183, 0.8668, 0.1516, 0.0061] 
2025-03-12 01:20:09.593899: Epoch time: 215.07 s 
2025-03-12 01:20:11.332413:  
2025-03-12 01:20:11.332814: Epoch 119 
2025-03-12 01:20:11.333070: Current learning rate: 0.00892 
2025-03-12 01:23:46.584594: train_loss -0.7274 
2025-03-12 01:23:46.585060: val_loss 1.6453 
2025-03-12 01:23:46.585309: Pseudo dice [0.6185, 0.0963, 0.8758, 0.093, 0.0021] 
2025-03-12 01:23:46.585498: Epoch time: 215.26 s 
2025-03-12 01:23:48.296876:  
2025-03-12 01:23:48.297251: Epoch 120 
2025-03-12 01:23:48.297521: Current learning rate: 0.00891 
2025-03-12 01:27:23.499154: train_loss -0.7252 
2025-03-12 01:27:23.499654: val_loss 1.1863 
2025-03-12 01:27:23.499798: Pseudo dice [0.6387, 0.0766, 0.8866, 0.3809, 0.1462] 
2025-03-12 01:27:23.499902: Epoch time: 215.21 s 
2025-03-12 01:27:25.176272:  
2025-03-12 01:27:25.176675: Epoch 121 
2025-03-12 01:27:25.176811: Current learning rate: 0.0089 
2025-03-12 01:31:00.318890: train_loss -0.7338 
2025-03-12 01:31:00.319293: val_loss 1.4619 
2025-03-12 01:31:00.319409: Pseudo dice [0.6076, 0.0514, 0.8843, 0.2626, 0.0267] 
2025-03-12 01:31:00.319509: Epoch time: 215.15 s 
2025-03-12 01:31:01.993208:  
2025-03-12 01:31:01.993529: Epoch 122 
2025-03-12 01:31:01.993717: Current learning rate: 0.00889 
2025-03-12 01:34:36.953922: train_loss -0.7032 
2025-03-12 01:34:36.954354: val_loss 1.2477 
2025-03-12 01:34:36.954532: Pseudo dice [0.5767, 0.1034, 0.8665, 0.3314, 0.0446] 
2025-03-12 01:34:36.954696: Epoch time: 214.97 s 
2025-03-12 01:34:38.662517:  
2025-03-12 01:34:38.662845: Epoch 123 
2025-03-12 01:34:38.663093: Current learning rate: 0.00889 
2025-03-12 01:38:13.808265: train_loss -0.6108 
2025-03-12 01:38:13.808659: val_loss 1.3346 
2025-03-12 01:38:13.808832: Pseudo dice [0.6109, 0.0477, 0.856, 0.0675, 0.0066] 
2025-03-12 01:38:13.808991: Epoch time: 215.15 s 
2025-03-12 01:38:15.556465:  
2025-03-12 01:38:15.556818: Epoch 124 
2025-03-12 01:38:15.557047: Current learning rate: 0.00888 
2025-03-12 01:41:51.155802: train_loss -0.6681 
2025-03-12 01:41:51.156310: val_loss 1.1694 
2025-03-12 01:41:51.156535: Pseudo dice [0.6061, 0.1485, 0.8754, 0.2817, 0.0192] 
2025-03-12 01:41:51.156756: Epoch time: 215.61 s 
2025-03-12 01:41:53.046717:  
2025-03-12 01:41:53.047078: Epoch 125 
2025-03-12 01:41:53.047271: Current learning rate: 0.00887 
2025-03-12 01:45:28.384269: train_loss -0.7063 
2025-03-12 01:45:28.384828: val_loss 1.355 
2025-03-12 01:45:28.385411: Pseudo dice [0.6192, 0.051, 0.8915, 0.346, 0.0] 
2025-03-12 01:45:28.385794: Epoch time: 215.34 s 
2025-03-12 01:45:30.206884:  
2025-03-12 01:45:30.207190: Epoch 126 
2025-03-12 01:45:30.207423: Current learning rate: 0.00886 
2025-03-12 01:49:05.464018: train_loss -0.7381 
2025-03-12 01:49:05.464451: val_loss 1.1945 
2025-03-12 01:49:05.464583: Pseudo dice [0.6438, 0.1118, 0.8943, 0.2798, 0.0248] 
2025-03-12 01:49:05.464686: Epoch time: 215.26 s 
2025-03-12 01:49:07.222432:  
2025-03-12 01:49:07.222897: Epoch 127 
2025-03-12 01:49:07.223339: Current learning rate: 0.00885 
2025-03-12 01:52:42.315919: train_loss -0.744 
2025-03-12 01:52:42.316275: val_loss 1.2828 
2025-03-12 01:52:42.316411: Pseudo dice [0.6553, 0.0686, 0.8827, 0.2724, 0.0093] 
2025-03-12 01:52:42.316510: Epoch time: 215.1 s 
2025-03-12 01:52:44.047851:  
2025-03-12 01:52:44.048161: Epoch 128 
2025-03-12 01:52:44.048290: Current learning rate: 0.00884 
2025-03-12 01:56:19.046001: train_loss -0.7508 
2025-03-12 01:56:19.046363: val_loss 1.2616 
2025-03-12 01:56:19.046499: Pseudo dice [0.6234, 0.0339, 0.8918, 0.2611, 0.0789] 
2025-03-12 01:56:19.046603: Epoch time: 215.0 s 
2025-03-12 01:56:20.771218:  
2025-03-12 01:56:20.771744: Epoch 129 
2025-03-12 01:56:20.772417: Current learning rate: 0.00883 
2025-03-12 01:59:55.809972: train_loss -0.7358 
2025-03-12 01:59:55.810285: val_loss 1.3799 
2025-03-12 01:59:55.810448: Pseudo dice [0.613, 0.0737, 0.8917, 0.1937, 0.0005] 
2025-03-12 01:59:55.810894: Epoch time: 215.05 s 
2025-03-12 01:59:57.584761:  
2025-03-12 01:59:57.585059: Epoch 130 
2025-03-12 01:59:57.585316: Current learning rate: 0.00882 
2025-03-12 02:03:32.827965: train_loss -0.7486 
2025-03-12 02:03:32.828389: val_loss 1.3841 
2025-03-12 02:03:32.828605: Pseudo dice [0.5867, 0.0631, 0.8739, 0.2541, 0.0267] 
2025-03-12 02:03:32.828714: Epoch time: 215.25 s 
2025-03-12 02:03:35.371811:  
2025-03-12 02:03:35.372152: Epoch 131 
2025-03-12 02:03:35.372347: Current learning rate: 0.00881 
2025-03-12 02:07:10.599485: train_loss -0.7442 
2025-03-12 02:07:10.599867: val_loss 1.7059 
2025-03-12 02:07:10.600138: Pseudo dice [0.5976, 0.0192, 0.8843, 0.1572, 0.001] 
2025-03-12 02:07:10.600260: Epoch time: 215.23 s 
2025-03-12 02:07:12.374460:  
2025-03-12 02:07:12.374734: Epoch 132 
2025-03-12 02:07:12.375152: Current learning rate: 0.0088 
2025-03-12 02:10:47.517011: train_loss -0.7562 
2025-03-12 02:10:47.517327: val_loss 1.5044 
2025-03-12 02:10:47.517445: Pseudo dice [0.596, 0.063, 0.8726, 0.3071, 0.0137] 
2025-03-12 02:10:47.517542: Epoch time: 215.15 s 
2025-03-12 02:10:49.245723:  
2025-03-12 02:10:49.246212: Epoch 133 
2025-03-12 02:10:49.246451: Current learning rate: 0.00879 
2025-03-12 02:14:24.422350: train_loss -0.7623 
2025-03-12 02:14:24.422763: val_loss 1.5322 
2025-03-12 02:14:24.422885: Pseudo dice [0.6722, 0.0251, 0.8894, 0.3089, 0.0407] 
2025-03-12 02:14:24.423372: Epoch time: 215.18 s 
2025-03-12 02:14:26.163580:  
2025-03-12 02:14:26.163972: Epoch 134 
2025-03-12 02:14:26.164178: Current learning rate: 0.00879 
2025-03-12 02:18:01.472171: train_loss -0.7588 
2025-03-12 02:18:01.472615: val_loss 1.5178 
2025-03-12 02:18:01.472837: Pseudo dice [0.5981, 0.0613, 0.8825, 0.3188, 0.0004] 
2025-03-12 02:18:01.472946: Epoch time: 215.31 s 
2025-03-12 02:18:03.276463:  
2025-03-12 02:18:03.276826: Epoch 135 
2025-03-12 02:18:03.277027: Current learning rate: 0.00878 
2025-03-12 02:21:38.446889: train_loss -0.6821 
2025-03-12 02:21:38.447247: val_loss 1.3342 
2025-03-12 02:21:38.447433: Pseudo dice [0.6796, 0.0482, 0.8656, 0.133, 0.0028] 
2025-03-12 02:21:38.447589: Epoch time: 215.18 s 
2025-03-12 02:21:40.233922:  
2025-03-12 02:21:40.234200: Epoch 136 
2025-03-12 02:21:40.234420: Current learning rate: 0.00877 
2025-03-12 02:25:15.223313: train_loss -0.6334 
2025-03-12 02:25:15.223759: val_loss 1.0158 
2025-03-12 02:25:15.223943: Pseudo dice [0.6942, 0.0443, 0.8946, 0.2464, 0.212] 
2025-03-12 02:25:15.224097: Epoch time: 215.0 s 
2025-03-12 02:25:16.963637:  
2025-03-12 02:25:16.964046: Epoch 137 
2025-03-12 02:25:16.964503: Current learning rate: 0.00876 
2025-03-12 02:28:51.948736: train_loss -0.7035 
2025-03-12 02:28:51.949064: val_loss 1.2409 
2025-03-12 02:28:51.949187: Pseudo dice [0.606, 0.0723, 0.8811, 0.2866, 0.0084] 
2025-03-12 02:28:51.949285: Epoch time: 214.99 s 
2025-03-12 02:28:54.420879:  
2025-03-12 02:28:54.421457: Epoch 138 
2025-03-12 02:28:54.421829: Current learning rate: 0.00875 
2025-03-12 02:32:29.432840: train_loss -0.7285 
2025-03-12 02:32:29.433268: val_loss 1.362 
2025-03-12 02:32:29.433439: Pseudo dice [0.6383, 0.056, 0.8774, 0.2815, 0.0009] 
2025-03-12 02:32:29.433542: Epoch time: 215.02 s 
2025-03-12 02:32:31.225758:  
2025-03-12 02:32:31.225972: Epoch 139 
2025-03-12 02:32:31.226139: Current learning rate: 0.00874 
2025-03-12 02:36:06.310539: train_loss -0.7383 
2025-03-12 02:36:06.311013: val_loss 1.6097 
2025-03-12 02:36:06.311190: Pseudo dice [0.6254, 0.0449, 0.8681, 0.1707, 0.1199] 
2025-03-12 02:36:06.311303: Epoch time: 215.09 s 
2025-03-12 02:36:08.036254:  
2025-03-12 02:36:08.036659: Epoch 140 
2025-03-12 02:36:08.036925: Current learning rate: 0.00873 
2025-03-12 02:39:43.299436: train_loss -0.7303 
2025-03-12 02:39:43.299834: val_loss 1.6511 
2025-03-12 02:39:43.300009: Pseudo dice [0.5913, 0.0686, 0.8615, 0.2362, 0.0062] 
2025-03-12 02:39:43.300161: Epoch time: 215.27 s 
2025-03-12 02:39:45.066355:  
2025-03-12 02:39:45.066900: Epoch 141 
2025-03-12 02:39:45.067191: Current learning rate: 0.00872 
2025-03-12 02:43:20.113336: train_loss -0.7288 
2025-03-12 02:43:20.113785: val_loss 1.5518 
2025-03-12 02:43:20.114066: Pseudo dice [0.6346, 0.0521, 0.8765, 0.2752, 0.0003] 
2025-03-12 02:43:20.114253: Epoch time: 215.06 s 
2025-03-12 02:43:21.967035:  
2025-03-12 02:43:21.967391: Epoch 142 
2025-03-12 02:43:21.967623: Current learning rate: 0.00871 
2025-03-12 02:46:57.064233: train_loss -0.7433 
2025-03-12 02:46:57.064684: val_loss 1.6221 
2025-03-12 02:46:57.064860: Pseudo dice [0.61, 0.0314, 0.8748, 0.1848, 0.0152] 
2025-03-12 02:46:57.064975: Epoch time: 215.1 s 
2025-03-12 02:46:58.847518:  
2025-03-12 02:46:58.848011: Epoch 143 
2025-03-12 02:46:58.848205: Current learning rate: 0.0087 
2025-03-12 02:50:33.881187: train_loss -0.7512 
2025-03-12 02:50:33.881523: val_loss 1.3872 
2025-03-12 02:50:33.881655: Pseudo dice [0.6518, 0.059, 0.8928, 0.3163, 0.0396] 
2025-03-12 02:50:33.881765: Epoch time: 215.04 s 
2025-03-12 02:50:35.640482:  
2025-03-12 02:50:35.640914: Epoch 144 
2025-03-12 02:50:35.641236: Current learning rate: 0.00869 
2025-03-12 02:54:10.987224: train_loss -0.7498 
2025-03-12 02:54:10.987644: val_loss 1.2045 
2025-03-12 02:54:10.987819: Pseudo dice [0.6718, 0.0575, 0.8993, 0.2544, 0.0001] 
2025-03-12 02:54:10.987971: Epoch time: 215.35 s 
2025-03-12 02:54:12.783278:  
2025-03-12 02:54:12.783633: Epoch 145 
2025-03-12 02:54:12.783812: Current learning rate: 0.00868 
2025-03-12 02:57:47.717557: train_loss -0.7656 
2025-03-12 02:57:47.717967: val_loss 1.4338 
2025-03-12 02:57:47.718262: Pseudo dice [0.6618, 0.0505, 0.8848, 0.3156, 0.0004] 
2025-03-12 02:57:47.718421: Epoch time: 214.94 s 
2025-03-12 02:57:49.448688:  
2025-03-12 02:57:49.448971: Epoch 146 
2025-03-12 02:57:49.449162: Current learning rate: 0.00868 
2025-03-12 03:01:24.465930: train_loss -0.7535 
2025-03-12 03:01:24.466301: val_loss 1.5283 
2025-03-12 03:01:24.466430: Pseudo dice [0.6053, 0.0696, 0.8856, 0.1959, 0.0014] 
2025-03-12 03:01:24.466530: Epoch time: 215.02 s 
2025-03-12 03:01:26.199008:  
2025-03-12 03:01:26.199393: Epoch 147 
2025-03-12 03:01:26.199577: Current learning rate: 0.00867 
2025-03-12 03:05:01.147504: train_loss -0.7675 
2025-03-12 03:05:01.147937: val_loss 1.4006 
2025-03-12 03:05:01.148085: Pseudo dice [0.6702, 0.0654, 0.8948, 0.1826, 0.0006] 
2025-03-12 03:05:01.148275: Epoch time: 214.95 s 
2025-03-12 03:05:02.917921:  
2025-03-12 03:05:02.918262: Epoch 148 
2025-03-12 03:05:02.918433: Current learning rate: 0.00866 
2025-03-12 03:08:37.762792: train_loss -0.7555 
2025-03-12 03:08:37.763298: val_loss 1.7231 
2025-03-12 03:08:37.763525: Pseudo dice [0.598, 0.0272, 0.8873, 0.2643, 0.0001] 
2025-03-12 03:08:37.763711: Epoch time: 214.85 s 
2025-03-12 03:08:39.587768:  
2025-03-12 03:08:39.588103: Epoch 149 
2025-03-12 03:08:39.588284: Current learning rate: 0.00865 
2025-03-12 03:12:14.544163: train_loss -0.751 
2025-03-12 03:12:14.544693: val_loss 1.1988 
2025-03-12 03:12:14.544954: Pseudo dice [0.6424, 0.0685, 0.9019, 0.1983, 0.0067] 
2025-03-12 03:12:14.545132: Epoch time: 214.96 s 
2025-03-12 03:12:19.594585:  
2025-03-12 03:12:19.594910: Epoch 150 
2025-03-12 03:12:19.595126: Current learning rate: 0.00864 
2025-03-12 03:15:54.539763: train_loss -0.7 
2025-03-12 03:15:54.540056: val_loss 1.3136 
2025-03-12 03:15:54.540164: Pseudo dice [0.6128, 0.1253, 0.8754, 0.2487, 0.0026] 
2025-03-12 03:15:54.540262: Epoch time: 214.95 s 
2025-03-12 03:15:56.951805:  
2025-03-12 03:15:56.952114: Epoch 151 
2025-03-12 03:15:56.952425: Current learning rate: 0.00863 
2025-03-12 03:19:32.138794: train_loss -0.6959 
2025-03-12 03:19:32.139219: val_loss 1.3278 
2025-03-12 03:19:32.139446: Pseudo dice [0.6241, 0.0934, 0.8613, 0.2496, 0.0357] 
2025-03-12 03:19:32.139641: Epoch time: 215.19 s 
2025-03-12 03:19:33.869449:  
2025-03-12 03:19:33.869828: Epoch 152 
2025-03-12 03:19:33.870018: Current learning rate: 0.00862 
2025-03-12 03:23:09.066593: train_loss -0.6874 
2025-03-12 03:23:09.066982: val_loss 1.3237 
2025-03-12 03:23:09.067098: Pseudo dice [0.597, 0.0933, 0.8828, 0.2686, 0.0] 
2025-03-12 03:23:09.067194: Epoch time: 215.2 s 
2025-03-12 03:23:10.829401:  
2025-03-12 03:23:10.829832: Epoch 153 
2025-03-12 03:23:10.829985: Current learning rate: 0.00861 
2025-03-12 03:26:46.112017: train_loss -0.6444 
2025-03-12 03:26:46.112389: val_loss 0.9715 
2025-03-12 03:26:46.112587: Pseudo dice [0.6642, 0.0287, 0.8805, 0.3031, 0.0016] 
2025-03-12 03:26:46.112746: Epoch time: 215.29 s 
2025-03-12 03:26:47.874135:  
2025-03-12 03:26:47.874453: Epoch 154 
2025-03-12 03:26:47.874662: Current learning rate: 0.0086 
2025-03-12 03:30:23.174400: train_loss -0.6719 
2025-03-12 03:30:23.174818: val_loss 1.2311 
2025-03-12 03:30:23.174995: Pseudo dice [0.6572, 0.0433, 0.8775, 0.0957, 0.1944] 
2025-03-12 03:30:23.175149: Epoch time: 215.31 s 
2025-03-12 03:30:24.977298:  
2025-03-12 03:30:24.977517: Epoch 155 
2025-03-12 03:30:24.977640: Current learning rate: 0.00859 
2025-03-12 03:33:59.971109: train_loss -0.7314 
2025-03-12 03:33:59.971571: val_loss 1.2784 
2025-03-12 03:33:59.971859: Pseudo dice [0.5732, 0.0489, 0.891, 0.2929, 0.0] 
2025-03-12 03:33:59.972017: Epoch time: 215.0 s 
2025-03-12 03:34:01.758146:  
2025-03-12 03:34:01.758382: Epoch 156 
2025-03-12 03:34:01.758573: Current learning rate: 0.00858 
2025-03-12 03:37:36.837181: train_loss -0.7469 
2025-03-12 03:37:36.837505: val_loss 1.3355 
2025-03-12 03:37:36.837683: Pseudo dice [0.6178, 0.0229, 0.8868, 0.32, 0.0182] 
2025-03-12 03:37:36.837839: Epoch time: 215.09 s 
2025-03-12 03:37:38.620186:  
2025-03-12 03:37:38.620524: Epoch 157 
2025-03-12 03:37:38.620730: Current learning rate: 0.00858 
2025-03-12 03:41:13.677526: train_loss -0.742 
2025-03-12 03:41:13.678292: val_loss 1.355 
2025-03-12 03:41:13.678472: Pseudo dice [0.6216, 0.0323, 0.8869, 0.2021, 0.0004] 
2025-03-12 03:41:13.678623: Epoch time: 215.06 s 
2025-03-12 03:41:16.168853:  
2025-03-12 03:41:16.169237: Epoch 158 
2025-03-12 03:41:16.169518: Current learning rate: 0.00857 
2025-03-12 03:44:51.056242: train_loss -0.7496 
2025-03-12 03:44:51.056670: val_loss 1.3149 
2025-03-12 03:44:51.056840: Pseudo dice [0.6102, 0.069, 0.8897, 0.284, 0.0895] 
2025-03-12 03:44:51.056944: Epoch time: 214.89 s 
2025-03-12 03:44:52.842555:  
2025-03-12 03:44:52.842932: Epoch 159 
2025-03-12 03:44:52.843299: Current learning rate: 0.00856 
2025-03-12 03:48:27.829052: train_loss -0.7596 
2025-03-12 03:48:27.829386: val_loss 1.5096 
2025-03-12 03:48:27.829519: Pseudo dice [0.5891, 0.0381, 0.8821, 0.27, 0.0223] 
2025-03-12 03:48:27.829629: Epoch time: 214.99 s 
2025-03-12 03:48:29.664321:  
2025-03-12 03:48:29.664783: Epoch 160 
2025-03-12 03:48:29.665050: Current learning rate: 0.00855 
2025-03-12 03:52:04.987454: train_loss -0.7385 
2025-03-12 03:52:04.987840: val_loss 1.4521 
2025-03-12 03:52:04.988007: Pseudo dice [0.6476, 0.0631, 0.8902, 0.1037, 0.0] 
2025-03-12 03:52:04.988107: Epoch time: 215.34 s 
2025-03-12 03:52:06.829341:  
2025-03-12 03:52:06.829659: Epoch 161 
2025-03-12 03:52:06.829898: Current learning rate: 0.00854 
2025-03-12 03:55:41.801187: train_loss -0.7568 
2025-03-12 03:55:41.801613: val_loss 1.5028 
2025-03-12 03:55:41.801776: Pseudo dice [0.6217, 0.0724, 0.8853, 0.233, 0.0244] 
2025-03-12 03:55:41.801939: Epoch time: 214.98 s 
2025-03-12 03:55:43.539069:  
2025-03-12 03:55:43.539395: Epoch 162 
2025-03-12 03:55:43.539577: Current learning rate: 0.00853 
2025-03-12 03:59:18.678242: train_loss -0.7543 
2025-03-12 03:59:18.678707: val_loss 1.3664 
2025-03-12 03:59:18.678956: Pseudo dice [0.6471, 0.0842, 0.8904, 0.1646, 0.0163] 
2025-03-12 03:59:18.679121: Epoch time: 215.15 s 
2025-03-12 03:59:20.495274:  
2025-03-12 03:59:20.495650: Epoch 163 
2025-03-12 03:59:20.495959: Current learning rate: 0.00852 
2025-03-12 04:02:55.653693: train_loss -0.7546 
2025-03-12 04:02:55.654102: val_loss 1.7374 
2025-03-12 04:02:55.654399: Pseudo dice [0.6084, 0.0525, 0.8719, 0.2183, 0.002] 
2025-03-12 04:02:55.654571: Epoch time: 215.16 s 
2025-03-12 04:02:57.416707:  
2025-03-12 04:02:57.417039: Epoch 164 
2025-03-12 04:02:57.417211: Current learning rate: 0.00851 
2025-03-12 04:06:33.095178: train_loss -0.7528 
2025-03-12 04:06:33.095478: val_loss 1.4605 
2025-03-12 04:06:33.095613: Pseudo dice [0.6451, 0.0591, 0.889, 0.1854, 0.0] 
2025-03-12 04:06:33.095715: Epoch time: 215.68 s 
2025-03-12 04:06:34.875255:  
2025-03-12 04:06:34.875487: Epoch 165 
2025-03-12 04:06:34.875612: Current learning rate: 0.0085 
2025-03-12 04:10:09.976613: train_loss -0.7264 
2025-03-12 04:10:09.976983: val_loss 1.0804 
2025-03-12 04:10:09.977170: Pseudo dice [0.6905, 0.0806, 0.9001, 0.2496, 0.0052] 
2025-03-12 04:10:09.977347: Epoch time: 215.11 s 
2025-03-12 04:10:11.668134:  
2025-03-12 04:10:11.668618: Epoch 166 
2025-03-12 04:10:11.669125: Current learning rate: 0.00849 
2025-03-12 04:13:46.401714: train_loss -0.7356 
2025-03-12 04:13:46.402238: val_loss 1.3728 
2025-03-12 04:13:46.402531: Pseudo dice [0.6535, 0.0681, 0.9032, 0.2388, 0.0639] 
2025-03-12 04:13:46.402695: Epoch time: 214.74 s 
2025-03-12 04:13:48.174114:  
2025-03-12 04:13:48.174449: Epoch 167 
2025-03-12 04:13:48.174640: Current learning rate: 0.00848 
2025-03-12 04:17:22.969349: train_loss -0.7604 
2025-03-12 04:17:22.969743: val_loss 1.6084 
2025-03-12 04:17:22.969857: Pseudo dice [0.6276, 0.039, 0.8788, 0.2897, 0.1316] 
2025-03-12 04:17:22.969959: Epoch time: 214.8 s 
2025-03-12 04:17:24.736262:  
2025-03-12 04:17:24.736568: Epoch 168 
2025-03-12 04:17:24.736787: Current learning rate: 0.00847 
2025-03-12 04:20:59.630464: train_loss -0.7516 
2025-03-12 04:20:59.630922: val_loss 1.755 
2025-03-12 04:20:59.631590: Pseudo dice [0.5786, 0.0254, 0.8746, 0.2751, 0.0013] 
2025-03-12 04:20:59.631898: Epoch time: 214.9 s 
2025-03-12 04:21:01.442461:  
2025-03-12 04:21:01.442831: Epoch 169 
2025-03-12 04:21:01.443136: Current learning rate: 0.00847 
2025-03-12 04:24:36.417567: train_loss -0.76 
2025-03-12 04:24:36.417900: val_loss 1.4837 
2025-03-12 04:24:36.418087: Pseudo dice [0.6254, 0.0643, 0.8706, 0.3449, 0.0011] 
2025-03-12 04:24:36.418280: Epoch time: 214.98 s 
2025-03-12 04:24:38.222743:  
2025-03-12 04:24:38.223085: Epoch 170 
2025-03-12 04:24:38.223255: Current learning rate: 0.00846 
2025-03-12 04:28:13.326840: train_loss -0.7415 
2025-03-12 04:28:13.327248: val_loss 1.4587 
2025-03-12 04:28:13.327405: Pseudo dice [0.6579, 0.095, 0.882, 0.1954, 0.0001] 
2025-03-12 04:28:13.327574: Epoch time: 215.11 s 
2025-03-12 04:28:15.860228:  
2025-03-12 04:28:15.860654: Epoch 171 
2025-03-12 04:28:15.860897: Current learning rate: 0.00845 
2025-03-12 04:31:50.922439: train_loss -0.7415 
2025-03-12 04:31:50.922871: val_loss 1.5382 
2025-03-12 04:31:50.923071: Pseudo dice [0.6316, 0.0318, 0.8921, 0.2666, 0.001] 
2025-03-12 04:31:50.923227: Epoch time: 215.07 s 
2025-03-12 04:31:52.712896:  
2025-03-12 04:31:52.713204: Epoch 172 
2025-03-12 04:31:52.713403: Current learning rate: 0.00844 
2025-03-12 04:35:27.734416: train_loss -0.7578 
2025-03-12 04:35:27.734871: val_loss 1.375 
2025-03-12 04:35:27.735149: Pseudo dice [0.6827, 0.0333, 0.8911, 0.25, 0.0019] 
2025-03-12 04:35:27.735383: Epoch time: 215.03 s 
2025-03-12 04:35:29.544237:  
2025-03-12 04:35:29.544585: Epoch 173 
2025-03-12 04:35:29.544792: Current learning rate: 0.00843 
2025-03-12 04:39:04.873422: train_loss -0.6968 
2025-03-12 04:39:04.873867: val_loss 1.5298 
2025-03-12 04:39:04.874062: Pseudo dice [0.4538, 0.0176, 0.8561, 0.1209, 0.0759] 
2025-03-12 04:39:04.874235: Epoch time: 215.34 s 
2025-03-12 04:39:06.596019:  
2025-03-12 04:39:06.596401: Epoch 174 
2025-03-12 04:39:06.596577: Current learning rate: 0.00842 
2025-03-12 04:42:42.028616: train_loss -0.7072 
2025-03-12 04:42:42.029070: val_loss 1.415 
2025-03-12 04:42:42.029181: Pseudo dice [0.6336, 0.0885, 0.8731, 0.315, 0.0006] 
2025-03-12 04:42:42.029285: Epoch time: 215.44 s 
2025-03-12 04:42:43.754510:  
2025-03-12 04:42:43.754868: Epoch 175 
2025-03-12 04:42:43.755041: Current learning rate: 0.00841 
2025-03-12 04:46:19.016546: train_loss -0.7228 
2025-03-12 04:46:19.016883: val_loss 1.3793 
2025-03-12 04:46:19.017011: Pseudo dice [0.5616, 0.0428, 0.8555, 0.2688, 0.0165] 
2025-03-12 04:46:19.017304: Epoch time: 215.27 s 
2025-03-12 04:46:20.736041:  
2025-03-12 04:46:20.736386: Epoch 176 
2025-03-12 04:46:20.736537: Current learning rate: 0.0084 
2025-03-12 04:49:56.080561: train_loss -0.7444 
2025-03-12 04:49:56.080914: val_loss 1.5862 
2025-03-12 04:49:56.081081: Pseudo dice [0.5923, 0.04, 0.8772, 0.3024, 0.0001] 
2025-03-12 04:49:56.081246: Epoch time: 215.35 s 
2025-03-12 04:49:57.803512:  
2025-03-12 04:49:57.803870: Epoch 177 
2025-03-12 04:49:57.804048: Current learning rate: 0.00839 
2025-03-12 04:53:32.964956: train_loss -0.7345 
2025-03-12 04:53:32.965345: val_loss 1.3213 
2025-03-12 04:53:32.965512: Pseudo dice [0.6161, 0.0292, 0.8865, 0.2887, 0.0562] 
2025-03-12 04:53:32.965721: Epoch time: 215.17 s 
2025-03-12 04:53:35.412240:  
2025-03-12 04:53:35.412600: Epoch 178 
2025-03-12 04:53:35.412766: Current learning rate: 0.00838 
2025-03-12 04:57:10.796264: train_loss -0.725 
2025-03-12 04:57:10.796749: val_loss 1.6228 
2025-03-12 04:57:10.796965: Pseudo dice [0.5173, 0.0288, 0.8779, 0.1527, 0.0077] 
2025-03-12 04:57:10.797110: Epoch time: 215.39 s 
2025-03-12 04:57:12.522607:  
2025-03-12 04:57:12.522942: Epoch 179 
2025-03-12 04:57:12.523156: Current learning rate: 0.00837 
2025-03-12 05:00:47.727149: train_loss -0.7624 
2025-03-12 05:00:47.727487: val_loss 1.5658 
2025-03-12 05:00:47.727627: Pseudo dice [0.6214, 0.0261, 0.8828, 0.1945, 0.0225] 
2025-03-12 05:00:47.727739: Epoch time: 215.21 s 
2025-03-12 05:00:49.476172:  
2025-03-12 05:00:49.476520: Epoch 180 
2025-03-12 05:00:49.476873: Current learning rate: 0.00836 
2025-03-12 05:04:24.519922: train_loss -0.7592 
2025-03-12 05:04:24.520254: val_loss 1.4834 
2025-03-12 05:04:24.520380: Pseudo dice [0.5874, 0.0683, 0.8794, 0.1311, 0.0856] 
2025-03-12 05:04:24.520565: Epoch time: 215.05 s 
2025-03-12 05:04:26.269821:  
2025-03-12 05:04:26.270455: Epoch 181 
2025-03-12 05:04:26.270651: Current learning rate: 0.00836 
2025-03-12 05:08:01.489399: train_loss -0.7486 
2025-03-12 05:08:01.489759: val_loss 1.3827 
2025-03-12 05:08:01.489887: Pseudo dice [0.6232, 0.0187, 0.8953, 0.2479, 0.0002] 
2025-03-12 05:08:01.489987: Epoch time: 215.23 s 
2025-03-12 05:08:03.210781:  
2025-03-12 05:08:03.211151: Epoch 182 
2025-03-12 05:08:03.211337: Current learning rate: 0.00835 
2025-03-12 05:11:38.444001: train_loss -0.7505 
2025-03-12 05:11:38.444328: val_loss 1.7196 
2025-03-12 05:11:38.444457: Pseudo dice [0.6115, 0.0443, 0.8777, 0.2728, 0.0007] 
2025-03-12 05:11:38.444624: Epoch time: 215.24 s 
2025-03-12 05:11:40.158307:  
2025-03-12 05:11:40.158520: Epoch 183 
2025-03-12 05:11:40.158684: Current learning rate: 0.00834 
2025-03-12 05:15:15.228206: train_loss -0.7623 
2025-03-12 05:15:15.228540: val_loss 1.511 
2025-03-12 05:15:15.228774: Pseudo dice [0.6115, 0.0455, 0.8899, 0.2836, 0.0004] 
2025-03-12 05:15:15.228935: Epoch time: 215.08 s 
2025-03-12 05:15:17.124582:  
2025-03-12 05:15:17.124984: Epoch 184 
2025-03-12 05:15:17.125205: Current learning rate: 0.00833 
2025-03-12 05:18:52.695355: train_loss -0.7437 
2025-03-12 05:18:52.695827: val_loss 1.3366 
2025-03-12 05:18:52.696018: Pseudo dice [0.6408, 0.0211, 0.891, 0.2993, 0.001] 
2025-03-12 05:18:52.696169: Epoch time: 215.58 s 
2025-03-12 05:18:54.477272:  
2025-03-12 05:18:54.477650: Epoch 185 
2025-03-12 05:18:54.477831: Current learning rate: 0.00832 
2025-03-12 05:22:29.813546: train_loss -0.7695 
2025-03-12 05:22:29.814006: val_loss 1.3689 
2025-03-12 05:22:29.814195: Pseudo dice [0.6408, 0.0587, 0.8912, 0.2607, 0.0202] 
2025-03-12 05:22:29.814374: Epoch time: 215.34 s 
2025-03-12 05:22:31.663512:  
2025-03-12 05:22:31.663953: Epoch 186 
2025-03-12 05:22:31.664250: Current learning rate: 0.00831 
2025-03-12 05:26:06.738021: train_loss -0.7566 
2025-03-12 05:26:06.738416: val_loss 1.3998 
2025-03-12 05:26:06.738565: Pseudo dice [0.668, 0.0669, 0.8929, 0.1926, 0.0] 
2025-03-12 05:26:06.738688: Epoch time: 215.09 s 
2025-03-12 05:26:08.533754:  
2025-03-12 05:26:08.534011: Epoch 187 
2025-03-12 05:26:08.534194: Current learning rate: 0.0083 
2025-03-12 05:29:43.464201: train_loss -0.7692 
2025-03-12 05:29:43.464544: val_loss 1.4795 
2025-03-12 05:29:43.464685: Pseudo dice [0.6431, 0.0565, 0.8786, 0.1592, 0.0146] 
2025-03-12 05:29:43.464794: Epoch time: 214.94 s 
2025-03-12 05:29:45.260069:  
2025-03-12 05:29:45.260404: Epoch 188 
2025-03-12 05:29:45.260572: Current learning rate: 0.00829 
2025-03-12 05:33:20.265627: train_loss -0.7648 
2025-03-12 05:33:20.266255: val_loss 1.4444 
2025-03-12 05:33:20.266481: Pseudo dice [0.6093, 0.0991, 0.8879, 0.3038, 0.0009] 
2025-03-12 05:33:20.266647: Epoch time: 215.01 s 
2025-03-12 05:33:22.023235:  
2025-03-12 05:33:22.023647: Epoch 189 
2025-03-12 05:33:22.023874: Current learning rate: 0.00828 
2025-03-12 05:36:57.028034: train_loss -0.7762 
2025-03-12 05:36:57.028452: val_loss 1.6165 
2025-03-12 05:36:57.028616: Pseudo dice [0.5982, 0.1466, 0.8823, 0.2675, 0.1132] 
2025-03-12 05:36:57.028721: Epoch time: 215.01 s 
2025-03-12 05:36:58.792545:  
2025-03-12 05:36:58.792922: Epoch 190 
2025-03-12 05:36:58.793101: Current learning rate: 0.00827 
2025-03-12 05:40:34.132314: train_loss -0.7733 
2025-03-12 05:40:34.132665: val_loss 1.2277 
2025-03-12 05:40:34.132806: Pseudo dice [0.6805, 0.0445, 0.9069, 0.2359, 0.0] 
2025-03-12 05:40:34.132908: Epoch time: 215.35 s 
2025-03-12 05:40:35.924382:  
2025-03-12 05:40:35.924783: Epoch 191 
2025-03-12 05:40:35.924962: Current learning rate: 0.00826 
2025-03-12 05:44:11.601917: train_loss -0.7481 
2025-03-12 05:44:11.602420: val_loss 1.6253 
2025-03-12 05:44:11.602679: Pseudo dice [0.5827, 0.0696, 0.8767, 0.1376, 0.0001] 
2025-03-12 05:44:11.602885: Epoch time: 215.68 s 
2025-03-12 05:44:13.393020:  
2025-03-12 05:44:13.393415: Epoch 192 
2025-03-12 05:44:13.393614: Current learning rate: 0.00825 
2025-03-12 05:47:48.559952: train_loss -0.7568 
2025-03-12 05:47:48.560373: val_loss 1.3447 
2025-03-12 05:47:48.560565: Pseudo dice [0.5971, 0.0317, 0.8861, 0.3295, 0.0041] 
2025-03-12 05:47:48.560676: Epoch time: 215.17 s 
2025-03-12 05:47:50.360600:  
2025-03-12 05:47:50.361023: Epoch 193 
2025-03-12 05:47:50.361262: Current learning rate: 0.00824 
2025-03-12 05:51:25.493351: train_loss -0.7866 
2025-03-12 05:51:25.493935: val_loss 1.2432 
2025-03-12 05:51:25.494168: Pseudo dice [0.6626, 0.0956, 0.8967, 0.3426, 0.0135] 
2025-03-12 05:51:25.494323: Epoch time: 215.15 s 
2025-03-12 05:51:27.340846:  
2025-03-12 05:51:27.341241: Epoch 194 
2025-03-12 05:51:27.341450: Current learning rate: 0.00824 
2025-03-12 05:55:02.397371: train_loss -0.7813 
2025-03-12 05:55:02.397781: val_loss 1.7033 
2025-03-12 05:55:02.397974: Pseudo dice [0.6288, 0.0512, 0.8807, 0.2688, 0.0] 
2025-03-12 05:55:02.398131: Epoch time: 215.06 s 
2025-03-12 05:55:04.192617:  
2025-03-12 05:55:04.192947: Epoch 195 
2025-03-12 05:55:04.193156: Current learning rate: 0.00823 
2025-03-12 05:58:39.263621: train_loss -0.7682 
2025-03-12 05:58:39.264144: val_loss 1.3226 
2025-03-12 05:58:39.264360: Pseudo dice [0.6725, 0.0596, 0.9012, 0.1365, 0.0007] 
2025-03-12 05:58:39.264522: Epoch time: 215.08 s 
2025-03-12 05:58:41.028075:  
2025-03-12 05:58:41.028455: Epoch 196 
2025-03-12 05:58:41.028626: Current learning rate: 0.00822 
2025-03-12 06:02:16.081329: train_loss -0.7696 
2025-03-12 06:02:16.081684: val_loss 1.4455 
2025-03-12 06:02:16.081846: Pseudo dice [0.6391, 0.0641, 0.8906, 0.3317, 0.0004] 
2025-03-12 06:02:16.082026: Epoch time: 215.06 s 
2025-03-12 06:02:17.945742:  
2025-03-12 06:02:17.946046: Epoch 197 
2025-03-12 06:02:17.946243: Current learning rate: 0.00821 
2025-03-12 06:05:52.894559: train_loss -0.7721 
2025-03-12 06:05:52.894941: val_loss 1.3862 
2025-03-12 06:05:52.895052: Pseudo dice [0.6629, 0.0431, 0.8853, 0.299, 0.001] 
2025-03-12 06:05:52.895157: Epoch time: 214.95 s 
2025-03-12 06:05:55.517873:  
2025-03-12 06:05:55.518199: Epoch 198 
2025-03-12 06:05:55.518402: Current learning rate: 0.0082 
2025-03-12 06:09:30.638430: train_loss -0.7704 
2025-03-12 06:09:30.638891: val_loss 1.5098 
2025-03-12 06:09:30.639075: Pseudo dice [0.6455, 0.0755, 0.8886, 0.2712, 0.0] 
2025-03-12 06:09:30.639374: Epoch time: 215.13 s 
2025-03-12 06:09:32.436815:  
2025-03-12 06:09:32.437213: Epoch 199 
2025-03-12 06:09:32.437394: Current learning rate: 0.00819 
2025-03-12 06:13:07.488445: train_loss -0.7566 
2025-03-12 06:13:07.488955: val_loss 1.6711 
2025-03-12 06:13:07.489215: Pseudo dice [0.6313, 0.0579, 0.8809, 0.2561, 0.0194] 
2025-03-12 06:13:07.489384: Epoch time: 215.06 s 
2025-03-12 06:13:11.777337:  
2025-03-12 06:13:11.777688: Epoch 200 
2025-03-12 06:13:11.777866: Current learning rate: 0.00818 
2025-03-12 06:16:46.549268: train_loss -0.7588 
2025-03-12 06:16:46.549690: val_loss 1.462 
2025-03-12 06:16:46.549883: Pseudo dice [0.6445, 0.0614, 0.8637, 0.1734, 0.025] 
2025-03-12 06:16:46.550062: Epoch time: 214.78 s 
2025-03-12 06:16:48.379573:  
2025-03-12 06:16:48.379937: Epoch 201 
2025-03-12 06:16:48.380253: Current learning rate: 0.00817 
2025-03-12 06:20:23.600570: train_loss -0.7116 
2025-03-12 06:20:23.600984: val_loss 1.3585 
2025-03-12 06:20:23.601107: Pseudo dice [0.6088, 0.0644, 0.8832, 0.3279, 0.0036] 
2025-03-12 06:20:23.601224: Epoch time: 215.23 s 
2025-03-12 06:20:25.452579:  
2025-03-12 06:20:25.453089: Epoch 202 
2025-03-12 06:20:25.453353: Current learning rate: 0.00816 
2025-03-12 06:24:00.563546: train_loss -0.7258 
2025-03-12 06:24:00.563961: val_loss 1.4023 
2025-03-12 06:24:00.564077: Pseudo dice [0.6224, 0.0534, 0.8844, 0.2723, 0.0001] 
2025-03-12 06:24:00.564180: Epoch time: 215.12 s 
2025-03-12 06:24:02.352318:  
2025-03-12 06:24:02.352624: Epoch 203 
2025-03-12 06:24:02.352934: Current learning rate: 0.00815 
2025-03-12 06:27:37.364945: train_loss -0.7261 
2025-03-12 06:27:37.365380: val_loss 1.5101 
2025-03-12 06:27:37.365570: Pseudo dice [0.6203, 0.0322, 0.8837, 0.0454, 0.0009] 
2025-03-12 06:27:37.365674: Epoch time: 215.02 s 
2025-03-12 06:27:39.137896:  
2025-03-12 06:27:39.138252: Epoch 204 
2025-03-12 06:27:39.138546: Current learning rate: 0.00814 
2025-03-12 06:31:14.692362: train_loss -0.7209 
2025-03-12 06:31:14.692678: val_loss 1.6457 
2025-03-12 06:31:14.692805: Pseudo dice [0.5921, 0.1101, 0.8731, 0.2859, 0.0009] 
2025-03-12 06:31:14.708853: Epoch time: 215.56 s 
2025-03-12 06:31:16.508965:  
2025-03-12 06:31:16.509479: Epoch 205 
2025-03-12 06:31:16.509829: Current learning rate: 0.00813 
2025-03-12 06:34:51.419573: train_loss -0.7333 
2025-03-12 06:34:51.420134: val_loss 1.4141 
2025-03-12 06:34:51.420337: Pseudo dice [0.6352, 0.0343, 0.8811, 0.1969, 0.0001] 
2025-03-12 06:34:51.420495: Epoch time: 214.92 s 
2025-03-12 06:34:53.092945:  
2025-03-12 06:34:53.093326: Epoch 206 
2025-03-12 06:34:53.093566: Current learning rate: 0.00813 
2025-03-12 06:38:28.174367: train_loss -0.7242 
2025-03-12 06:38:28.174759: val_loss 1.4557 
2025-03-12 06:38:28.174919: Pseudo dice [0.5824, 0.0703, 0.8857, 0.3682, 0.0445] 
2025-03-12 06:38:28.175072: Epoch time: 215.09 s 
2025-03-12 06:38:29.823920:  
2025-03-12 06:38:29.824262: Epoch 207 
2025-03-12 06:38:29.824463: Current learning rate: 0.00812 
2025-03-12 06:42:05.047554: train_loss -0.7032 
2025-03-12 06:42:05.047978: val_loss 1.247 
2025-03-12 06:42:05.048151: Pseudo dice [0.6072, 0.0743, 0.8775, 0.2085, 0.0636] 
2025-03-12 06:42:05.048313: Epoch time: 215.23 s 
2025-03-12 06:42:06.784341:  
2025-03-12 06:42:06.784597: Epoch 208 
2025-03-12 06:42:06.784767: Current learning rate: 0.00811 
2025-03-12 06:45:41.871341: train_loss -0.7328 
2025-03-12 06:45:41.871755: val_loss 1.5495 
2025-03-12 06:45:41.871929: Pseudo dice [0.5845, 0.0455, 0.8702, 0.1523, 0.0054] 
2025-03-12 06:45:41.872097: Epoch time: 215.09 s 
2025-03-12 06:45:43.604899:  
2025-03-12 06:45:43.605217: Epoch 209 
2025-03-12 06:45:43.605446: Current learning rate: 0.0081 
2025-03-12 06:49:18.739583: train_loss -0.7429 
2025-03-12 06:49:18.740109: val_loss 1.5049 
2025-03-12 06:49:18.740313: Pseudo dice [0.6294, 0.0461, 0.8813, 0.1768, 0.0051] 
2025-03-12 06:49:18.740470: Epoch time: 215.14 s 
2025-03-12 06:49:20.409852:  
2025-03-12 06:49:20.410143: Epoch 210 
2025-03-12 06:49:20.410342: Current learning rate: 0.00809 
2025-03-12 06:52:55.435276: train_loss -0.7458 
2025-03-12 06:52:55.435760: val_loss 1.3384 
2025-03-12 06:52:55.435908: Pseudo dice [0.6101, 0.1422, 0.8855, 0.0628, 0.0281] 
2025-03-12 06:52:55.436011: Epoch time: 215.03 s 
2025-03-12 06:52:57.072797:  
2025-03-12 06:52:57.073151: Epoch 211 
2025-03-12 06:52:57.073360: Current learning rate: 0.00808 
2025-03-12 06:56:32.608229: train_loss -0.7161 
2025-03-12 06:56:32.608660: val_loss 1.432 
2025-03-12 06:56:32.608868: Pseudo dice [0.6369, 0.071, 0.8658, 0.1159, 0.0001] 
2025-03-12 06:56:32.609012: Epoch time: 215.54 s 
2025-03-12 06:56:34.264918:  
2025-03-12 06:56:34.265245: Epoch 212 
2025-03-12 06:56:34.265431: Current learning rate: 0.00807 
2025-03-12 07:00:09.031701: train_loss -0.7391 
2025-03-12 07:00:09.032062: val_loss 1.2029 
2025-03-12 07:00:09.032218: Pseudo dice [0.6196, 0.0399, 0.8738, 0.151, 0.0067] 
2025-03-12 07:00:09.032320: Epoch time: 214.77 s 
2025-03-12 07:00:10.699951:  
2025-03-12 07:00:10.700313: Epoch 213 
2025-03-12 07:00:10.700474: Current learning rate: 0.00806 
2025-03-12 07:03:45.446301: train_loss -0.7378 
2025-03-12 07:03:45.446748: val_loss 1.6012 
2025-03-12 07:03:45.446974: Pseudo dice [0.6003, 0.0459, 0.8796, 0.2128, 0.0227] 
2025-03-12 07:03:45.447201: Epoch time: 214.75 s 
2025-03-12 07:03:47.128170:  
2025-03-12 07:03:47.128512: Epoch 214 
2025-03-12 07:03:47.128680: Current learning rate: 0.00805 
2025-03-12 07:07:21.838131: train_loss -0.7325 
2025-03-12 07:07:21.838647: val_loss 1.361 
2025-03-12 07:07:21.838918: Pseudo dice [0.5822, 0.0324, 0.8501, 0.0493, 0.0518] 
2025-03-12 07:07:21.839073: Epoch time: 214.72 s 
2025-03-12 07:07:23.530133:  
2025-03-12 07:07:23.530526: Epoch 215 
2025-03-12 07:07:23.530786: Current learning rate: 0.00804 
2025-03-12 07:10:58.444827: train_loss -0.664 
2025-03-12 07:10:58.445221: val_loss 1.2378 
2025-03-12 07:10:58.445427: Pseudo dice [0.6292, 0.0801, 0.8804, 0.1921, 0.005] 
2025-03-12 07:10:58.445531: Epoch time: 214.93 s 
2025-03-12 07:11:00.112898:  
2025-03-12 07:11:00.113227: Epoch 216 
2025-03-12 07:11:00.113394: Current learning rate: 0.00803 
2025-03-12 07:14:34.738141: train_loss -0.7263 
2025-03-12 07:14:34.738639: val_loss 1.3509 
2025-03-12 07:14:34.738882: Pseudo dice [0.6237, 0.0634, 0.8837, 0.2276, 0.0194] 
2025-03-12 07:14:34.739076: Epoch time: 214.63 s 
2025-03-12 07:14:36.414795:  
2025-03-12 07:14:36.415071: Epoch 217 
2025-03-12 07:14:36.415261: Current learning rate: 0.00802 
2025-03-12 07:18:11.020304: train_loss -0.7539 
2025-03-12 07:18:11.020733: val_loss 1.5126 
2025-03-12 07:18:11.020855: Pseudo dice [0.6241, 0.0514, 0.8802, 0.2145, 0.0038] 
2025-03-12 07:18:11.020954: Epoch time: 214.61 s 
2025-03-12 07:18:13.389832:  
2025-03-12 07:18:13.390183: Epoch 218 
2025-03-12 07:18:13.390395: Current learning rate: 0.00801 
2025-03-12 07:21:48.062294: train_loss -0.7603 
2025-03-12 07:21:48.062712: val_loss 1.5089 
2025-03-12 07:21:48.063018: Pseudo dice [0.6152, 0.0551, 0.8947, 0.3074, 0.0071] 
2025-03-12 07:21:48.063244: Epoch time: 214.68 s 
2025-03-12 07:21:49.710968:  
2025-03-12 07:21:49.711283: Epoch 219 
2025-03-12 07:21:49.711396: Current learning rate: 0.00801 
2025-03-12 07:25:24.646185: train_loss -0.752 
2025-03-12 07:25:24.646601: val_loss 1.5338 
2025-03-12 07:25:24.646715: Pseudo dice [0.659, 0.0737, 0.8847, 0.0442, 0.0088] 
2025-03-12 07:25:24.646816: Epoch time: 214.94 s 
2025-03-12 07:25:26.304053:  
2025-03-12 07:25:26.304340: Epoch 220 
2025-03-12 07:25:26.304837: Current learning rate: 0.008 
2025-03-12 07:29:01.331285: train_loss -0.7588 
2025-03-12 07:29:01.331648: val_loss 1.2854 
2025-03-12 07:29:01.331778: Pseudo dice [0.6461, 0.0979, 0.8834, 0.3081, 0.0012] 
2025-03-12 07:29:01.331873: Epoch time: 215.03 s 
2025-03-12 07:29:02.993508:  
2025-03-12 07:29:02.993875: Epoch 221 
2025-03-12 07:29:02.994048: Current learning rate: 0.00799 
2025-03-12 07:32:37.787485: train_loss -0.7442 
2025-03-12 07:32:37.787919: val_loss 1.4478 
2025-03-12 07:32:37.788031: Pseudo dice [0.6409, 0.0228, 0.878, 0.3193, 0.0121] 
2025-03-12 07:32:37.788128: Epoch time: 214.8 s 
2025-03-12 07:32:39.442172:  
2025-03-12 07:32:39.442500: Epoch 222 
2025-03-12 07:32:39.442768: Current learning rate: 0.00798 
2025-03-12 07:36:14.296832: train_loss -0.7102 
2025-03-12 07:36:14.297201: val_loss 1.5032 
2025-03-12 07:36:14.297361: Pseudo dice [0.6197, 0.0151, 0.8489, 0.2618, 0.0002] 
2025-03-12 07:36:14.297526: Epoch time: 214.86 s 
2025-03-12 07:36:16.017140:  
2025-03-12 07:36:16.017446: Epoch 223 
2025-03-12 07:36:16.017766: Current learning rate: 0.00797 
2025-03-12 07:39:51.289296: train_loss -0.6313 
2025-03-12 07:39:51.289765: val_loss 1.2326 
2025-03-12 07:39:51.290022: Pseudo dice [0.551, 0.083, 0.864, 0.3092, 0.0018] 
2025-03-12 07:39:51.290155: Epoch time: 215.28 s 
2025-03-12 07:39:52.954904:  
2025-03-12 07:39:52.955247: Epoch 224 
2025-03-12 07:39:52.955467: Current learning rate: 0.00796 
2025-03-12 07:43:27.894922: train_loss -0.6775 
2025-03-12 07:43:27.895462: val_loss 1.1921 
2025-03-12 07:43:27.895636: Pseudo dice [0.6301, 0.128, 0.8717, 0.2001, 0.014] 
2025-03-12 07:43:27.895786: Epoch time: 214.95 s 
2025-03-12 07:43:30.229565:  
2025-03-12 07:43:30.229867: Epoch 225 
2025-03-12 07:43:30.230088: Current learning rate: 0.00795 
2025-03-12 07:47:05.279313: train_loss -0.7212 
2025-03-12 07:47:05.279825: val_loss 1.6296 
2025-03-12 07:47:05.280006: Pseudo dice [0.5831, 0.0884, 0.8746, 0.0683, 0.0013] 
2025-03-12 07:47:05.280115: Epoch time: 215.05 s 
2025-03-12 07:47:06.963196:  
2025-03-12 07:47:06.963605: Epoch 226 
2025-03-12 07:47:06.963763: Current learning rate: 0.00794 
2025-03-12 07:50:41.962883: train_loss -0.7394 
2025-03-12 07:50:41.963289: val_loss 1.2892 
2025-03-12 07:50:41.963476: Pseudo dice [0.6351, 0.115, 0.8934, 0.2429, 0.003] 
2025-03-12 07:50:41.963588: Epoch time: 215.01 s 
2025-03-12 07:50:43.666282:  
2025-03-12 07:50:43.666697: Epoch 227 
2025-03-12 07:50:43.666869: Current learning rate: 0.00793 
2025-03-12 07:54:18.733530: train_loss -0.734 
2025-03-12 07:54:18.733870: val_loss 1.3504 
2025-03-12 07:54:18.734041: Pseudo dice [0.6434, 0.0577, 0.8957, 0.3139, 0.0037] 
2025-03-12 07:54:18.734319: Epoch time: 215.07 s 
2025-03-12 07:54:20.377342:  
2025-03-12 07:54:20.377731: Epoch 228 
2025-03-12 07:54:20.377891: Current learning rate: 0.00792 
2025-03-12 07:57:55.327940: train_loss -0.7629 
2025-03-12 07:57:55.328304: val_loss 1.1941 
2025-03-12 07:57:55.328427: Pseudo dice [0.6535, 0.0532, 0.9065, 0.3472, 0.0007] 
2025-03-12 07:57:55.328525: Epoch time: 214.96 s 
2025-03-12 07:57:56.991474:  
2025-03-12 07:57:56.991799: Epoch 229 
2025-03-12 07:57:56.991982: Current learning rate: 0.00791 
2025-03-12 08:01:31.709768: train_loss -0.7601 
2025-03-12 08:01:31.710190: val_loss 1.2355 
2025-03-12 08:01:31.710349: Pseudo dice [0.6467, 0.0593, 0.8978, 0.2751, 0.001] 
2025-03-12 08:01:31.710598: Epoch time: 214.72 s 
2025-03-12 08:01:33.358482:  
2025-03-12 08:01:33.358801: Epoch 230 
2025-03-12 08:01:33.358984: Current learning rate: 0.0079 
2025-03-12 08:05:08.025315: train_loss -0.7491 
2025-03-12 08:05:08.025678: val_loss 1.1855 
2025-03-12 08:05:08.025857: Pseudo dice [0.6475, 0.0553, 0.8873, 0.4105, 0.0072] 
2025-03-12 08:05:08.026041: Epoch time: 214.67 s 
2025-03-12 08:05:09.786264:  
2025-03-12 08:05:09.786642: Epoch 231 
2025-03-12 08:05:09.786784: Current learning rate: 0.00789 
2025-03-12 08:08:44.589314: train_loss -0.7524 
2025-03-12 08:08:44.589788: val_loss 1.247 
2025-03-12 08:08:44.589950: Pseudo dice [0.6861, 0.1108, 0.8975, 0.1948, 0.0] 
2025-03-12 08:08:44.590051: Epoch time: 214.81 s 
2025-03-12 08:08:46.951419:  
2025-03-12 08:08:46.951720: Epoch 232 
2025-03-12 08:08:46.951950: Current learning rate: 0.00789 
2025-03-12 08:12:21.965849: train_loss -0.7526 
2025-03-12 08:12:21.966219: val_loss 1.2618 
2025-03-12 08:12:21.966383: Pseudo dice [0.6517, 0.0966, 0.8874, 0.2864, 0.0143] 
2025-03-12 08:12:21.966489: Epoch time: 215.02 s 
2025-03-12 08:12:23.590925:  
2025-03-12 08:12:23.591261: Epoch 233 
2025-03-12 08:12:23.591452: Current learning rate: 0.00788 
2025-03-12 08:15:58.890750: train_loss -0.7781 
2025-03-12 08:15:58.891103: val_loss 1.6598 
2025-03-12 08:15:58.891285: Pseudo dice [0.5922, 0.0744, 0.8697, 0.1686, 0.0026] 
2025-03-12 08:15:58.891441: Epoch time: 215.31 s 
2025-03-12 08:16:00.532243:  
2025-03-12 08:16:00.532619: Epoch 234 
2025-03-12 08:16:00.532822: Current learning rate: 0.00787 
2025-03-12 08:19:35.720060: train_loss -0.7535 
2025-03-12 08:19:35.720410: val_loss 1.6362 
2025-03-12 08:19:35.720538: Pseudo dice [0.4943, 0.1189, 0.8609, 0.163, 0.0026] 
2025-03-12 08:19:35.720647: Epoch time: 215.19 s 
2025-03-12 08:19:37.432910:  
2025-03-12 08:19:37.433208: Epoch 235 
2025-03-12 08:19:37.433407: Current learning rate: 0.00786 
2025-03-12 08:23:12.611083: train_loss -0.7308 
2025-03-12 08:23:12.611557: val_loss 1.3226 
2025-03-12 08:23:12.611894: Pseudo dice [0.6469, 0.0302, 0.8972, 0.1889, 0.0002] 
2025-03-12 08:23:12.612108: Epoch time: 215.18 s 
2025-03-12 08:23:14.248663:  
2025-03-12 08:23:14.248981: Epoch 236 
2025-03-12 08:23:14.249166: Current learning rate: 0.00785 
2025-03-12 08:26:49.394186: train_loss -0.748 
2025-03-12 08:26:49.394480: val_loss 1.4061 
2025-03-12 08:26:49.394629: Pseudo dice [0.6618, 0.091, 0.8799, 0.2961, 0.0004] 
2025-03-12 08:26:49.394947: Epoch time: 215.15 s 
2025-03-12 08:26:51.047062:  
2025-03-12 08:26:51.047370: Epoch 237 
2025-03-12 08:26:51.047483: Current learning rate: 0.00784 
2025-03-12 08:30:25.996674: train_loss -0.7507 
2025-03-12 08:30:25.996984: val_loss 1.4503 
2025-03-12 08:30:25.997099: Pseudo dice [0.6049, 0.1598, 0.8857, 0.3047, 0.0016] 
2025-03-12 08:30:25.997246: Epoch time: 214.96 s 
2025-03-12 08:30:27.667673:  
2025-03-12 08:30:27.668050: Epoch 238 
2025-03-12 08:30:27.668321: Current learning rate: 0.00783 
2025-03-12 08:34:02.917250: train_loss -0.7732 
2025-03-12 08:34:02.917701: val_loss 1.4079 
2025-03-12 08:34:02.917862: Pseudo dice [0.6482, 0.0773, 0.8911, 0.2861, 0.0044] 
2025-03-12 08:34:02.917967: Epoch time: 215.26 s 
2025-03-12 08:34:05.244404:  
2025-03-12 08:34:05.244811: Epoch 239 
2025-03-12 08:34:05.244977: Current learning rate: 0.00782 
2025-03-12 08:37:40.424327: train_loss -0.7658 
2025-03-12 08:37:40.424786: val_loss 1.7393 
2025-03-12 08:37:40.425109: Pseudo dice [0.5845, 0.0828, 0.8761, 0.0308, 0.0021] 
2025-03-12 08:37:40.425315: Epoch time: 215.19 s 
2025-03-12 08:37:42.074984:  
2025-03-12 08:37:42.075400: Epoch 240 
2025-03-12 08:37:42.075567: Current learning rate: 0.00781 
2025-03-12 08:41:17.384863: train_loss -0.7776 
2025-03-12 08:41:17.385347: val_loss 1.3729 
2025-03-12 08:41:17.385617: Pseudo dice [0.6212, 0.0604, 0.8923, 0.1565, 0.0043] 
2025-03-12 08:41:17.385784: Epoch time: 215.32 s 
2025-03-12 08:41:19.050216:  
2025-03-12 08:41:19.050509: Epoch 241 
2025-03-12 08:41:19.050746: Current learning rate: 0.0078 
2025-03-12 08:44:54.294854: train_loss -0.7607 
2025-03-12 08:44:54.295354: val_loss 1.3229 
2025-03-12 08:44:54.295604: Pseudo dice [0.6492, 0.0718, 0.8958, 0.1797, 0.0015] 
2025-03-12 08:44:54.295765: Epoch time: 215.25 s 
2025-03-12 08:44:55.947835:  
2025-03-12 08:44:55.948191: Epoch 242 
2025-03-12 08:44:55.948421: Current learning rate: 0.00779 
2025-03-12 08:48:31.191365: train_loss -0.7693 
2025-03-12 08:48:31.191704: val_loss 1.3936 
2025-03-12 08:48:31.191892: Pseudo dice [0.6269, 0.0784, 0.8823, 0.2051, 0.0005] 
2025-03-12 08:48:31.192031: Epoch time: 215.25 s 
2025-03-12 08:48:32.875119:  
2025-03-12 08:48:32.875464: Epoch 243 
2025-03-12 08:48:32.875682: Current learning rate: 0.00778 
2025-03-12 08:52:07.867921: train_loss -0.7842 
2025-03-12 08:52:07.868343: val_loss 1.3034 
2025-03-12 08:52:07.868594: Pseudo dice [0.6276, 0.0558, 0.8984, 0.319, 0.002] 
2025-03-12 08:52:07.868777: Epoch time: 215.0 s 
2025-03-12 08:52:09.580514:  
2025-03-12 08:52:09.580818: Epoch 244 
2025-03-12 08:52:09.581085: Current learning rate: 0.00777 
2025-03-12 08:55:44.561632: train_loss -0.7803 
2025-03-12 08:55:44.561993: val_loss 1.4361 
2025-03-12 08:55:44.562317: Pseudo dice [0.6552, 0.0495, 0.9083, 0.1851, 0.0] 
2025-03-12 08:55:44.562483: Epoch time: 214.99 s 
2025-03-12 08:55:46.222537:  
2025-03-12 08:55:46.222940: Epoch 245 
2025-03-12 08:55:46.223105: Current learning rate: 0.00777 
2025-03-12 08:59:21.233492: train_loss -0.7619 
2025-03-12 08:59:21.233843: val_loss 1.1732 
2025-03-12 08:59:21.233967: Pseudo dice [0.6468, 0.0407, 0.9078, 0.2757, 0.0] 
2025-03-12 08:59:21.234070: Epoch time: 215.02 s 
2025-03-12 08:59:23.651309:  
2025-03-12 08:59:23.651725: Epoch 246 
2025-03-12 08:59:23.651988: Current learning rate: 0.00776 
2025-03-12 09:02:58.738178: train_loss -0.7603 
2025-03-12 09:02:58.738618: val_loss 1.4008 
2025-03-12 09:02:58.738751: Pseudo dice [0.6065, 0.1222, 0.8844, 0.1994, 0.0006] 
2025-03-12 09:02:58.738851: Epoch time: 215.09 s 
2025-03-12 09:03:00.514125:  
2025-03-12 09:03:00.514583: Epoch 247 
2025-03-12 09:03:00.514870: Current learning rate: 0.00775 
2025-03-12 09:06:35.608120: train_loss -0.7645 
2025-03-12 09:06:35.608515: val_loss 1.1314 
2025-03-12 09:06:35.608642: Pseudo dice [0.6592, 0.0996, 0.8891, 0.1441, 0.0] 
2025-03-12 09:06:35.608743: Epoch time: 215.1 s 
2025-03-12 09:06:37.282230:  
2025-03-12 09:06:37.282620: Epoch 248 
2025-03-12 09:06:37.282801: Current learning rate: 0.00774 
2025-03-12 09:10:12.239083: train_loss -0.7558 
2025-03-12 09:10:12.239511: val_loss 1.4132 
2025-03-12 09:10:12.239647: Pseudo dice [0.5873, 0.0615, 0.8757, 0.172, 0.0022] 
2025-03-12 09:10:12.239754: Epoch time: 214.96 s 
2025-03-12 09:10:13.962712:  
2025-03-12 09:10:13.963015: Epoch 249 
2025-03-12 09:10:13.963260: Current learning rate: 0.00773 
2025-03-12 09:13:49.060850: train_loss -0.7701 
2025-03-12 09:13:49.061260: val_loss 1.743 
2025-03-12 09:13:49.061455: Pseudo dice [0.5585, 0.0469, 0.8719, 0.0274, 0.0002] 
2025-03-12 09:13:49.061628: Epoch time: 215.1 s 
2025-03-12 09:13:53.620099:  
2025-03-12 09:13:53.620486: Epoch 250 
2025-03-12 09:13:53.620773: Current learning rate: 0.00772 
2025-03-12 09:17:28.588955: train_loss -0.7152 
2025-03-12 09:17:28.589319: val_loss 1.3527 
2025-03-12 09:17:28.589452: Pseudo dice [0.6596, 0.1057, 0.879, 0.2393, 0.0011] 
2025-03-12 09:17:28.589564: Epoch time: 214.98 s 
2025-03-12 09:17:30.319743:  
2025-03-12 09:17:30.320180: Epoch 251 
2025-03-12 09:17:30.320373: Current learning rate: 0.00771 
2025-03-12 09:21:05.285023: train_loss -0.7471 
2025-03-12 09:21:05.285530: val_loss 1.6606 
2025-03-12 09:21:05.285740: Pseudo dice [0.6127, 0.0723, 0.8689, 0.1158, 0.0045] 
2025-03-12 09:21:05.285898: Epoch time: 214.98 s 
2025-03-12 09:21:06.974393:  
2025-03-12 09:21:06.974822: Epoch 252 
2025-03-12 09:21:06.975087: Current learning rate: 0.0077 
2025-03-12 09:24:42.209930: train_loss -0.7606 
2025-03-12 09:24:42.210314: val_loss 1.2459 
2025-03-12 09:24:42.210469: Pseudo dice [0.6493, 0.1148, 0.9062, 0.2796, 0.0022] 
2025-03-12 09:24:42.210584: Epoch time: 215.25 s 
2025-03-12 09:24:43.928046:  
2025-03-12 09:24:43.928389: Epoch 253 
2025-03-12 09:24:43.928581: Current learning rate: 0.00769 
2025-03-12 09:28:18.882779: train_loss -0.7628 
2025-03-12 09:28:18.883118: val_loss 1.6309 
2025-03-12 09:28:18.883241: Pseudo dice [0.6146, 0.0846, 0.8837, 0.1612, 0.0052] 
2025-03-12 09:28:18.883342: Epoch time: 214.96 s 
2025-03-12 09:28:20.573099:  
2025-03-12 09:28:20.573657: Epoch 254 
2025-03-12 09:28:20.573945: Current learning rate: 0.00768 
2025-03-12 09:31:55.565785: train_loss -0.7809 
2025-03-12 09:31:55.566205: val_loss 1.5148 
2025-03-12 09:31:55.566401: Pseudo dice [0.6369, 0.1212, 0.8967, 0.2139, 0.0004] 
2025-03-12 09:31:55.566581: Epoch time: 215.01 s 
2025-03-12 09:31:57.302180:  
2025-03-12 09:31:57.302597: Epoch 255 
2025-03-12 09:31:57.302807: Current learning rate: 0.00767 
2025-03-12 09:35:32.221879: train_loss -0.7831 
2025-03-12 09:35:32.222323: val_loss 1.458 
2025-03-12 09:35:32.222501: Pseudo dice [0.6398, 0.0564, 0.8802, 0.1918, 0.0166] 
2025-03-12 09:35:32.222687: Epoch time: 214.93 s 
2025-03-12 09:35:33.897320:  
2025-03-12 09:35:33.897646: Epoch 256 
2025-03-12 09:35:33.897888: Current learning rate: 0.00766 
2025-03-12 09:39:08.839987: train_loss -0.7682 
2025-03-12 09:39:08.840402: val_loss 1.5975 
2025-03-12 09:39:08.840564: Pseudo dice [0.613, 0.0906, 0.8714, 0.2598, 0.0087] 
2025-03-12 09:39:08.840671: Epoch time: 214.95 s 
2025-03-12 09:39:10.600449:  
2025-03-12 09:39:10.600840: Epoch 257 
2025-03-12 09:39:10.601002: Current learning rate: 0.00765 
2025-03-12 09:42:45.499188: train_loss -0.7777 
2025-03-12 09:42:45.499660: val_loss 1.8035 
2025-03-12 09:42:45.499915: Pseudo dice [0.5592, 0.0644, 0.8505, 0.1415, 0.0] 
2025-03-12 09:42:45.500082: Epoch time: 214.9 s 
2025-03-12 09:42:47.211927:  
2025-03-12 09:42:47.212310: Epoch 258 
2025-03-12 09:42:47.212475: Current learning rate: 0.00764 
2025-03-12 09:46:22.247933: train_loss -0.7614 
2025-03-12 09:46:22.248358: val_loss 1.6833 
2025-03-12 09:46:22.248505: Pseudo dice [0.6221, 0.0936, 0.8755, 0.2337, 0.0019] 
2025-03-12 09:46:22.248693: Epoch time: 215.04 s 
2025-03-12 09:46:23.970464:  
2025-03-12 09:46:23.970802: Epoch 259 
2025-03-12 09:46:23.970978: Current learning rate: 0.00764 
2025-03-12 09:49:59.422055: train_loss -0.7725 
2025-03-12 09:49:59.422573: val_loss 1.507 
2025-03-12 09:49:59.422826: Pseudo dice [0.6123, 0.0382, 0.8812, 0.1179, 0.0019] 
2025-03-12 09:49:59.423025: Epoch time: 215.46 s 
2025-03-12 09:50:01.117466:  
2025-03-12 09:50:01.117940: Epoch 260 
2025-03-12 09:50:01.118161: Current learning rate: 0.00763 
2025-03-12 09:53:35.373531: train_loss -0.7668 
2025-03-12 09:53:35.373886: val_loss 1.4849 
2025-03-12 09:53:35.374013: Pseudo dice [0.5925, 0.0619, 0.8758, 0.1883, 0.0] 
2025-03-12 09:53:35.374110: Epoch time: 214.26 s 
2025-03-12 09:53:37.086780:  
2025-03-12 09:53:37.087134: Epoch 261 
2025-03-12 09:53:37.087370: Current learning rate: 0.00762 
2025-03-12 09:57:11.869206: train_loss -0.7757 
2025-03-12 09:57:11.869693: val_loss 1.5217 
2025-03-12 09:57:11.869889: Pseudo dice [0.5924, 0.0607, 0.9012, 0.1039, 0.0002] 
2025-03-12 09:57:11.870243: Epoch time: 214.79 s 
2025-03-12 09:57:13.582169:  
2025-03-12 09:57:13.582577: Epoch 262 
2025-03-12 09:57:13.582734: Current learning rate: 0.00761 
2025-03-12 10:00:48.902645: train_loss -0.7766 
2025-03-12 10:00:48.903083: val_loss 1.4347 
2025-03-12 10:00:48.903247: Pseudo dice [0.6092, 0.0764, 0.8961, 0.1471, 0.0] 
2025-03-12 10:00:48.903407: Epoch time: 215.33 s 
2025-03-12 10:00:50.646265:  
2025-03-12 10:00:50.646622: Epoch 263 
2025-03-12 10:00:50.646796: Current learning rate: 0.0076 
2025-03-12 10:04:25.931914: train_loss -0.791 
2025-03-12 10:04:25.932272: val_loss 1.3959 
2025-03-12 10:04:25.932473: Pseudo dice [0.6295, 0.0284, 0.898, 0.2625, 0.0012] 
2025-03-12 10:04:25.932658: Epoch time: 215.29 s 
2025-03-12 10:04:27.727635:  
2025-03-12 10:04:27.727973: Epoch 264 
2025-03-12 10:04:27.728212: Current learning rate: 0.00759 
2025-03-12 10:08:03.042112: train_loss -0.7854 
2025-03-12 10:08:03.042680: val_loss 1.4563 
2025-03-12 10:08:03.042864: Pseudo dice [0.6413, 0.0452, 0.8898, 0.1987, 0.0] 
2025-03-12 10:08:03.042973: Epoch time: 215.32 s 
2025-03-12 10:08:04.751756:  
2025-03-12 10:08:04.752116: Epoch 265 
2025-03-12 10:08:04.752325: Current learning rate: 0.00758 
2025-03-12 10:11:39.898306: train_loss -0.7829 
2025-03-12 10:11:39.898789: val_loss 1.2558 
2025-03-12 10:11:39.899120: Pseudo dice [0.6328, 0.1219, 0.8944, 0.3338, 0.0053] 
2025-03-12 10:11:39.899288: Epoch time: 215.15 s 
2025-03-12 10:11:41.594507:  
2025-03-12 10:11:41.594878: Epoch 266 
2025-03-12 10:11:41.595069: Current learning rate: 0.00757 
2025-03-12 10:15:17.450793: train_loss -0.7717 
2025-03-12 10:15:17.451229: val_loss 1.1563 
2025-03-12 10:15:17.451401: Pseudo dice [0.6648, 0.0761, 0.8979, 0.4003, 0.0017] 
2025-03-12 10:15:17.451545: Epoch time: 215.86 s 
2025-03-12 10:15:19.186955:  
2025-03-12 10:15:19.187415: Epoch 267 
2025-03-12 10:15:19.187711: Current learning rate: 0.00756 
2025-03-12 10:18:54.638809: train_loss -0.7589 
2025-03-12 10:18:54.639342: val_loss 1.5048 
2025-03-12 10:18:54.639596: Pseudo dice [0.6422, 0.033, 0.8721, 0.2224, 0.0019] 
2025-03-12 10:18:54.639810: Epoch time: 215.47 s 
2025-03-12 10:18:56.483615:  
2025-03-12 10:18:56.484056: Epoch 268 
2025-03-12 10:18:56.484322: Current learning rate: 0.00755 
2025-03-12 10:22:31.052636: train_loss -0.7783 
2025-03-12 10:22:31.053010: val_loss 1.4903 
2025-03-12 10:22:31.053123: Pseudo dice [0.6296, 0.0264, 0.8899, 0.1484, 0.0016] 
2025-03-12 10:22:31.053222: Epoch time: 214.58 s 
2025-03-12 10:22:32.717685:  
2025-03-12 10:22:32.717989: Epoch 269 
2025-03-12 10:22:32.718158: Current learning rate: 0.00754 
2025-03-12 10:26:07.109440: train_loss -0.7741 
2025-03-12 10:26:07.109921: val_loss 1.5126 
2025-03-12 10:26:07.110211: Pseudo dice [0.6056, 0.0376, 0.8773, 0.227, 0.0013] 
2025-03-12 10:26:07.110412: Epoch time: 214.4 s 
2025-03-12 10:26:08.789113:  
2025-03-12 10:26:08.789449: Epoch 270 
2025-03-12 10:26:08.789609: Current learning rate: 0.00753 
2025-03-12 10:29:43.603122: train_loss -0.7848 
2025-03-12 10:29:43.603616: val_loss 1.5495 
2025-03-12 10:29:43.603800: Pseudo dice [0.6178, 0.0342, 0.8933, 0.2713, 0.0072] 
2025-03-12 10:29:43.603961: Epoch time: 214.82 s 
2025-03-12 10:29:45.259128:  
2025-03-12 10:29:45.259390: Epoch 271 
2025-03-12 10:29:45.259601: Current learning rate: 0.00752 
2025-03-12 10:33:20.208074: train_loss -0.7748 
2025-03-12 10:33:20.208476: val_loss 1.3552 
2025-03-12 10:33:20.208604: Pseudo dice [0.598, 0.0598, 0.8884, 0.269, 0.0001] 
2025-03-12 10:33:20.208709: Epoch time: 214.95 s 
2025-03-12 10:33:21.896502:  
2025-03-12 10:33:21.896799: Epoch 272 
2025-03-12 10:33:21.896990: Current learning rate: 0.00751 
2025-03-12 10:36:56.752024: train_loss -0.7657 
2025-03-12 10:36:56.752323: val_loss 1.3018 
2025-03-12 10:36:56.752441: Pseudo dice [0.6524, 0.0695, 0.8824, 0.1922, 0.0088] 
2025-03-12 10:36:56.752537: Epoch time: 214.86 s 
2025-03-12 10:36:59.134750:  
2025-03-12 10:36:59.135082: Epoch 273 
2025-03-12 10:36:59.135341: Current learning rate: 0.00751 
2025-03-12 10:40:34.193966: train_loss -0.7632 
2025-03-12 10:40:34.194355: val_loss 1.4586 
2025-03-12 10:40:34.194536: Pseudo dice [0.6624, 0.0479, 0.8968, 0.1946, 0.0067] 
2025-03-12 10:40:34.194672: Epoch time: 215.06 s 
2025-03-12 10:40:35.891990:  
2025-03-12 10:40:35.892302: Epoch 274 
2025-03-12 10:40:35.892485: Current learning rate: 0.0075 
2025-03-12 10:44:10.906128: train_loss -0.7408 
2025-03-12 10:44:10.906652: val_loss 1.2922 
2025-03-12 10:44:10.906882: Pseudo dice [0.6475, 0.0656, 0.893, 0.2341, 0.0045] 
2025-03-12 10:44:10.907050: Epoch time: 215.02 s 
2025-03-12 10:44:12.566861:  
2025-03-12 10:44:12.567149: Epoch 275 
2025-03-12 10:44:12.567378: Current learning rate: 0.00749 
2025-03-12 10:47:47.522145: train_loss -0.7582 
2025-03-12 10:47:47.522498: val_loss 1.4141 
2025-03-12 10:47:47.523049: Pseudo dice [0.6116, 0.0259, 0.8794, 0.3528, 0.0] 
2025-03-12 10:47:47.523231: Epoch time: 214.96 s 
2025-03-12 10:47:49.180980:  
2025-03-12 10:47:49.181274: Epoch 276 
2025-03-12 10:47:49.181486: Current learning rate: 0.00748 
2025-03-12 10:51:24.130298: train_loss -0.7438 
2025-03-12 10:51:24.130623: val_loss 1.4736 
2025-03-12 10:51:24.130800: Pseudo dice [0.5734, 0.0095, 0.8718, 0.3021, 0.0858] 
2025-03-12 10:51:24.130956: Epoch time: 214.96 s 
2025-03-12 10:51:25.785446:  
2025-03-12 10:51:25.785777: Epoch 277 
2025-03-12 10:51:25.785933: Current learning rate: 0.00747 
2025-03-12 10:55:00.851008: train_loss -0.6147 
2025-03-12 10:55:00.851321: val_loss 1.5968 
2025-03-12 10:55:00.851541: Pseudo dice [0.5569, 0.0515, 0.8566, 0.1824, 0.2096] 
2025-03-12 10:55:00.851743: Epoch time: 215.07 s 
2025-03-12 10:55:02.508429:  
2025-03-12 10:55:02.508694: Epoch 278 
2025-03-12 10:55:02.508874: Current learning rate: 0.00746 
2025-03-12 10:58:37.488845: train_loss -0.623 
2025-03-12 10:58:37.489198: val_loss 1.165 
2025-03-12 10:58:37.489328: Pseudo dice [0.5551, 0.1759, 0.8606, 0.1476, 0.005] 
2025-03-12 10:58:37.489428: Epoch time: 214.99 s 
2025-03-12 10:58:39.162679:  
2025-03-12 10:58:39.162961: Epoch 279 
2025-03-12 10:58:39.163143: Current learning rate: 0.00745 
2025-03-12 11:02:13.961428: train_loss -0.656 
2025-03-12 11:02:13.961805: val_loss 1.3172 
2025-03-12 11:02:13.961997: Pseudo dice [0.5912, 0.046, 0.8952, 0.2397, 0.0769] 
2025-03-12 11:02:13.962164: Epoch time: 214.8 s 
2025-03-12 11:02:15.610311:  
2025-03-12 11:02:15.610648: Epoch 280 
2025-03-12 11:02:15.610872: Current learning rate: 0.00744 
2025-03-12 11:05:50.721624: train_loss -0.7291 
2025-03-12 11:05:50.721989: val_loss 1.1863 
2025-03-12 11:05:50.722166: Pseudo dice [0.6184, 0.1057, 0.9031, 0.185, 0.0972] 
2025-03-12 11:05:50.722324: Epoch time: 215.12 s 
2025-03-12 11:05:52.377201:  
2025-03-12 11:05:52.377466: Epoch 281 
2025-03-12 11:05:52.377749: Current learning rate: 0.00743 
2025-03-12 11:09:27.122533: train_loss -0.7055 
2025-03-12 11:09:27.122909: val_loss 1.4696 
2025-03-12 11:09:27.123195: Pseudo dice [0.5616, 0.0485, 0.8772, 0.1839, 0.0196] 
2025-03-12 11:09:27.123400: Epoch time: 214.75 s 
2025-03-12 11:09:28.777171:  
2025-03-12 11:09:28.777525: Epoch 282 
2025-03-12 11:09:28.777714: Current learning rate: 0.00742 
2025-03-12 11:13:03.630438: train_loss -0.6884 
2025-03-12 11:13:03.630822: val_loss 1.2612 
2025-03-12 11:13:03.630965: Pseudo dice [0.5891, 0.0306, 0.8714, 0.2989, 0.2271] 
2025-03-12 11:13:03.631148: Epoch time: 214.86 s 
2025-03-12 11:13:05.320199:  
2025-03-12 11:13:05.320543: Epoch 283 
2025-03-12 11:13:05.320720: Current learning rate: 0.00741 
2025-03-12 11:16:40.199924: train_loss -0.672 
2025-03-12 11:16:40.200260: val_loss 1.3443 
2025-03-12 11:16:40.200400: Pseudo dice [0.5401, 0.0298, 0.8687, 0.2603, 0.0786] 
2025-03-12 11:16:40.200497: Epoch time: 214.88 s 
2025-03-12 11:16:41.891417:  
2025-03-12 11:16:41.891717: Epoch 284 
2025-03-12 11:16:41.891947: Current learning rate: 0.0074 
2025-03-12 11:20:16.725117: train_loss -0.7055 
2025-03-12 11:20:16.725641: val_loss 1.4207 
2025-03-12 11:20:16.725990: Pseudo dice [0.5841, 0.0369, 0.8609, 0.1625, 0.0002] 
2025-03-12 11:20:16.726232: Epoch time: 214.84 s 
2025-03-12 11:20:18.412200:  
2025-03-12 11:20:18.412503: Epoch 285 
2025-03-12 11:20:18.412796: Current learning rate: 0.00739 
2025-03-12 11:23:53.203094: train_loss -0.7498 
2025-03-12 11:23:53.203493: val_loss 1.2797 
2025-03-12 11:23:53.203618: Pseudo dice [0.626, 0.0359, 0.9071, 0.3083, 0.0] 
2025-03-12 11:23:53.203717: Epoch time: 214.8 s 
2025-03-12 11:23:54.860532:  
2025-03-12 11:23:54.860873: Epoch 286 
2025-03-12 11:23:54.861040: Current learning rate: 0.00738 
2025-03-12 11:27:29.609711: train_loss -0.7271 
2025-03-12 11:27:29.610125: val_loss 1.2487 
2025-03-12 11:27:29.610260: Pseudo dice [0.6389, 0.0593, 0.886, 0.2775, 0.1968] 
2025-03-12 11:27:29.610358: Epoch time: 214.76 s 
2025-03-12 11:27:32.002514:  
2025-03-12 11:27:32.002923: Epoch 287 
2025-03-12 11:27:32.003122: Current learning rate: 0.00738 
2025-03-12 11:31:06.967525: train_loss -0.7361 
2025-03-12 11:31:06.968146: val_loss 1.2702 
2025-03-12 11:31:06.968318: Pseudo dice [0.6175, 0.039, 0.8727, 0.1061, 0.0694] 
2025-03-12 11:31:06.968431: Epoch time: 214.97 s 
2025-03-12 11:31:08.696206:  
2025-03-12 11:31:08.696582: Epoch 288 
2025-03-12 11:31:08.696758: Current learning rate: 0.00737 
2025-03-12 11:34:43.765411: train_loss -0.7222 
2025-03-12 11:34:43.765808: val_loss 1.5601 
2025-03-12 11:34:43.766120: Pseudo dice [0.5668, 0.0559, 0.8622, 0.1151, 0.0] 
2025-03-12 11:34:43.766230: Epoch time: 215.08 s 
2025-03-12 11:34:45.462875:  
2025-03-12 11:34:45.463192: Epoch 289 
2025-03-12 11:34:45.463353: Current learning rate: 0.00736 
2025-03-12 11:38:20.679038: train_loss -0.7144 
2025-03-12 11:38:20.679374: val_loss 1.3607 
2025-03-12 11:38:20.679502: Pseudo dice [0.5969, 0.1224, 0.8883, 0.0821, 0.0704] 
2025-03-12 11:38:20.679611: Epoch time: 215.22 s 
2025-03-12 11:38:22.404066:  
2025-03-12 11:38:22.404431: Epoch 290 
2025-03-12 11:38:22.404640: Current learning rate: 0.00735 
2025-03-12 11:41:57.443139: train_loss -0.7414 
2025-03-12 11:41:57.443506: val_loss 1.5428 
2025-03-12 11:41:57.443631: Pseudo dice [0.59, 0.0329, 0.8756, 0.1964, 0.0006] 
2025-03-12 11:41:57.443729: Epoch time: 215.05 s 
2025-03-12 11:41:59.139666:  
2025-03-12 11:41:59.140026: Epoch 291 
2025-03-12 11:41:59.140218: Current learning rate: 0.00734 
2025-03-12 11:45:34.265569: train_loss -0.7133 
2025-03-12 11:45:34.265932: val_loss 1.106 
2025-03-12 11:45:34.266053: Pseudo dice [0.6031, 0.0512, 0.8772, 0.4216, 0.0479] 
2025-03-12 11:45:34.266153: Epoch time: 215.13 s 
2025-03-12 11:45:35.959344:  
2025-03-12 11:45:35.959680: Epoch 292 
2025-03-12 11:45:35.959877: Current learning rate: 0.00733 
2025-03-12 11:49:10.880503: train_loss -0.7203 
2025-03-12 11:49:10.881065: val_loss 1.3733 
2025-03-12 11:49:10.881349: Pseudo dice [0.6306, 0.0464, 0.8738, 0.2537, 0.0008] 
2025-03-12 11:49:10.881476: Epoch time: 214.93 s 
2025-03-12 11:49:12.633183:  
2025-03-12 11:49:12.633493: Epoch 293 
2025-03-12 11:49:12.633672: Current learning rate: 0.00732 
2025-03-12 11:52:47.546741: train_loss -0.767 
2025-03-12 11:52:47.547050: val_loss 1.3854 
2025-03-12 11:52:47.547234: Pseudo dice [0.644, 0.0383, 0.8859, 0.1967, 0.0011] 
2025-03-12 11:52:47.547395: Epoch time: 214.92 s 
2025-03-12 11:52:49.908608:  
2025-03-12 11:52:49.908986: Epoch 294 
2025-03-12 11:52:49.909151: Current learning rate: 0.00731 
2025-03-12 11:56:24.705722: train_loss -0.7572 
2025-03-12 11:56:24.706140: val_loss 1.325 
2025-03-12 11:56:24.706394: Pseudo dice [0.5943, 0.0898, 0.8856, 0.3481, 0.0] 
2025-03-12 11:56:24.706588: Epoch time: 214.8 s 
2025-03-12 11:56:26.392628:  
2025-03-12 11:56:26.392984: Epoch 295 
2025-03-12 11:56:26.393150: Current learning rate: 0.0073 
2025-03-12 12:00:01.439362: train_loss -0.7467 
2025-03-12 12:00:01.439820: val_loss 1.4567 
2025-03-12 12:00:01.440110: Pseudo dice [0.5966, 0.0374, 0.8865, 0.3226, 0.012] 
2025-03-12 12:00:01.440267: Epoch time: 215.05 s 
2025-03-12 12:00:03.149153:  
2025-03-12 12:00:03.149436: Epoch 296 
2025-03-12 12:00:03.149605: Current learning rate: 0.00729 
2025-03-12 12:03:38.050246: train_loss -0.7595 
2025-03-12 12:03:38.050584: val_loss 1.5313 
2025-03-12 12:03:38.050717: Pseudo dice [0.6198, 0.0605, 0.8872, 0.303, 0.0023] 
2025-03-12 12:03:38.050815: Epoch time: 214.91 s 
2025-03-12 12:03:39.760478:  
2025-03-12 12:03:39.760826: Epoch 297 
2025-03-12 12:03:39.761062: Current learning rate: 0.00728 
2025-03-12 12:07:14.827250: train_loss -0.7724 
2025-03-12 12:07:14.827586: val_loss 1.7183 
2025-03-12 12:07:14.827800: Pseudo dice [0.597, 0.0855, 0.8702, 0.2327, 0.0019] 
2025-03-12 12:07:14.827958: Epoch time: 215.07 s 
2025-03-12 12:07:16.552049:  
2025-03-12 12:07:16.552413: Epoch 298 
2025-03-12 12:07:16.552914: Current learning rate: 0.00727 
2025-03-12 12:10:51.733947: train_loss -0.7695 
2025-03-12 12:10:51.734310: val_loss 1.4323 
2025-03-12 12:10:51.734442: Pseudo dice [0.6694, 0.0539, 0.88, 0.1252, 0.0] 
2025-03-12 12:10:51.734538: Epoch time: 215.19 s 
2025-03-12 12:10:53.460810:  
2025-03-12 12:10:53.461143: Epoch 299 
2025-03-12 12:10:53.461338: Current learning rate: 0.00726 
2025-03-12 12:14:28.229919: train_loss -0.7768 
2025-03-12 12:14:28.230288: val_loss 1.3797 
2025-03-12 12:14:28.230402: Pseudo dice [0.618, 0.1047, 0.8889, 0.3519, 0.009] 
2025-03-12 12:14:28.230499: Epoch time: 214.77 s 
2025-03-12 12:14:32.702180:  
2025-03-12 12:14:32.702506: Epoch 300 
2025-03-12 12:14:32.702828: Current learning rate: 0.00725 
2025-03-12 12:18:07.563638: train_loss -0.758 
2025-03-12 12:18:07.564078: val_loss 1.4922 
2025-03-12 12:18:07.564320: Pseudo dice [0.6179, 0.0439, 0.8952, 0.3358, 0.0004] 
2025-03-12 12:18:07.564459: Epoch time: 214.87 s 
2025-03-12 12:18:09.265233:  
2025-03-12 12:18:09.265584: Epoch 301 
2025-03-12 12:18:09.265808: Current learning rate: 0.00724 
2025-03-12 12:21:43.848345: train_loss -0.7616 
2025-03-12 12:21:43.848760: val_loss 1.5109 
2025-03-12 12:21:43.848949: Pseudo dice [0.5704, 0.1005, 0.8856, 0.2815, 0.0006] 
2025-03-12 12:21:43.849112: Epoch time: 214.59 s 
2025-03-12 12:21:45.602533:  
2025-03-12 12:21:45.602803: Epoch 302 
2025-03-12 12:21:45.603003: Current learning rate: 0.00724 
2025-03-12 12:25:20.108845: train_loss -0.7618 
2025-03-12 12:25:20.109169: val_loss 1.4673 
2025-03-12 12:25:20.109293: Pseudo dice [0.6248, 0.0924, 0.8782, 0.2405, 0.0097] 
2025-03-12 12:25:20.109393: Epoch time: 214.51 s 
2025-03-12 12:25:21.841054:  
2025-03-12 12:25:21.841446: Epoch 303 
2025-03-12 12:25:21.841710: Current learning rate: 0.00723 
2025-03-12 12:28:56.501733: train_loss -0.7586 
2025-03-12 12:28:56.502078: val_loss 1.3426 
2025-03-12 12:28:56.502220: Pseudo dice [0.6165, 0.0811, 0.8901, 0.2709, 0.0162] 
2025-03-12 12:28:56.502322: Epoch time: 214.67 s 
2025-03-12 12:28:58.201842:  
2025-03-12 12:28:58.202195: Epoch 304 
2025-03-12 12:28:58.202362: Current learning rate: 0.00722 
2025-03-12 12:32:32.769329: train_loss -0.7532 
2025-03-12 12:32:32.769697: val_loss 1.5471 
2025-03-12 12:32:32.769896: Pseudo dice [0.6135, 0.0325, 0.8799, 0.2456, 0.001] 
2025-03-12 12:32:32.770019: Epoch time: 214.57 s 
2025-03-12 12:32:34.485183:  
2025-03-12 12:32:34.485449: Epoch 305 
2025-03-12 12:32:34.485704: Current learning rate: 0.00721 
2025-03-12 12:36:08.974923: train_loss -0.7726 
2025-03-12 12:36:08.975250: val_loss 1.4429 
2025-03-12 12:36:08.975384: Pseudo dice [0.6299, 0.0144, 0.893, 0.2385, 0.0266] 
2025-03-12 12:36:08.975482: Epoch time: 214.5 s 
2025-03-12 12:36:10.670330:  
2025-03-12 12:36:10.670652: Epoch 306 
2025-03-12 12:36:10.670825: Current learning rate: 0.0072 
2025-03-12 12:39:45.195871: train_loss -0.7846 
2025-03-12 12:39:45.196584: val_loss 1.6637 
2025-03-12 12:39:45.197008: Pseudo dice [0.5264, 0.0184, 0.8801, 0.1029, 0.0002] 
2025-03-12 12:39:45.197276: Epoch time: 214.53 s 
2025-03-12 12:39:46.913183:  
2025-03-12 12:39:46.913631: Epoch 307 
2025-03-12 12:39:46.913920: Current learning rate: 0.00719 
2025-03-12 12:43:21.959223: train_loss -0.7718 
2025-03-12 12:43:21.959764: val_loss 1.5674 
2025-03-12 12:43:21.959955: Pseudo dice [0.5977, 0.0279, 0.8846, 0.1024, 0.0003] 
2025-03-12 12:43:21.960126: Epoch time: 215.05 s 
2025-03-12 12:43:23.676611:  
2025-03-12 12:43:23.676945: Epoch 308 
2025-03-12 12:43:23.677109: Current learning rate: 0.00718 
2025-03-12 12:46:58.239436: train_loss -0.769 
2025-03-12 12:46:58.239741: val_loss 1.321 
2025-03-12 12:46:58.239930: Pseudo dice [0.6257, 0.0502, 0.8978, 0.2663, 0.0172] 
2025-03-12 12:46:58.240098: Epoch time: 214.57 s 
2025-03-12 12:46:59.961850:  
2025-03-12 12:46:59.962182: Epoch 309 
2025-03-12 12:46:59.962404: Current learning rate: 0.00717 
2025-03-12 12:50:34.548422: train_loss -0.7836 
2025-03-12 12:50:34.548786: val_loss 1.5605 
2025-03-12 12:50:34.548971: Pseudo dice [0.5882, 0.0227, 0.8723, 0.1695, 0.0108] 
2025-03-12 12:50:34.549071: Epoch time: 214.59 s 
2025-03-12 12:50:36.256564:  
2025-03-12 12:50:36.256900: Epoch 310 
2025-03-12 12:50:36.257103: Current learning rate: 0.00716 
2025-03-12 12:54:10.866678: train_loss -0.7771 
2025-03-12 12:54:10.867086: val_loss 1.4654 
2025-03-12 12:54:10.867249: Pseudo dice [0.5995, 0.0752, 0.8951, 0.1217, 0.0005] 
2025-03-12 12:54:10.867385: Epoch time: 214.62 s 
2025-03-12 12:54:12.579482:  
2025-03-12 12:54:12.579923: Epoch 311 
2025-03-12 12:54:12.580120: Current learning rate: 0.00715 
2025-03-12 12:57:47.237642: train_loss -0.748 
2025-03-12 12:57:47.237966: val_loss 1.6775 
2025-03-12 12:57:47.238151: Pseudo dice [0.6121, 0.0692, 0.8794, 0.3446, 0.0007] 
2025-03-12 12:57:47.238312: Epoch time: 214.66 s 
2025-03-12 12:57:48.966542:  
2025-03-12 12:57:48.966894: Epoch 312 
2025-03-12 12:57:48.967062: Current learning rate: 0.00714 
2025-03-12 13:01:23.662418: train_loss -0.775 
2025-03-12 13:01:23.662732: val_loss 1.5348 
2025-03-12 13:01:23.662851: Pseudo dice [0.594, 0.0364, 0.8845, 0.2968, 0.001] 
2025-03-12 13:01:23.662950: Epoch time: 214.7 s 
2025-03-12 13:01:25.393761:  
2025-03-12 13:01:25.394057: Epoch 313 
2025-03-12 13:01:25.394284: Current learning rate: 0.00713 
2025-03-12 13:04:59.998194: train_loss -0.7717 
2025-03-12 13:04:59.998775: val_loss 1.6298 
2025-03-12 13:04:59.999055: Pseudo dice [0.6178, 0.0366, 0.8826, 0.2555, 0.0] 
2025-03-12 13:04:59.999241: Epoch time: 214.61 s 
2025-03-12 13:05:02.378277:  
2025-03-12 13:05:02.378716: Epoch 314 
2025-03-12 13:05:02.379192: Current learning rate: 0.00712 
2025-03-12 13:08:36.836278: train_loss -0.771 
2025-03-12 13:08:36.836714: val_loss 1.5236 
2025-03-12 13:08:36.836962: Pseudo dice [0.6315, 0.0324, 0.8942, 0.2274, 0.0] 
2025-03-12 13:08:36.837127: Epoch time: 214.46 s 
2025-03-12 13:08:38.562422:  
2025-03-12 13:08:38.562788: Epoch 315 
2025-03-12 13:08:38.562957: Current learning rate: 0.00711 
2025-03-12 13:12:13.031039: train_loss -0.7692 
2025-03-12 13:12:13.031599: val_loss 1.6518 
2025-03-12 13:12:13.032098: Pseudo dice [0.6036, 0.0345, 0.8712, 0.1172, 0.0] 
2025-03-12 13:12:13.032453: Epoch time: 214.47 s 
2025-03-12 13:12:14.752219:  
2025-03-12 13:12:14.752599: Epoch 316 
2025-03-12 13:12:14.752824: Current learning rate: 0.0071 
2025-03-12 13:15:49.375032: train_loss -0.7386 
2025-03-12 13:15:49.375425: val_loss 1.4044 
2025-03-12 13:15:49.375566: Pseudo dice [0.6375, 0.0938, 0.8828, 0.2267, 0.0003] 
2025-03-12 13:15:49.375671: Epoch time: 214.63 s 
2025-03-12 13:15:51.095396:  
2025-03-12 13:15:51.095750: Epoch 317 
2025-03-12 13:15:51.095941: Current learning rate: 0.0071 
2025-03-12 13:19:25.643091: train_loss -0.7604 
2025-03-12 13:19:25.643450: val_loss 1.3771 
2025-03-12 13:19:25.643623: Pseudo dice [0.5946, 0.0183, 0.8871, 0.1163, 0.0019] 
2025-03-12 13:19:25.643783: Epoch time: 214.55 s 
2025-03-12 13:19:27.356035:  
2025-03-12 13:19:27.356357: Epoch 318 
2025-03-12 13:19:27.356597: Current learning rate: 0.00709 
2025-03-12 13:23:01.901453: train_loss -0.7371 
2025-03-12 13:23:01.901872: val_loss 1.4042 
2025-03-12 13:23:01.902059: Pseudo dice [0.5916, 0.0529, 0.869, 0.1976, 0.0002] 
2025-03-12 13:23:01.902166: Epoch time: 214.55 s 
2025-03-12 13:23:03.634083:  
2025-03-12 13:23:03.634417: Epoch 319 
2025-03-12 13:23:03.634640: Current learning rate: 0.00708 
2025-03-12 13:26:38.160368: train_loss -0.7206 
2025-03-12 13:26:38.160779: val_loss 1.305 
2025-03-12 13:26:38.161058: Pseudo dice [0.5579, 0.0375, 0.8476, 0.3052, 0.0] 
2025-03-12 13:26:38.161269: Epoch time: 214.53 s 
2025-03-12 13:26:39.854582:  
2025-03-12 13:26:39.854848: Epoch 320 
2025-03-12 13:26:39.855066: Current learning rate: 0.00707 
2025-03-12 13:30:14.498878: train_loss -0.7048 
2025-03-12 13:30:14.499204: val_loss 1.3007 
2025-03-12 13:30:14.499335: Pseudo dice [0.6501, 0.0557, 0.8809, 0.1574, 0.0019] 
2025-03-12 13:30:14.499432: Epoch time: 214.65 s 
2025-03-12 13:30:16.898839:  
2025-03-12 13:30:16.899146: Epoch 321 
2025-03-12 13:30:16.899389: Current learning rate: 0.00706 
2025-03-12 13:33:51.664672: train_loss -0.7199 
2025-03-12 13:33:51.665204: val_loss 1.5238 
2025-03-12 13:33:51.665503: Pseudo dice [0.5958, 0.1051, 0.8856, 0.2522, 0.0] 
2025-03-12 13:33:51.665683: Epoch time: 214.77 s 
2025-03-12 13:33:53.391684:  
2025-03-12 13:33:53.392014: Epoch 322 
2025-03-12 13:33:53.392159: Current learning rate: 0.00705 
2025-03-12 13:37:28.060694: train_loss -0.7365 
2025-03-12 13:37:28.061122: val_loss 1.6591 
2025-03-12 13:37:28.061231: Pseudo dice [0.586, 0.0563, 0.8489, 0.3573, 0.0004] 
2025-03-12 13:37:28.061328: Epoch time: 214.67 s 
2025-03-12 13:37:29.780509:  
2025-03-12 13:37:29.780911: Epoch 323 
2025-03-12 13:37:29.781116: Current learning rate: 0.00704 
2025-03-12 13:41:04.458636: train_loss -0.7575 
2025-03-12 13:41:04.459178: val_loss 1.1955 
2025-03-12 13:41:04.459368: Pseudo dice [0.6493, 0.0425, 0.8901, 0.2765, 0.0021] 
2025-03-12 13:41:04.459530: Epoch time: 214.68 s 
2025-03-12 13:41:06.233780:  
2025-03-12 13:41:06.234113: Epoch 324 
2025-03-12 13:41:06.234353: Current learning rate: 0.00703 
2025-03-12 13:44:40.790844: train_loss -0.7589 
2025-03-12 13:44:40.791176: val_loss 1.505 
2025-03-12 13:44:40.791376: Pseudo dice [0.6266, 0.0482, 0.8934, 0.3284, 0.0025] 
2025-03-12 13:44:40.791529: Epoch time: 214.56 s 
2025-03-12 13:44:42.519032:  
2025-03-12 13:44:42.519416: Epoch 325 
2025-03-12 13:44:42.519667: Current learning rate: 0.00702 
2025-03-12 13:48:17.006820: train_loss -0.7754 
2025-03-12 13:48:17.007137: val_loss 1.5876 
2025-03-12 13:48:17.007269: Pseudo dice [0.6018, 0.0318, 0.8767, 0.2006, 0.076] 
2025-03-12 13:48:17.007367: Epoch time: 214.49 s 
2025-03-12 13:48:18.714532:  
2025-03-12 13:48:18.714851: Epoch 326 
2025-03-12 13:48:18.715016: Current learning rate: 0.00701 


#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-03-12 15:00:44.804018: do_dummy_2d_data_aug: False 
2025-03-12 15:00:44.806065: Using splits from existing split file: /data/hotaru/projects/nnUNet/nnunetv2/dataset/nnUNet_preprocessed/Dataset001_puma/splits_final.json 
2025-03-12 15:00:44.806879: The split file contains 5 splits. 
2025-03-12 15:00:44.807002: Desired fold for training: 5 
2025-03-12 15:00:44.807072: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2025-03-12 15:00:44.840452: This random 80:20 split has 164 training and 41 validation cases. 
2025-03-12 15:01:00.864257: Using torch.compile... 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 13, 'patch_size': [896, 768], 'median_image_size_in_voxels': [1024.0, 1024.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization', 'ZScoreNormalization', 'ZScoreNormalization'], 'use_mask_for_norm': [False, False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_puma', 'plans_name': 'nnUNetResEncUNetLPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 1024, 1024], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 158.0860595703125, 'median': 166.0, 'min': 0.0, 'percentile_00_5': 30.0, 'percentile_99_5': 246.0, 'std': 50.084224700927734}, '1': {'max': 255.0, 'mean': 106.11481475830078, 'median': 101.0, 'min': 0.0, 'percentile_00_5': 15.0, 'percentile_99_5': 225.0, 'std': 49.32137680053711}, '2': {'max': 255.0, 'mean': 178.19647216796875, 'median': 182.0, 'min': 0.0, 'percentile_00_5': 46.0, 'percentile_99_5': 244.0, 'std': 34.66091537475586}}} 
 
2025-03-12 15:01:05.931923: unpacking dataset... 
2025-03-12 15:01:11.602020: unpacking done... 
2025-03-12 15:01:11.692411: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-03-12 15:01:11.948342:  
2025-03-12 15:01:11.948990: Epoch 0 
2025-03-12 15:01:11.950204: Current learning rate: 0.01 
2025-03-12 15:07:42.086000: train_loss 0.8106 
2025-03-12 15:07:42.086362: val_loss 0.8767 
2025-03-12 15:07:42.086489: Pseudo dice [0.3368, 0.0, 0.7802, 0.0, 0.0] 
2025-03-12 15:07:42.086597: Epoch time: 390.16 s 
2025-03-12 15:07:42.086665: Yayy! New best EMA pseudo Dice: 0.2234 
2025-03-12 15:07:45.221182:  
2025-03-12 15:07:45.221531: Epoch 1 
2025-03-12 15:07:45.221752: Current learning rate: 0.00995 
2025-03-12 15:11:19.076330: train_loss 0.5179 
2025-03-12 15:11:19.076777: val_loss 0.7593 
2025-03-12 15:11:19.076962: Pseudo dice [0.5202, 0.0, 0.8337, 0.0, 0.0] 
2025-03-12 15:11:19.077112: Epoch time: 213.86 s 
2025-03-12 15:11:19.077184: Yayy! New best EMA pseudo Dice: 0.2281 
2025-03-12 15:11:22.254854:  
2025-03-12 15:11:22.255193: Epoch 2 
2025-03-12 15:11:22.255589: Current learning rate: 0.00991 
2025-03-12 15:14:56.343712: train_loss 0.3697 
2025-03-12 15:14:56.344127: val_loss 0.6639 
2025-03-12 15:14:56.344307: Pseudo dice [0.5215, 0.0, 0.8544, 0.0144, 0.0] 
2025-03-12 15:14:56.344481: Epoch time: 214.09 s 
2025-03-12 15:14:56.344646: Yayy! New best EMA pseudo Dice: 0.2331 
2025-03-12 15:15:00.715028:  
2025-03-12 15:15:00.715389: Epoch 3 
2025-03-12 15:15:00.715535: Current learning rate: 0.00986 
2025-03-12 15:18:34.905976: train_loss 0.2265 
2025-03-12 15:18:34.906333: val_loss 0.6875 
2025-03-12 15:18:34.906528: Pseudo dice [0.5726, 0.0, 0.85, 0.2595, 0.0] 
2025-03-12 15:18:34.906769: Epoch time: 214.2 s 
2025-03-12 15:18:34.906921: Yayy! New best EMA pseudo Dice: 0.2435 
2025-03-12 15:18:39.674273:  
2025-03-12 15:18:39.674639: Epoch 4 
2025-03-12 15:18:39.674820: Current learning rate: 0.00982 
2025-03-12 15:22:13.907565: train_loss 0.0952 
2025-03-12 15:22:13.908002: val_loss 0.6228 
2025-03-12 15:22:13.908193: Pseudo dice [0.5589, 0.0, 0.852, 0.2926, 0.0] 
2025-03-12 15:22:13.908352: Epoch time: 214.24 s 
2025-03-12 15:22:13.908422: Yayy! New best EMA pseudo Dice: 0.2532 
2025-03-12 15:22:17.225929:  
2025-03-12 15:22:17.226215: Epoch 5 
2025-03-12 15:22:17.226464: Current learning rate: 0.00977 
2025-03-12 15:25:52.319485: train_loss -0.0056 
2025-03-12 15:25:52.319900: val_loss 0.5899 
2025-03-12 15:25:52.320028: Pseudo dice [0.5991, 0.0, 0.8755, 0.1362, 0.0] 
2025-03-12 15:25:52.320127: Epoch time: 215.1 s 
2025-03-12 15:25:52.320291: Yayy! New best EMA pseudo Dice: 0.2601 
2025-03-12 15:25:56.744459:  
2025-03-12 15:25:56.744799: Epoch 6 
2025-03-12 15:25:56.745058: Current learning rate: 0.00973 
2025-03-12 15:29:31.048732: train_loss -0.0586 
2025-03-12 15:29:31.049102: val_loss 0.5576 
2025-03-12 15:29:31.049240: Pseudo dice [0.6321, 0.0, 0.8759, 0.2621, 0.0] 
2025-03-12 15:29:31.049413: Epoch time: 214.31 s 
2025-03-12 15:29:31.049567: Yayy! New best EMA pseudo Dice: 0.2695 
2025-03-12 15:29:35.466525:  
2025-03-12 15:29:35.466869: Epoch 7 
2025-03-12 15:29:35.467381: Current learning rate: 0.00968 
2025-03-12 15:33:10.209941: train_loss -0.0825 
2025-03-12 15:33:10.210316: val_loss 0.6518 
2025-03-12 15:33:10.210441: Pseudo dice [0.6047, 0.0, 0.8783, 0.1046, 0.0] 
2025-03-12 15:33:10.210536: Epoch time: 214.75 s 
2025-03-12 15:33:10.210612: Yayy! New best EMA pseudo Dice: 0.2743 
2025-03-12 15:33:17.190106:  
2025-03-12 15:33:17.190428: Epoch 8 
2025-03-12 15:33:17.190711: Current learning rate: 0.00964 
2025-03-12 15:36:51.857240: train_loss -0.1648 
2025-03-12 15:36:51.857679: val_loss 0.6591 
2025-03-12 15:36:51.857829: Pseudo dice [0.5579, 0.038, 0.8718, 0.227, 0.0149] 
2025-03-12 15:36:51.858001: Epoch time: 214.67 s 
2025-03-12 15:36:51.858116: Yayy! New best EMA pseudo Dice: 0.281 
2025-03-12 15:36:55.291528:  
2025-03-12 15:36:55.291866: Epoch 9 
2025-03-12 15:36:55.292060: Current learning rate: 0.00959 
2025-03-12 15:40:29.624006: train_loss -0.2285 
2025-03-12 15:40:29.624519: val_loss 0.8033 
2025-03-12 15:40:29.624702: Pseudo dice [0.5631, 0.2257, 0.8599, 0.2422, 0.0058] 
2025-03-12 15:40:29.624938: Epoch time: 214.34 s 
2025-03-12 15:40:29.625025: Yayy! New best EMA pseudo Dice: 0.2909 
2025-03-12 15:40:34.288799:  
2025-03-12 15:40:34.289213: Epoch 10 
2025-03-12 15:40:34.289398: Current learning rate: 0.00955 
2025-03-12 15:44:08.742583: train_loss -0.249 
2025-03-12 15:44:08.742944: val_loss 0.708 
2025-03-12 15:44:08.743169: Pseudo dice [0.559, 0.114, 0.8647, 0.0346, 0.0531] 
2025-03-12 15:44:08.743335: Epoch time: 214.46 s 
2025-03-12 15:44:08.743455: Yayy! New best EMA pseudo Dice: 0.2943 
2025-03-12 15:44:13.828893:  
2025-03-12 15:44:13.829267: Epoch 11 
2025-03-12 15:44:13.829430: Current learning rate: 0.0095 
2025-03-12 15:47:48.196965: train_loss -0.3163 
2025-03-12 15:47:48.197417: val_loss 0.9 
2025-03-12 15:47:48.197667: Pseudo dice [0.5783, 0.0726, 0.8535, 0.3441, 0.0492] 
2025-03-12 15:47:48.197768: Epoch time: 214.37 s 
2025-03-12 15:47:48.197842: Yayy! New best EMA pseudo Dice: 0.3028 
2025-03-12 15:47:51.556653:  
2025-03-12 15:47:51.556930: Epoch 12 
2025-03-12 15:47:51.557103: Current learning rate: 0.00946 
2025-03-12 15:51:25.910707: train_loss -0.3356 
2025-03-12 15:51:25.911021: val_loss 0.7822 
2025-03-12 15:51:25.911170: Pseudo dice [0.6217, 0.2186, 0.8892, 0.0394, 0.0055] 
2025-03-12 15:51:25.911268: Epoch time: 214.36 s 
2025-03-12 15:51:25.911338: Yayy! New best EMA pseudo Dice: 0.308 
2025-03-12 15:51:29.140845:  
2025-03-12 15:51:29.141179: Epoch 13 
2025-03-12 15:51:29.141356: Current learning rate: 0.00941 
2025-03-12 15:55:03.274078: train_loss -0.402 
2025-03-12 15:55:03.274415: val_loss 0.9941 
2025-03-12 15:55:03.274667: Pseudo dice [0.5593, 0.127, 0.8789, 0.2668, 0.0715] 
2025-03-12 15:55:03.274931: Epoch time: 214.14 s 
2025-03-12 15:55:03.275105: Yayy! New best EMA pseudo Dice: 0.3153 
2025-03-12 15:55:06.694179:  
2025-03-12 15:55:06.694674: Epoch 14 
2025-03-12 15:55:06.695163: Current learning rate: 0.00937 
2025-03-12 15:58:41.058291: train_loss -0.4622 
2025-03-12 15:58:41.058783: val_loss 0.8331 
2025-03-12 15:58:41.058985: Pseudo dice [0.6314, 0.1196, 0.8948, 0.2046, 0.0668] 
2025-03-12 15:58:41.059083: Epoch time: 214.37 s 
2025-03-12 15:58:41.059150: Yayy! New best EMA pseudo Dice: 0.3221 
2025-03-12 15:58:44.448747:  
2025-03-12 15:58:44.449085: Epoch 15 
2025-03-12 15:58:44.449255: Current learning rate: 0.00932 
2025-03-12 16:02:18.858407: train_loss -0.4741 
2025-03-12 16:02:18.858829: val_loss 1.1803 
2025-03-12 16:02:18.858961: Pseudo dice [0.5093, 0.1018, 0.8583, 0.0466, 0.0735] 
2025-03-12 16:02:18.859055: Epoch time: 214.41 s 
2025-03-12 16:02:20.603683:  
2025-03-12 16:02:20.603987: Epoch 16 
2025-03-12 16:02:20.604136: Current learning rate: 0.00928 
2025-03-12 16:05:55.325605: train_loss -0.4949 
2025-03-12 16:05:55.326064: val_loss 1.1142 
2025-03-12 16:05:55.326384: Pseudo dice [0.6291, 0.075, 0.8643, 0.2408, 0.0453] 
2025-03-12 16:05:55.326530: Epoch time: 214.73 s 
2025-03-12 16:05:55.326640: Yayy! New best EMA pseudo Dice: 0.3266 
2025-03-12 16:05:59.704326:  
2025-03-12 16:05:59.704727: Epoch 17 
2025-03-12 16:05:59.704986: Current learning rate: 0.00923 
2025-03-12 16:09:34.233509: train_loss -0.4839 
2025-03-12 16:09:34.233856: val_loss 0.9029 
2025-03-12 16:09:34.234006: Pseudo dice [0.629, 0.204, 0.8836, 0.2584, 0.0535] 
2025-03-12 16:09:34.234123: Epoch time: 214.53 s 
2025-03-12 16:09:34.234203: Yayy! New best EMA pseudo Dice: 0.3345 
2025-03-12 16:09:37.573539:  
2025-03-12 16:09:37.573874: Epoch 18 
2025-03-12 16:09:37.574135: Current learning rate: 0.00919 
2025-03-12 16:13:12.182933: train_loss -0.5142 
2025-03-12 16:13:12.183458: val_loss 0.9658 
2025-03-12 16:13:12.183747: Pseudo dice [0.6187, 0.0758, 0.8789, 0.2207, 0.0002] 
2025-03-12 16:13:12.183910: Epoch time: 214.61 s 
2025-03-12 16:13:12.184046: Yayy! New best EMA pseudo Dice: 0.3369 
2025-03-12 16:13:15.487754:  
2025-03-12 16:13:15.488018: Epoch 19 
2025-03-12 16:13:15.488285: Current learning rate: 0.00914 
2025-03-12 16:16:50.252113: train_loss -0.5152 
2025-03-12 16:16:50.252527: val_loss 1.104 
2025-03-12 16:16:50.252709: Pseudo dice [0.5752, 0.1394, 0.8869, 0.2629, 0.0206] 
2025-03-12 16:16:50.252875: Epoch time: 214.77 s 
2025-03-12 16:16:50.252952: Yayy! New best EMA pseudo Dice: 0.341 
2025-03-12 16:16:54.938942:  
2025-03-12 16:16:54.939582: Epoch 20 
2025-03-12 16:16:54.939748: Current learning rate: 0.0091 
2025-03-12 16:20:29.893581: train_loss -0.5423 
2025-03-12 16:20:29.894262: val_loss 1.2012 
2025-03-12 16:20:29.894600: Pseudo dice [0.5487, 0.0692, 0.8851, 0.1384, 0.003] 
2025-03-12 16:20:29.894972: Epoch time: 214.96 s 
2025-03-12 16:20:31.810963:  
2025-03-12 16:20:31.811439: Epoch 21 
2025-03-12 16:20:31.812449: Current learning rate: 0.00905 
2025-03-12 16:24:06.509173: train_loss -0.5797 
2025-03-12 16:24:06.509520: val_loss 1.2867 
2025-03-12 16:24:06.509680: Pseudo dice [0.5791, 0.1024, 0.8718, 0.0814, 0.0011] 
2025-03-12 16:24:06.509784: Epoch time: 214.71 s 
2025-03-12 16:24:08.152460:  
2025-03-12 16:24:08.152855: Epoch 22 
2025-03-12 16:24:08.152994: Current learning rate: 0.009 
2025-03-12 16:27:43.143197: train_loss -0.618 
2025-03-12 16:27:43.143776: val_loss 1.2023 
2025-03-12 16:27:43.144046: Pseudo dice [0.558, 0.0672, 0.878, 0.113, 0.0079] 
2025-03-12 16:27:43.144212: Epoch time: 215.0 s 
2025-03-12 16:27:44.792781:  
2025-03-12 16:27:44.793143: Epoch 23 
2025-03-12 16:27:44.793332: Current learning rate: 0.00896 
2025-03-12 16:31:19.574694: train_loss -0.5829 
2025-03-12 16:31:19.575081: val_loss 1.0227 
2025-03-12 16:31:19.575226: Pseudo dice [0.5846, 0.0982, 0.8566, 0.3222, 0.2267] 
2025-03-12 16:31:19.575375: Epoch time: 214.79 s 
2025-03-12 16:31:19.575531: Yayy! New best EMA pseudo Dice: 0.3452 
2025-03-12 16:31:22.842278:  
2025-03-12 16:31:22.842628: Epoch 24 
2025-03-12 16:31:22.842783: Current learning rate: 0.00891 
2025-03-12 16:34:58.067331: train_loss -0.56 
2025-03-12 16:34:58.067696: val_loss 1.2538 
2025-03-12 16:34:58.067841: Pseudo dice [0.5868, 0.1431, 0.8698, 0.0556, 0.0184] 
2025-03-12 16:34:58.068082: Epoch time: 215.23 s 
2025-03-12 16:34:59.802080:  
2025-03-12 16:34:59.802397: Epoch 25 
2025-03-12 16:34:59.802602: Current learning rate: 0.00887 
2025-03-12 16:38:34.782178: train_loss -0.5622 
2025-03-12 16:38:34.783850: val_loss 1.1639 
2025-03-12 16:38:34.785047: Pseudo dice [0.6266, 0.0814, 0.8872, 0.1648, 0.0185] 
2025-03-12 16:38:34.785829: Epoch time: 214.99 s 
2025-03-12 16:38:34.786056: Yayy! New best EMA pseudo Dice: 0.3453 
2025-03-12 16:38:39.716195:  
2025-03-12 16:38:39.716604: Epoch 26 
2025-03-12 16:38:39.716947: Current learning rate: 0.00882 
2025-03-12 16:42:14.649287: train_loss -0.5416 
2025-03-12 16:42:14.649662: val_loss 1.1051 
2025-03-12 16:42:14.649837: Pseudo dice [0.613, 0.1196, 0.8918, 0.0544, 0.074] 
2025-03-12 16:42:14.649997: Epoch time: 214.94 s 
2025-03-12 16:42:14.650123: Yayy! New best EMA pseudo Dice: 0.3458 
2025-03-12 16:42:17.874332:  
2025-03-12 16:42:17.874623: Epoch 27 
2025-03-12 16:42:17.874858: Current learning rate: 0.00878 
2025-03-12 16:45:52.068817: train_loss -0.5887 
2025-03-12 16:45:52.069355: val_loss 1.1905 
2025-03-12 16:45:52.069513: Pseudo dice [0.5583, 0.1048, 0.8743, 0.0858, 0.041] 
2025-03-12 16:45:52.069622: Epoch time: 214.2 s 
2025-03-12 16:45:53.783764:  
2025-03-12 16:45:53.784117: Epoch 28 
2025-03-12 16:45:53.784282: Current learning rate: 0.00873 
2025-03-12 16:49:27.183578: train_loss -0.6189 
2025-03-12 16:49:27.184029: val_loss 1.2538 
2025-03-12 16:49:27.184170: Pseudo dice [0.641, 0.0599, 0.8857, 0.0908, 0.0453] 
2025-03-12 16:49:27.184268: Epoch time: 213.41 s 
2025-03-12 16:49:28.836868:  
2025-03-12 16:49:28.837258: Epoch 29 
2025-03-12 16:49:28.837445: Current learning rate: 0.00868 
2025-03-12 16:53:01.454100: train_loss -0.6149 
2025-03-12 16:53:01.454434: val_loss 1.1042 
2025-03-12 16:53:01.454587: Pseudo dice [0.6371, 0.1413, 0.8724, 0.1524, 0.0189] 
2025-03-12 16:53:01.454686: Epoch time: 212.62 s 
2025-03-12 16:53:01.454762: Yayy! New best EMA pseudo Dice: 0.3465 
2025-03-12 16:53:06.557705:  
2025-03-12 16:53:06.558033: Epoch 30 
2025-03-12 16:53:06.558203: Current learning rate: 0.00864 
2025-03-12 16:56:39.182894: train_loss -0.6436 
2025-03-12 16:56:39.183323: val_loss 1.2126 
2025-03-12 16:56:39.183513: Pseudo dice [0.5971, 0.1455, 0.9078, 0.1326, 0.0026] 
2025-03-12 16:56:39.183674: Epoch time: 212.63 s 
2025-03-12 16:56:39.183777: Yayy! New best EMA pseudo Dice: 0.3476 
2025-03-12 16:56:43.734223:  
2025-03-12 16:56:43.734583: Epoch 31 
2025-03-12 16:56:43.734817: Current learning rate: 0.00859 
2025-03-12 17:00:16.421858: train_loss -0.6159 
2025-03-12 17:00:16.422238: val_loss 1.2741 
2025-03-12 17:00:16.422435: Pseudo dice [0.619, 0.1654, 0.858, 0.2712, 0.2911] 
2025-03-12 17:00:16.422682: Epoch time: 212.69 s 
2025-03-12 17:00:16.422878: Yayy! New best EMA pseudo Dice: 0.3569 
2025-03-12 17:00:21.127028:  
2025-03-12 17:00:21.127377: Epoch 32 
2025-03-12 17:00:21.127528: Current learning rate: 0.00855 
2025-03-12 17:03:53.620221: train_loss -0.6058 
2025-03-12 17:03:53.620723: val_loss 1.354 
2025-03-12 17:03:53.620862: Pseudo dice [0.5854, 0.0593, 0.8364, 0.277, 0.0317] 
2025-03-12 17:03:53.620962: Epoch time: 212.5 s 
2025-03-12 17:03:53.621035: Yayy! New best EMA pseudo Dice: 0.357 
2025-03-12 17:03:57.926110:  
2025-03-12 17:03:57.926455: Epoch 33 
2025-03-12 17:03:57.926633: Current learning rate: 0.0085 
2025-03-12 17:07:30.608659: train_loss -0.6418 
2025-03-12 17:07:30.609483: val_loss 1.0146 
2025-03-12 17:07:30.609834: Pseudo dice [0.6385, 0.05, 0.906, 0.089, 0.0173] 
2025-03-12 17:07:30.610101: Epoch time: 212.69 s 
2025-03-12 17:07:33.368710:  
2025-03-12 17:07:33.369279: Epoch 34 
2025-03-12 17:07:33.369572: Current learning rate: 0.00846 
2025-03-12 17:11:06.432749: train_loss -0.6635 
2025-03-12 17:11:06.433119: val_loss 1.3383 
2025-03-12 17:11:06.433403: Pseudo dice [0.6026, 0.1845, 0.8663, 0.1044, 0.092] 
2025-03-12 17:11:06.433590: Epoch time: 213.08 s 
2025-03-12 17:11:08.234288:  
2025-03-12 17:11:08.234643: Epoch 35 
2025-03-12 17:11:08.234866: Current learning rate: 0.00841 
2025-03-12 17:14:41.165215: train_loss -0.6495 
2025-03-12 17:14:41.165543: val_loss 1.1911 
2025-03-12 17:14:41.165696: Pseudo dice [0.6064, 0.0426, 0.8794, 0.2776, 0.0004] 
2025-03-12 17:14:41.165791: Epoch time: 212.94 s 
2025-03-12 17:14:41.165860: Yayy! New best EMA pseudo Dice: 0.3572 
2025-03-12 17:14:45.882951:  
2025-03-12 17:14:45.883311: Epoch 36 
2025-03-12 17:14:45.883543: Current learning rate: 0.00836 
2025-03-12 17:18:18.243118: train_loss -0.6394 
2025-03-12 17:18:18.244052: val_loss 1.2875 
2025-03-12 17:18:18.244270: Pseudo dice [0.5773, 0.0448, 0.8746, 0.0936, 0.0245] 
2025-03-12 17:18:18.244412: Epoch time: 212.36 s 
2025-03-12 17:18:20.105519:  
2025-03-12 17:18:20.105889: Epoch 37 
2025-03-12 17:18:20.106114: Current learning rate: 0.00832 
2025-03-12 17:21:52.823907: train_loss -0.6703 
2025-03-12 17:21:52.824460: val_loss 1.3254 
2025-03-12 17:21:52.824751: Pseudo dice [0.5818, 0.0457, 0.8911, 0.2212, 0.0028] 
2025-03-12 17:21:52.824853: Epoch time: 212.72 s 
2025-03-12 17:21:54.578333:  
2025-03-12 17:21:54.578641: Epoch 38 
2025-03-12 17:21:54.578850: Current learning rate: 0.00827 
2025-03-12 17:25:27.660905: train_loss -0.6762 
2025-03-12 17:25:27.661422: val_loss 1.3147 
2025-03-12 17:25:27.661600: Pseudo dice [0.6402, 0.0816, 0.8866, 0.1566, 0.0146] 
2025-03-12 17:25:27.661727: Epoch time: 213.09 s 
2025-03-12 17:25:29.423274:  
2025-03-12 17:25:29.423588: Epoch 39 
2025-03-12 17:25:29.423931: Current learning rate: 0.00823 
2025-03-12 17:29:01.934613: train_loss -0.6903 
2025-03-12 17:29:01.935024: val_loss 1.6553 
2025-03-12 17:29:01.935215: Pseudo dice [0.596, 0.0374, 0.8866, 0.1621, 0.0025] 
2025-03-12 17:29:01.935395: Epoch time: 212.52 s 
2025-03-12 17:29:03.661379:  
2025-03-12 17:29:03.661734: Epoch 40 
2025-03-12 17:29:03.661924: Current learning rate: 0.00818 
2025-03-12 17:32:36.199651: train_loss -0.6818 
2025-03-12 17:32:36.200077: val_loss 0.9691 
2025-03-12 17:32:36.200540: Pseudo dice [0.6751, 0.0431, 0.8997, 0.2943, 0.0277] 
2025-03-12 17:32:36.200865: Epoch time: 212.54 s 
2025-03-12 17:32:38.623318:  
2025-03-12 17:32:38.623577: Epoch 41 
2025-03-12 17:32:38.623811: Current learning rate: 0.00813 
2025-03-12 17:36:11.305784: train_loss -0.6896 
2025-03-12 17:36:11.306196: val_loss 1.3607 
2025-03-12 17:36:11.306398: Pseudo dice [0.5934, 0.0679, 0.878, 0.0777, 0.0378] 
2025-03-12 17:36:11.306543: Epoch time: 212.69 s 
2025-03-12 17:36:12.975923:  
2025-03-12 17:36:12.976725: Epoch 42 
2025-03-12 17:36:12.976972: Current learning rate: 0.00809 
2025-03-12 17:39:45.745829: train_loss -0.6682 
2025-03-12 17:39:45.746281: val_loss 1.4335 
2025-03-12 17:39:45.746400: Pseudo dice [0.5893, 0.0712, 0.8966, 0.19, 0.0028] 
2025-03-12 17:39:45.746497: Epoch time: 212.78 s 
2025-03-12 17:39:47.384041:  
2025-03-12 17:39:47.384421: Epoch 43 
2025-03-12 17:39:47.384938: Current learning rate: 0.00804 
2025-03-12 17:43:20.282564: train_loss -0.6787 
2025-03-12 17:43:20.283065: val_loss 1.3076 
2025-03-12 17:43:20.283394: Pseudo dice [0.5836, 0.0297, 0.8552, 0.1062, 0.011] 
2025-03-12 17:43:20.283657: Epoch time: 212.9 s 
2025-03-12 17:43:21.953602:  
2025-03-12 17:43:21.954207: Epoch 44 
2025-03-12 17:43:21.954505: Current learning rate: 0.008 
2025-03-12 17:46:54.840042: train_loss -0.6898 
2025-03-12 17:46:54.840379: val_loss 1.1609 
2025-03-12 17:46:54.840598: Pseudo dice [0.6293, 0.0487, 0.8889, 0.144, 0.0088] 
2025-03-12 17:46:54.840784: Epoch time: 212.89 s 
2025-03-12 17:46:56.463083:  
2025-03-12 17:46:56.463430: Epoch 45 
2025-03-12 17:46:56.463648: Current learning rate: 0.00795 
2025-03-12 17:50:28.939996: train_loss -0.6677 
2025-03-12 17:50:28.940331: val_loss 1.4055 
2025-03-12 17:50:28.940553: Pseudo dice [0.5736, 0.1414, 0.8838, 0.1256, 0.0157] 
2025-03-12 17:50:28.940713: Epoch time: 212.48 s 
2025-03-12 17:50:30.516025:  
2025-03-12 17:50:30.516370: Epoch 46 
2025-03-12 17:50:30.516538: Current learning rate: 0.0079 
2025-03-12 17:54:03.135950: train_loss -0.7117 
2025-03-12 17:54:03.136442: val_loss 1.2258 
2025-03-12 17:54:03.136731: Pseudo dice [0.6608, 0.1282, 0.9006, 0.2476, 0.0284] 
2025-03-12 17:54:03.136912: Epoch time: 212.63 s 
2025-03-12 17:54:04.731906:  
2025-03-12 17:54:04.732180: Epoch 47 
2025-03-12 17:54:04.732677: Current learning rate: 0.00786 
2025-03-12 17:57:37.919357: train_loss -0.7101 
2025-03-12 17:57:37.919894: val_loss 1.234 
2025-03-12 17:57:37.920165: Pseudo dice [0.6449, 0.0809, 0.8773, 0.2643, 0.1113] 
2025-03-12 17:57:37.920331: Epoch time: 213.19 s 
2025-03-12 17:57:37.920475: Yayy! New best EMA pseudo Dice: 0.3573 
2025-03-12 17:57:41.288395:  
2025-03-12 17:57:41.288750: Epoch 48 
2025-03-12 17:57:41.289008: Current learning rate: 0.00781 
2025-03-12 18:01:13.838939: train_loss -0.7054 
2025-03-12 18:01:13.839368: val_loss 1.209 
2025-03-12 18:01:13.839627: Pseudo dice [0.6882, 0.0608, 0.8978, 0.1543, 0.0025] 
2025-03-12 18:01:13.839818: Epoch time: 212.56 s 
2025-03-12 18:01:13.839985: Yayy! New best EMA pseudo Dice: 0.3577 
2025-03-12 18:01:18.492712:  
2025-03-12 18:01:18.493030: Epoch 49 
2025-03-12 18:01:18.493242: Current learning rate: 0.00777 
2025-03-12 18:04:51.214463: train_loss -0.694 
2025-03-12 18:04:51.214977: val_loss 1.1684 
2025-03-12 18:04:51.215150: Pseudo dice [0.6318, 0.0724, 0.8836, 0.1874, 0.12] 
2025-03-12 18:04:51.215253: Epoch time: 212.73 s 
2025-03-12 18:04:52.560007: Yayy! New best EMA pseudo Dice: 0.3598 
2025-03-12 18:04:57.134660:  
2025-03-12 18:04:57.134986: Epoch 50 
2025-03-12 18:04:57.135413: Current learning rate: 0.00772 
2025-03-12 18:08:29.670306: train_loss -0.6986 
2025-03-12 18:08:29.670728: val_loss 1.5974 
2025-03-12 18:08:29.670888: Pseudo dice [0.5978, 0.1054, 0.8879, 0.1901, 0.0123] 
2025-03-12 18:08:29.670992: Epoch time: 212.54 s 
2025-03-12 18:08:31.321267:  
2025-03-12 18:08:31.321490: Epoch 51 
2025-03-12 18:08:31.321763: Current learning rate: 0.00767 
2025-03-12 18:12:04.006258: train_loss -0.6936 
2025-03-12 18:12:04.006664: val_loss 1.3923 
2025-03-12 18:12:04.006821: Pseudo dice [0.6167, 0.0535, 0.8523, 0.1104, 0.0453] 
2025-03-12 18:12:04.006922: Epoch time: 212.69 s 
2025-03-12 18:12:05.644535:  
2025-03-12 18:12:05.645074: Epoch 52 
2025-03-12 18:12:05.645484: Current learning rate: 0.00763 
2025-03-12 18:15:38.173938: train_loss -0.6964 
2025-03-12 18:15:38.174425: val_loss 1.1753 
2025-03-12 18:15:38.174691: Pseudo dice [0.6162, 0.167, 0.8882, 0.1621, 0.0883] 
2025-03-12 18:15:38.174845: Epoch time: 212.54 s 
2025-03-12 18:15:38.174965: Yayy! New best EMA pseudo Dice: 0.36 
2025-03-12 18:15:43.066257:  
2025-03-12 18:15:43.066586: Epoch 53 
2025-03-12 18:15:43.066854: Current learning rate: 0.00758 
2025-03-12 18:19:16.062324: train_loss -0.7091 
2025-03-12 18:19:16.062675: val_loss 1.2344 
2025-03-12 18:19:16.062888: Pseudo dice [0.6362, 0.1447, 0.8855, 0.103, 0.0547] 
2025-03-12 18:19:16.063040: Epoch time: 213.0 s 
2025-03-12 18:19:16.063144: Yayy! New best EMA pseudo Dice: 0.3605 
2025-03-12 18:19:20.675058:  
2025-03-12 18:19:20.675473: Epoch 54 
2025-03-12 18:19:20.675904: Current learning rate: 0.00753 
2025-03-12 18:22:53.480814: train_loss -0.7339 
2025-03-12 18:22:53.481395: val_loss 1.2635 
2025-03-12 18:22:53.482128: Pseudo dice [0.6211, 0.1251, 0.8701, 0.1578, 0.1702] 
2025-03-12 18:22:53.482432: Epoch time: 212.81 s 
2025-03-12 18:22:53.482636: Yayy! New best EMA pseudo Dice: 0.3633 
2025-03-12 18:22:57.876527:  
2025-03-12 18:22:57.876876: Epoch 55 
2025-03-12 18:22:57.877220: Current learning rate: 0.00749 
2025-03-12 18:26:30.628070: train_loss -0.7073 
2025-03-12 18:26:30.628553: val_loss 1.4502 
2025-03-12 18:26:30.628731: Pseudo dice [0.6324, 0.121, 0.8867, 0.0774, 0.0508] 
2025-03-12 18:26:30.628900: Epoch time: 212.76 s 
2025-03-12 18:26:32.279738:  
2025-03-12 18:26:32.280063: Epoch 56 
2025-03-12 18:26:32.280232: Current learning rate: 0.00744 
2025-03-12 18:30:05.093864: train_loss -0.7121 
2025-03-12 18:30:05.094198: val_loss 1.3115 
2025-03-12 18:30:05.094339: Pseudo dice [0.5896, 0.1245, 0.8891, 0.1997, 0.001] 
2025-03-12 18:30:05.094433: Epoch time: 212.82 s 
2025-03-12 18:30:06.749377:  
2025-03-12 18:30:06.749766: Epoch 57 
2025-03-12 18:30:06.749947: Current learning rate: 0.00739 
2025-03-12 18:33:39.645301: train_loss -0.7222 
2025-03-12 18:33:39.645668: val_loss 1.4045 
2025-03-12 18:33:39.645807: Pseudo dice [0.638, 0.103, 0.8888, 0.2333, 0.0165] 
2025-03-12 18:33:39.645900: Epoch time: 212.9 s 
2025-03-12 18:33:39.645963: Yayy! New best EMA pseudo Dice: 0.3636 
2025-03-12 18:33:44.161684:  
2025-03-12 18:33:44.161964: Epoch 58 
2025-03-12 18:33:44.162110: Current learning rate: 0.00735 
2025-03-12 18:37:17.004784: train_loss -0.7166 
2025-03-12 18:37:17.005242: val_loss 1.5212 
2025-03-12 18:37:17.005460: Pseudo dice [0.6117, 0.063, 0.8689, 0.129, 0.02] 
2025-03-12 18:37:17.005631: Epoch time: 212.85 s 
2025-03-12 18:37:18.648735:  
2025-03-12 18:37:18.649058: Epoch 59 
2025-03-12 18:37:18.649245: Current learning rate: 0.0073 
2025-03-12 18:40:51.325621: train_loss -0.6737 
2025-03-12 18:40:51.326057: val_loss 1.2057 
2025-03-12 18:40:51.326681: Pseudo dice [0.6091, 0.1089, 0.8896, 0.0903, 0.0195] 
2025-03-12 18:40:51.327049: Epoch time: 212.68 s 
2025-03-12 18:40:53.065435:  
2025-03-12 18:40:53.065738: Epoch 60 
2025-03-12 18:40:53.065995: Current learning rate: 0.00725 
2025-03-12 18:44:25.550538: train_loss -0.6914 
2025-03-12 18:44:25.551155: val_loss 1.4679 
2025-03-12 18:44:25.551339: Pseudo dice [0.5584, 0.0843, 0.8676, 0.228, 0.0149] 
2025-03-12 18:44:25.551521: Epoch time: 212.49 s 
2025-03-12 18:44:27.224989:  
2025-03-12 18:44:27.225568: Epoch 61 
2025-03-12 18:44:27.225883: Current learning rate: 0.00721 
2025-03-12 18:47:59.893784: train_loss -0.7204 
2025-03-12 18:47:59.894405: val_loss 1.5689 
2025-03-12 18:47:59.894944: Pseudo dice [0.5758, 0.0803, 0.8859, 0.1795, 0.0028] 
2025-03-12 18:47:59.895205: Epoch time: 212.67 s 
2025-03-12 18:48:01.587207:  
2025-03-12 18:48:01.587554: Epoch 62 
2025-03-12 18:48:01.587931: Current learning rate: 0.00716 
2025-03-12 18:51:34.293823: train_loss -0.7195 
2025-03-12 18:51:34.294370: val_loss 1.4518 
2025-03-12 18:51:34.294595: Pseudo dice [0.5849, 0.0876, 0.8875, 0.1498, 0.0285] 
2025-03-12 18:51:34.294706: Epoch time: 212.71 s 
2025-03-12 18:51:36.004940:  
2025-03-12 18:51:36.005375: Epoch 63 
2025-03-12 18:51:36.005616: Current learning rate: 0.00711 
2025-03-12 18:55:08.765700: train_loss -0.7206 
2025-03-12 18:55:08.766147: val_loss 1.4203 
2025-03-12 18:55:08.766274: Pseudo dice [0.6063, 0.0848, 0.869, 0.253, 0.0465] 
2025-03-12 18:55:08.766371: Epoch time: 212.77 s 
2025-03-12 18:55:10.436850:  
2025-03-12 18:55:10.437251: Epoch 64 
2025-03-12 18:55:10.437523: Current learning rate: 0.00707 
2025-03-12 18:58:43.266693: train_loss -0.6993 
2025-03-12 18:58:43.267560: val_loss 1.2952 
2025-03-12 18:58:43.267770: Pseudo dice [0.6413, 0.0809, 0.8845, 0.2255, 0.0247] 
2025-03-12 18:58:43.267942: Epoch time: 212.84 s 
2025-03-12 18:58:45.041282:  
2025-03-12 18:58:45.041662: Epoch 65 
2025-03-12 18:58:45.041831: Current learning rate: 0.00702 
2025-03-12 19:02:17.945429: train_loss -0.7163 
2025-03-12 19:02:17.945803: val_loss 1.1304 
2025-03-12 19:02:17.945930: Pseudo dice [0.6477, 0.0722, 0.8975, 0.1375, 0.0308] 
2025-03-12 19:02:17.946023: Epoch time: 212.91 s 
2025-03-12 19:02:19.607173:  
2025-03-12 19:02:19.607423: Epoch 66 
2025-03-12 19:02:19.607609: Current learning rate: 0.00697 
2025-03-12 19:05:52.914182: train_loss -0.7375 
2025-03-12 19:05:52.914562: val_loss 1.5387 
2025-03-12 19:05:52.914699: Pseudo dice [0.5815, 0.0504, 0.8758, 0.298, 0.0046] 
2025-03-12 19:05:52.914798: Epoch time: 213.31 s 
2025-03-12 19:05:54.634274:  
2025-03-12 19:05:54.634596: Epoch 67 
2025-03-12 19:05:54.634823: Current learning rate: 0.00693 
2025-03-12 19:09:27.686613: train_loss -0.7342 
2025-03-12 19:09:27.687109: val_loss 1.5535 
2025-03-12 19:09:27.687321: Pseudo dice [0.6124, 0.0972, 0.8942, 0.1995, 0.0323] 
2025-03-12 19:09:27.687506: Epoch time: 213.06 s 
2025-03-12 19:09:29.413317:  
2025-03-12 19:09:29.413532: Epoch 68 
2025-03-12 19:09:29.413655: Current learning rate: 0.00688 
2025-03-12 19:13:02.449438: train_loss -0.7318 
2025-03-12 19:13:02.449945: val_loss 1.476 
2025-03-12 19:13:02.450254: Pseudo dice [0.6348, 0.0861, 0.9, 0.1679, 0.0104] 
2025-03-12 19:13:02.450431: Epoch time: 213.04 s 
2025-03-12 19:13:04.186917:  
2025-03-12 19:13:04.187264: Epoch 69 
2025-03-12 19:13:04.187519: Current learning rate: 0.00683 
2025-03-12 19:16:37.252909: train_loss -0.7381 
2025-03-12 19:16:37.253380: val_loss 1.4884 
2025-03-12 19:16:37.253630: Pseudo dice [0.5713, 0.0657, 0.8894, 0.2427, 0.045] 
2025-03-12 19:16:37.253792: Epoch time: 213.07 s 
2025-03-12 19:16:38.941338:  
2025-03-12 19:16:38.941697: Epoch 70 
2025-03-12 19:16:38.941911: Current learning rate: 0.00679 
2025-03-12 19:20:12.070477: train_loss -0.7342 
2025-03-12 19:20:12.070822: val_loss 1.3853 
2025-03-12 19:20:12.070965: Pseudo dice [0.6032, 0.1304, 0.8968, 0.1652, 0.0189] 
2025-03-12 19:20:12.071062: Epoch time: 213.13 s 
2025-03-12 19:20:13.770183:  
2025-03-12 19:20:13.770523: Epoch 71 
2025-03-12 19:20:13.770796: Current learning rate: 0.00674 
2025-03-12 19:23:46.211468: train_loss -0.7408 
2025-03-12 19:23:46.211814: val_loss 1.4386 
2025-03-12 19:23:46.212020: Pseudo dice [0.6493, 0.0458, 0.8858, 0.1459, 0.0357] 
2025-03-12 19:23:46.212178: Epoch time: 212.45 s 
2025-03-12 19:23:47.919693:  
2025-03-12 19:23:47.920029: Epoch 72 
2025-03-12 19:23:47.920207: Current learning rate: 0.00669 
2025-03-12 19:27:20.588962: train_loss -0.7383 
2025-03-12 19:27:20.589338: val_loss 1.6426 
2025-03-12 19:27:20.589525: Pseudo dice [0.5761, 0.095, 0.8755, 0.1661, 0.0854] 
2025-03-12 19:27:20.589696: Epoch time: 212.67 s 
2025-03-12 19:27:22.320945:  
2025-03-12 19:27:22.321288: Epoch 73 
2025-03-12 19:27:22.321450: Current learning rate: 0.00665 
2025-03-12 19:30:55.437608: train_loss -0.7416 
2025-03-12 19:30:55.438446: val_loss 1.3265 
2025-03-12 19:30:55.438848: Pseudo dice [0.6192, 0.1506, 0.9067, 0.157, 0.1594] 
2025-03-12 19:30:55.439115: Epoch time: 213.12 s 
2025-03-12 19:30:55.439308: Yayy! New best EMA pseudo Dice: 0.3636 
2025-03-12 19:30:59.880816:  
2025-03-12 19:30:59.881174: Epoch 74 
2025-03-12 19:30:59.881355: Current learning rate: 0.0066 
2025-03-12 19:34:32.798326: train_loss -0.7375 
2025-03-12 19:34:32.798856: val_loss 1.3024 
2025-03-12 19:34:32.799153: Pseudo dice [0.6377, 0.0899, 0.9, 0.2437, 0.0057] 
2025-03-12 19:34:32.799343: Epoch time: 212.92 s 
2025-03-12 19:34:32.799427: Yayy! New best EMA pseudo Dice: 0.3648 
2025-03-12 19:34:37.484423:  
2025-03-12 19:34:37.484758: Epoch 75 
2025-03-12 19:34:37.484954: Current learning rate: 0.00655 
2025-03-12 19:38:10.434490: train_loss -0.7508 
2025-03-12 19:38:10.434828: val_loss 1.6072 
2025-03-12 19:38:10.435046: Pseudo dice [0.6189, 0.0941, 0.8849, 0.1179, 0.0001] 
2025-03-12 19:38:10.435220: Epoch time: 212.95 s 
2025-03-12 19:38:12.141622:  
2025-03-12 19:38:12.141958: Epoch 76 
2025-03-12 19:38:12.142129: Current learning rate: 0.0065 
2025-03-12 19:41:44.973762: train_loss -0.746 
2025-03-12 19:41:44.974246: val_loss 1.3744 
2025-03-12 19:41:44.974571: Pseudo dice [0.6354, 0.1103, 0.8879, 0.2486, 0.0064] 
2025-03-12 19:41:44.974763: Epoch time: 212.84 s 
2025-03-12 19:41:46.685513:  
2025-03-12 19:41:46.685841: Epoch 77 
2025-03-12 19:41:46.686009: Current learning rate: 0.00646 
2025-03-12 19:45:19.541467: train_loss -0.7547 
2025-03-12 19:45:19.541898: val_loss 1.7246 
2025-03-12 19:45:19.542099: Pseudo dice [0.5757, 0.1101, 0.8875, 0.1431, 0.0001] 
2025-03-12 19:45:19.542260: Epoch time: 212.86 s 
2025-03-12 19:45:21.290074:  
2025-03-12 19:45:21.290308: Epoch 78 
2025-03-12 19:45:21.290451: Current learning rate: 0.00641 
2025-03-12 19:48:54.130687: train_loss -0.7531 
2025-03-12 19:48:54.131148: val_loss 1.7299 
2025-03-12 19:48:54.131274: Pseudo dice [0.5595, 0.0633, 0.8905, 0.1631, 0.0023] 
2025-03-12 19:48:54.131371: Epoch time: 212.85 s 
2025-03-12 19:48:55.871259:  
2025-03-12 19:48:55.871633: Epoch 79 
2025-03-12 19:48:55.871863: Current learning rate: 0.00636 
2025-03-12 19:52:29.069601: train_loss -0.7336 
2025-03-12 19:52:29.070012: val_loss 1.5274 
2025-03-12 19:52:29.070158: Pseudo dice [0.6125, 0.1116, 0.8895, 0.1524, 0.0377] 
2025-03-12 19:52:29.070257: Epoch time: 213.2 s 
2025-03-12 19:52:30.797273:  
2025-03-12 19:52:30.797527: Epoch 80 
2025-03-12 19:52:30.797781: Current learning rate: 0.00631 
2025-03-12 19:56:04.986058: train_loss -0.7372 
2025-03-12 19:56:04.986503: val_loss 1.2806 
2025-03-12 19:56:04.986693: Pseudo dice [0.6013, 0.077, 0.8956, 0.1127, 0.0031] 
2025-03-12 19:56:04.986836: Epoch time: 214.19 s 
2025-03-12 19:56:06.750567:  
2025-03-12 19:56:06.750858: Epoch 81 
2025-03-12 19:56:06.751119: Current learning rate: 0.00627 
2025-03-12 19:59:41.949141: train_loss -0.7347 
2025-03-12 19:59:41.949658: val_loss 1.6152 
2025-03-12 19:59:41.949857: Pseudo dice [0.5659, 0.1164, 0.896, 0.2085, 0.0103] 
2025-03-12 19:59:41.950007: Epoch time: 215.2 s 
2025-03-12 19:59:43.677042:  
2025-03-12 19:59:43.677385: Epoch 82 
2025-03-12 19:59:43.677568: Current learning rate: 0.00622 
2025-03-12 20:03:18.726683: train_loss -0.747 
2025-03-12 20:03:18.727011: val_loss 1.6708 
2025-03-12 20:03:18.727153: Pseudo dice [0.5639, 0.132, 0.8818, 0.0913, 0.0563] 
2025-03-12 20:03:18.727250: Epoch time: 215.06 s 
2025-03-12 20:03:20.332975:  
2025-03-12 20:03:20.333427: Epoch 83 
2025-03-12 20:03:20.333825: Current learning rate: 0.00617 
2025-03-12 20:06:55.470289: train_loss -0.748 
2025-03-12 20:06:55.470860: val_loss 1.425 
2025-03-12 20:06:55.471136: Pseudo dice [0.6121, 0.082, 0.8939, 0.1664, 0.011] 
2025-03-12 20:06:55.471241: Epoch time: 215.14 s 
2025-03-12 20:06:57.101354:  
2025-03-12 20:06:57.101725: Epoch 84 
2025-03-12 20:06:57.101929: Current learning rate: 0.00612 
2025-03-12 20:10:32.387318: train_loss -0.7514 
2025-03-12 20:10:32.387765: val_loss 1.597 
2025-03-12 20:10:32.387922: Pseudo dice [0.604, 0.1048, 0.8883, 0.1032, 0.0071] 
2025-03-12 20:10:32.388024: Epoch time: 215.29 s 
2025-03-12 20:10:34.025072:  
2025-03-12 20:10:34.025474: Epoch 85 
2025-03-12 20:10:34.025659: Current learning rate: 0.00608 
2025-03-12 20:14:09.178574: train_loss -0.7367 
2025-03-12 20:14:09.179094: val_loss 1.4997 
2025-03-12 20:14:09.179306: Pseudo dice [0.6282, 0.0568, 0.8902, 0.1508, 0.003] 
2025-03-12 20:14:09.179458: Epoch time: 215.16 s 
2025-03-12 20:14:10.837077:  
2025-03-12 20:14:10.837629: Epoch 86 
2025-03-12 20:14:10.837841: Current learning rate: 0.00603 
2025-03-12 20:17:45.966819: train_loss -0.7462 
2025-03-12 20:17:45.967227: val_loss 1.4558 
2025-03-12 20:17:45.967406: Pseudo dice [0.6349, 0.1519, 0.8956, 0.1714, 0.0055] 
2025-03-12 20:17:45.967558: Epoch time: 215.14 s 
2025-03-12 20:17:47.594823:  
2025-03-12 20:17:47.595311: Epoch 87 
2025-03-12 20:17:47.595586: Current learning rate: 0.00598 
2025-03-12 20:21:22.375028: train_loss -0.7531 
2025-03-12 20:21:22.375444: val_loss 1.4407 
2025-03-12 20:21:22.375625: Pseudo dice [0.6164, 0.1093, 0.8891, 0.1175, 0.0083] 
2025-03-12 20:21:22.376141: Epoch time: 214.79 s 
2025-03-12 20:21:24.032941:  
2025-03-12 20:21:24.033224: Epoch 88 
2025-03-12 20:21:24.033425: Current learning rate: 0.00593 
2025-03-12 20:24:58.836110: train_loss -0.7585 
2025-03-12 20:24:58.836525: val_loss 1.3277 
2025-03-12 20:24:58.836725: Pseudo dice [0.6305, 0.0208, 0.8985, 0.1261, 0.0195] 
2025-03-12 20:24:58.836825: Epoch time: 214.81 s 
2025-03-12 20:25:00.454817:  
2025-03-12 20:25:00.455112: Epoch 89 
2025-03-12 20:25:00.455288: Current learning rate: 0.00589 
2025-03-12 20:28:35.355029: train_loss -0.7586 
2025-03-12 20:28:35.355442: val_loss 1.6634 
2025-03-12 20:28:35.355571: Pseudo dice [0.5846, 0.0521, 0.8813, 0.033, 0.0087] 
2025-03-12 20:28:35.355669: Epoch time: 214.91 s 
2025-03-12 20:28:36.961050:  
2025-03-12 20:28:36.961332: Epoch 90 
2025-03-12 20:28:36.961494: Current learning rate: 0.00584 
2025-03-12 20:32:11.801625: train_loss -0.7326 
2025-03-12 20:32:11.802088: val_loss 1.837 
2025-03-12 20:32:11.802243: Pseudo dice [0.5673, 0.027, 0.8483, 0.0466, 0.2787] 
2025-03-12 20:32:11.802337: Epoch time: 214.85 s 
2025-03-12 20:32:13.401040:  
2025-03-12 20:32:13.401357: Epoch 91 
2025-03-12 20:32:13.401598: Current learning rate: 0.00579 
2025-03-12 20:35:48.309171: train_loss -0.7268 
2025-03-12 20:35:48.309580: val_loss 1.3191 
2025-03-12 20:35:48.309852: Pseudo dice [0.6695, 0.067, 0.8931, 0.1863, 0.01] 
2025-03-12 20:35:48.310080: Epoch time: 214.91 s 
2025-03-12 20:35:49.892781:  
2025-03-12 20:35:49.893314: Epoch 92 
2025-03-12 20:35:49.893675: Current learning rate: 0.00574 
2025-03-12 20:39:24.674167: train_loss -0.7416 
2025-03-12 20:39:24.674670: val_loss 1.575 
2025-03-12 20:39:24.674902: Pseudo dice [0.6194, 0.0914, 0.8786, 0.1006, 0.0044] 
2025-03-12 20:39:24.675041: Epoch time: 214.79 s 
2025-03-12 20:39:26.306367:  
2025-03-12 20:39:26.306757: Epoch 93 
2025-03-12 20:39:26.306982: Current learning rate: 0.0057 
2025-03-12 20:43:01.320757: train_loss -0.7483 
2025-03-12 20:43:01.321296: val_loss 1.4578 
2025-03-12 20:43:01.321573: Pseudo dice [0.6656, 0.1473, 0.8826, 0.0371, 0.0156] 
2025-03-12 20:43:01.321707: Epoch time: 215.02 s 
2025-03-12 20:43:02.907861:  
2025-03-12 20:43:02.908158: Epoch 94 
2025-03-12 20:43:02.908313: Current learning rate: 0.00565 
2025-03-12 20:46:37.729370: train_loss -0.7271 
2025-03-12 20:46:37.729797: val_loss 1.4113 
2025-03-12 20:46:37.729985: Pseudo dice [0.6527, 0.1277, 0.9022, 0.0426, 0.0505] 
2025-03-12 20:46:37.730108: Epoch time: 214.83 s 
2025-03-12 20:46:39.336693:  
2025-03-12 20:46:39.336935: Epoch 95 
2025-03-12 20:46:39.337189: Current learning rate: 0.0056 
2025-03-12 20:50:14.092048: train_loss -0.7496 
2025-03-12 20:50:14.092452: val_loss 1.3401 
2025-03-12 20:50:14.092615: Pseudo dice [0.6367, 0.1078, 0.8971, 0.2116, 0.0029] 
2025-03-12 20:50:14.092718: Epoch time: 214.76 s 
2025-03-12 20:50:15.685206:  
2025-03-12 20:50:15.685531: Epoch 96 
2025-03-12 20:50:15.685719: Current learning rate: 0.00555 
2025-03-12 20:53:50.565667: train_loss -0.7334 
2025-03-12 20:53:50.566000: val_loss 1.2685 
2025-03-12 20:53:50.566138: Pseudo dice [0.6049, 0.1724, 0.8811, 0.2338, 0.0016] 
2025-03-12 20:53:50.566244: Epoch time: 214.89 s 
2025-03-12 20:53:52.227915:  
2025-03-12 20:53:52.228249: Epoch 97 
2025-03-12 20:53:52.228408: Current learning rate: 0.0055 
2025-03-12 20:57:27.096751: train_loss -0.7117 
2025-03-12 20:57:27.097134: val_loss 1.3853 
2025-03-12 20:57:27.097255: Pseudo dice [0.659, 0.079, 0.8813, 0.1064, 0.009] 
2025-03-12 20:57:27.097347: Epoch time: 214.87 s 
2025-03-12 20:57:28.843353:  
2025-03-12 20:57:28.843992: Epoch 98 
2025-03-12 20:57:28.844192: Current learning rate: 0.00546 
2025-03-12 21:01:03.451558: train_loss -0.7278 
2025-03-12 21:01:03.451888: val_loss 1.2424 
2025-03-12 21:01:03.452036: Pseudo dice [0.672, 0.0176, 0.8855, 0.2351, 0.0379] 
2025-03-12 21:01:03.452129: Epoch time: 214.61 s 
2025-03-12 21:01:05.084703:  
2025-03-12 21:01:05.084960: Epoch 99 
2025-03-12 21:01:05.085134: Current learning rate: 0.00541 
2025-03-12 21:04:39.838959: train_loss -0.7459 
2025-03-12 21:04:39.839481: val_loss 1.1803 
2025-03-12 21:04:39.839780: Pseudo dice [0.6741, 0.0306, 0.9094, 0.1288, 0.002] 
2025-03-12 21:04:39.839927: Epoch time: 214.76 s 
2025-03-12 21:04:44.821294:  
2025-03-12 21:04:44.821640: Epoch 100 
2025-03-12 21:04:44.821811: Current learning rate: 0.00536 
2025-03-12 21:08:18.457392: train_loss -0.7577 
2025-03-12 21:08:18.457755: val_loss 1.3014 
2025-03-12 21:08:18.457892: Pseudo dice [0.6933, 0.0797, 0.9018, 0.0864, 0.0012] 
2025-03-12 21:08:18.458055: Epoch time: 213.64 s 
2025-03-12 21:08:20.186000:  
2025-03-12 21:08:20.186373: Epoch 101 
2025-03-12 21:08:20.186661: Current learning rate: 0.00531 
2025-03-12 21:11:53.576830: train_loss -0.737 
2025-03-12 21:11:53.577252: val_loss 1.3841 
2025-03-12 21:11:53.577450: Pseudo dice [0.6684, 0.0557, 0.886, 0.283, 0.0106] 
2025-03-12 21:11:53.577593: Epoch time: 213.4 s 
2025-03-12 21:11:55.280494:  
2025-03-12 21:11:55.280861: Epoch 102 
2025-03-12 21:11:55.281044: Current learning rate: 0.00526 
2025-03-12 21:15:29.943184: train_loss -0.7531 
2025-03-12 21:15:29.943700: val_loss 1.1831 
2025-03-12 21:15:29.943882: Pseudo dice [0.6679, 0.0891, 0.912, 0.1783, 0.0118] 
2025-03-12 21:15:29.943983: Epoch time: 214.67 s 
2025-03-12 21:15:31.674834:  
2025-03-12 21:15:31.675978: Epoch 103 
2025-03-12 21:15:31.676240: Current learning rate: 0.00521 
2025-03-12 21:19:05.687856: train_loss -0.7441 
2025-03-12 21:19:05.688500: val_loss 1.5294 
2025-03-12 21:19:05.688956: Pseudo dice [0.6257, 0.0405, 0.8924, 0.0489, 0.0114] 
2025-03-12 21:19:05.689276: Epoch time: 214.02 s 
2025-03-12 21:19:07.494275:  
2025-03-12 21:19:07.494630: Epoch 104 
2025-03-12 21:19:07.494821: Current learning rate: 0.00517 
2025-03-12 21:22:40.869514: train_loss -0.7415 
2025-03-12 21:22:40.870073: val_loss 1.2942 
2025-03-12 21:22:40.870263: Pseudo dice [0.6795, 0.0854, 0.9029, 0.1275, 0.0637] 
2025-03-12 21:22:40.870410: Epoch time: 213.38 s 
2025-03-12 21:22:42.524059:  
2025-03-12 21:22:42.524327: Epoch 105 
2025-03-12 21:22:42.524530: Current learning rate: 0.00512 
2025-03-12 21:26:15.924871: train_loss -0.7208 
2025-03-12 21:26:15.925292: val_loss 1.472 
2025-03-12 21:26:15.925406: Pseudo dice [0.5054, 0.1221, 0.8791, 0.1006, 0.0155] 
2025-03-12 21:26:15.925502: Epoch time: 213.41 s 
2025-03-12 21:26:17.580237:  
2025-03-12 21:26:17.580773: Epoch 106 
2025-03-12 21:26:17.581082: Current learning rate: 0.00507 
2025-03-12 21:29:51.071324: train_loss -0.7123 
2025-03-12 21:29:51.071715: val_loss 1.4308 
2025-03-12 21:29:51.072098: Pseudo dice [0.6305, 0.0673, 0.8927, 0.2568, 0.0017] 
2025-03-12 21:29:51.072298: Epoch time: 213.5 s 
2025-03-12 21:29:53.665604:  
2025-03-12 21:29:53.665936: Epoch 107 
2025-03-12 21:29:53.666263: Current learning rate: 0.00502 
2025-03-12 21:33:27.244414: train_loss -0.7368 
2025-03-12 21:33:27.245035: val_loss 1.4324 
2025-03-12 21:33:27.245280: Pseudo dice [0.6224, 0.0877, 0.8841, 0.1365, 0.0383] 
2025-03-12 21:33:27.245442: Epoch time: 213.58 s 
2025-03-12 21:33:28.965353:  
2025-03-12 21:33:28.965874: Epoch 108 
2025-03-12 21:33:28.966625: Current learning rate: 0.00497 
2025-03-12 21:37:02.488802: train_loss -0.7513 
2025-03-12 21:37:02.489133: val_loss 1.413 
2025-03-12 21:37:02.489261: Pseudo dice [0.6351, 0.0919, 0.8961, 0.2747, 0.0113] 
2025-03-12 21:37:02.489359: Epoch time: 213.53 s 
2025-03-12 21:37:04.135113:  
2025-03-12 21:37:04.135468: Epoch 109 
2025-03-12 21:37:04.135688: Current learning rate: 0.00492 
2025-03-12 21:40:37.736867: train_loss -0.7511 
2025-03-12 21:40:37.737264: val_loss 1.4303 
2025-03-12 21:40:37.737397: Pseudo dice [0.6146, 0.1046, 0.8943, 0.1541, 0.0036] 
2025-03-12 21:40:37.737496: Epoch time: 213.61 s 
2025-03-12 21:40:39.403222:  
2025-03-12 21:40:39.403526: Epoch 110 
2025-03-12 21:40:39.403716: Current learning rate: 0.00487 
2025-03-12 21:44:13.231547: train_loss -0.7515 
2025-03-12 21:44:13.231984: val_loss 1.4885 
2025-03-12 21:44:13.232117: Pseudo dice [0.6031, 0.0649, 0.8912, 0.2388, 0.0045] 
2025-03-12 21:44:13.232215: Epoch time: 213.84 s 
2025-03-12 21:44:14.930034:  
2025-03-12 21:44:14.930392: Epoch 111 
2025-03-12 21:44:14.930591: Current learning rate: 0.00483 
2025-03-12 21:47:48.245762: train_loss -0.7633 
2025-03-12 21:47:48.247118: val_loss 1.2728 
2025-03-12 21:47:48.247336: Pseudo dice [0.6506, 0.1092, 0.8874, 0.1437, 0.0624] 
2025-03-12 21:47:48.247509: Epoch time: 213.32 s 
2025-03-12 21:47:50.004850:  
2025-03-12 21:47:50.005243: Epoch 112 
2025-03-12 21:47:50.005686: Current learning rate: 0.00478 
2025-03-12 21:51:23.652512: train_loss -0.7529 
2025-03-12 21:51:23.652953: val_loss 1.5377 
2025-03-12 21:51:23.653141: Pseudo dice [0.6334, 0.0914, 0.8832, 0.3371, 0.0253] 
2025-03-12 21:51:23.653240: Epoch time: 213.65 s 
2025-03-12 21:51:25.298418:  
2025-03-12 21:51:25.298779: Epoch 113 
2025-03-12 21:51:25.298987: Current learning rate: 0.00473 
2025-03-12 21:54:58.618040: train_loss -0.7733 
2025-03-12 21:54:58.618597: val_loss 1.5742 
2025-03-12 21:54:58.618811: Pseudo dice [0.6072, 0.1172, 0.8957, 0.074, 0.0639] 
2025-03-12 21:54:58.618975: Epoch time: 213.33 s 
2025-03-12 21:55:01.604327:  
2025-03-12 21:55:01.604638: Epoch 114 
2025-03-12 21:55:01.604896: Current learning rate: 0.00468 
2025-03-12 21:58:35.068255: train_loss -0.7628 
2025-03-12 21:58:35.068620: val_loss 1.3292 
2025-03-12 21:58:35.068755: Pseudo dice [0.6164, 0.0522, 0.8746, 0.1788, 0.1041] 
2025-03-12 21:58:35.068853: Epoch time: 213.47 s 
2025-03-12 21:58:36.717904:  
2025-03-12 21:58:36.718216: Epoch 115 
2025-03-12 21:58:36.718344: Current learning rate: 0.00463 
2025-03-12 22:02:10.230779: train_loss -0.7698 
2025-03-12 22:02:10.231147: val_loss 1.5072 
2025-03-12 22:02:10.231276: Pseudo dice [0.6155, 0.1108, 0.8928, 0.1062, 0.0506] 
2025-03-12 22:02:10.231382: Epoch time: 213.52 s 
2025-03-12 22:02:11.969182:  
2025-03-12 22:02:11.969675: Epoch 116 
2025-03-12 22:02:11.969870: Current learning rate: 0.00458 
2025-03-12 22:05:45.664436: train_loss -0.7627 
2025-03-12 22:05:45.664843: val_loss 1.4636 
2025-03-12 22:05:45.664997: Pseudo dice [0.608, 0.0973, 0.9016, 0.1691, 0.0124] 
2025-03-12 22:05:45.665116: Epoch time: 213.7 s 
2025-03-12 22:05:47.362128:  
2025-03-12 22:05:47.362525: Epoch 117 
2025-03-12 22:05:47.362826: Current learning rate: 0.00453 
2025-03-12 22:09:21.926891: train_loss -0.7584 
2025-03-12 22:09:21.927268: val_loss 1.6012 
2025-03-12 22:09:21.927398: Pseudo dice [0.606, 0.076, 0.88, 0.1054, 0.0127] 
2025-03-12 22:09:21.927499: Epoch time: 214.57 s 
2025-03-12 22:09:23.624956:  
2025-03-12 22:09:23.625308: Epoch 118 
2025-03-12 22:09:23.625852: Current learning rate: 0.00448 
2025-03-12 22:12:59.076409: train_loss -0.7688 
2025-03-12 22:12:59.077194: val_loss 1.5341 
2025-03-12 22:12:59.077439: Pseudo dice [0.6291, 0.08, 0.8866, 0.1091, 0.0204] 
2025-03-12 22:12:59.077606: Epoch time: 215.46 s 
2025-03-12 22:13:00.839080:  
2025-03-12 22:13:00.839631: Epoch 119 
2025-03-12 22:13:00.839878: Current learning rate: 0.00443 
2025-03-12 22:16:36.179303: train_loss -0.7598 
2025-03-12 22:16:36.179801: val_loss 1.2951 
2025-03-12 22:16:36.179929: Pseudo dice [0.6526, 0.1599, 0.877, 0.1866, 0.1679] 
2025-03-12 22:16:36.180033: Epoch time: 215.35 s 
2025-03-12 22:16:38.267891:  
2025-03-12 22:16:38.268268: Epoch 120 
2025-03-12 22:16:38.268556: Current learning rate: 0.00438 
2025-03-12 22:20:12.257081: train_loss -0.7299 
2025-03-12 22:20:12.257444: val_loss 1.4326 
2025-03-12 22:20:12.257649: Pseudo dice [0.6228, 0.045, 0.8782, 0.1855, 0.0096] 
2025-03-12 22:20:12.257809: Epoch time: 214.0 s 
2025-03-12 22:20:14.956092:  
2025-03-12 22:20:14.957379: Epoch 121 
2025-03-12 22:20:14.957939: Current learning rate: 0.00433 
2025-03-12 22:23:50.377409: train_loss -0.7432 
2025-03-12 22:23:50.377902: val_loss 1.5137 
2025-03-12 22:23:50.378128: Pseudo dice [0.6045, 0.0477, 0.8872, 0.056, 0.1014] 
2025-03-12 22:23:50.378251: Epoch time: 215.43 s 
2025-03-12 22:23:52.148340:  
2025-03-12 22:23:52.148693: Epoch 122 
2025-03-12 22:23:52.148993: Current learning rate: 0.00429 
2025-03-12 22:27:28.587228: train_loss -0.763 
2025-03-12 22:27:28.588410: val_loss 1.514 
2025-03-12 22:27:28.588633: Pseudo dice [0.6239, 0.0804, 0.8929, 0.0852, 0.0518] 
2025-03-12 22:27:28.588773: Epoch time: 216.44 s 
2025-03-12 22:27:30.496358:  
2025-03-12 22:27:30.496789: Epoch 123 
2025-03-12 22:27:30.497139: Current learning rate: 0.00424 
2025-03-12 22:31:06.088584: train_loss -0.7631 
2025-03-12 22:31:06.089019: val_loss 1.2132 
2025-03-12 22:31:06.089231: Pseudo dice [0.6412, 0.0364, 0.9064, 0.1576, 0.1967] 
2025-03-12 22:31:06.089387: Epoch time: 215.6 s 
2025-03-12 22:31:07.928436:  
2025-03-12 22:31:07.928836: Epoch 124 
2025-03-12 22:31:07.929096: Current learning rate: 0.00419 
2025-03-12 22:34:45.424688: train_loss -0.7607 
2025-03-12 22:34:45.425121: val_loss 1.3287 
2025-03-12 22:34:45.425301: Pseudo dice [0.6384, 0.0637, 0.9035, 0.0546, 0.1261] 
2025-03-12 22:34:45.425426: Epoch time: 217.5 s 
2025-03-12 22:34:47.183941:  
2025-03-12 22:34:47.184507: Epoch 125 
2025-03-12 22:34:47.184878: Current learning rate: 0.00414 
2025-03-12 22:38:22.175802: train_loss -0.7697 
2025-03-12 22:38:22.176209: val_loss 1.6019 
2025-03-12 22:38:22.176337: Pseudo dice [0.6584, 0.03, 0.8799, 0.0761, 0.1263] 
2025-03-12 22:38:22.176438: Epoch time: 215.0 s 
2025-03-12 22:38:23.863653:  
2025-03-12 22:38:23.863918: Epoch 126 
2025-03-12 22:38:23.864089: Current learning rate: 0.00409 
2025-03-12 22:41:58.741325: train_loss -0.7686 
2025-03-12 22:41:58.741683: val_loss 1.4874 
2025-03-12 22:41:58.741821: Pseudo dice [0.6318, 0.0894, 0.8962, 0.0751, 0.0156] 
2025-03-12 22:41:58.741917: Epoch time: 214.88 s 
2025-03-12 22:42:00.559589:  
2025-03-12 22:42:00.559818: Epoch 127 
2025-03-12 22:42:00.560070: Current learning rate: 0.00404 
2025-03-12 22:45:36.206002: train_loss -0.7639 
2025-03-12 22:45:36.206590: val_loss 1.5375 
2025-03-12 22:45:36.206925: Pseudo dice [0.6678, 0.0613, 0.8934, 0.0505, 0.0321] 
2025-03-12 22:45:36.207169: Epoch time: 215.65 s 
2025-03-12 22:45:38.122367:  
2025-03-12 22:45:38.122750: Epoch 128 
2025-03-12 22:45:38.122957: Current learning rate: 0.00399 
2025-03-12 22:49:13.289572: train_loss -0.7701 
2025-03-12 22:49:13.289989: val_loss 1.4506 
2025-03-12 22:49:13.290179: Pseudo dice [0.6413, 0.0418, 0.8891, 0.1942, 0.0522] 
2025-03-12 22:49:13.290347: Epoch time: 215.17 s 
2025-03-12 22:49:15.058530:  
2025-03-12 22:49:15.058801: Epoch 129 
2025-03-12 22:49:15.059045: Current learning rate: 0.00394 
2025-03-12 22:52:50.195035: train_loss -0.7711 
2025-03-12 22:52:50.195499: val_loss 1.3098 
2025-03-12 22:52:50.195710: Pseudo dice [0.6589, 0.1117, 0.8871, 0.0855, 0.0609] 
2025-03-12 22:52:50.195852: Epoch time: 215.14 s 
2025-03-12 22:52:52.012994:  
2025-03-12 22:52:52.013293: Epoch 130 
2025-03-12 22:52:52.013511: Current learning rate: 0.00389 
2025-03-12 22:56:27.185205: train_loss -0.7788 
2025-03-12 22:56:27.185727: val_loss 1.5163 
2025-03-12 22:56:27.185973: Pseudo dice [0.5867, 0.0981, 0.8937, 0.1426, 0.0095] 
2025-03-12 22:56:27.186170: Epoch time: 215.18 s 
2025-03-12 22:56:28.960243:  
2025-03-12 22:56:28.960519: Epoch 131 
2025-03-12 22:56:28.960841: Current learning rate: 0.00384 
2025-03-12 23:00:04.192319: train_loss -0.7572 
2025-03-12 23:00:04.192901: val_loss 1.4241 
2025-03-12 23:00:04.193154: Pseudo dice [0.6496, 0.0419, 0.8866, 0.1884, 0.1394] 
2025-03-12 23:00:04.193291: Epoch time: 215.24 s 
2025-03-12 23:00:05.935008:  
2025-03-12 23:00:05.935312: Epoch 132 
2025-03-12 23:00:05.935715: Current learning rate: 0.00379 
2025-03-12 23:03:41.958528: train_loss -0.7698 
2025-03-12 23:03:41.958897: val_loss 1.481 
2025-03-12 23:03:41.959025: Pseudo dice [0.6376, 0.0301, 0.8993, 0.0744, 0.1684] 
2025-03-12 23:03:41.959124: Epoch time: 216.03 s 
2025-03-12 23:03:43.744376:  
2025-03-12 23:03:43.744844: Epoch 133 
2025-03-12 23:03:43.745035: Current learning rate: 0.00374 
2025-03-12 23:07:19.955788: train_loss -0.7773 
2025-03-12 23:07:19.957591: val_loss 1.5335 
2025-03-12 23:07:19.958245: Pseudo dice [0.6365, 0.0892, 0.8779, 0.107, 0.097] 
2025-03-12 23:07:19.959356: Epoch time: 216.23 s 
2025-03-12 23:07:21.778205:  
2025-03-12 23:07:21.778621: Epoch 134 
2025-03-12 23:07:21.778829: Current learning rate: 0.00369 
2025-03-12 23:10:58.115180: train_loss -0.7599 
2025-03-12 23:10:58.115589: val_loss 1.4538 
2025-03-12 23:10:58.115722: Pseudo dice [0.6448, 0.1315, 0.8821, 0.2402, 0.1994] 
2025-03-12 23:10:58.115819: Epoch time: 216.34 s 
2025-03-12 23:10:58.115891: Yayy! New best EMA pseudo Dice: 0.3653 
2025-03-12 23:11:02.637464:  
2025-03-12 23:11:02.637799: Epoch 135 
2025-03-12 23:11:02.638052: Current learning rate: 0.00364 
2025-03-12 23:14:38.663535: train_loss -0.7428 
2025-03-12 23:14:38.663958: val_loss 1.7535 
2025-03-12 23:14:38.664149: Pseudo dice [0.5465, 0.0767, 0.8866, 0.2149, 0.0026] 
2025-03-12 23:14:38.664311: Epoch time: 216.03 s 
2025-03-12 23:14:40.497616:  
2025-03-12 23:14:40.497915: Epoch 136 
2025-03-12 23:14:40.498151: Current learning rate: 0.00359 
